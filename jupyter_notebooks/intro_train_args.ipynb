{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/intro_train_args.ipynb\" \n",
    "   target=\"_parent\">\n",
    "   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Controlling training & evaluation or 'what do all these agent.train(...) args mean ?'\n",
    "\n",
    "There are quite a few arguments you can pass to an agents `train(...)` method. Quite a few of them start \n",
    "with `num_....` like `num_iterations` or `num_iteratons_between_eval`. \n",
    "They are used to control \n",
    "\n",
    "* the actual training - who often is the policy updated ? - as well as\n",
    "* the evaluation of the current policy - who well does the currently trained policy actually perform ?\n",
    "\n",
    "The training is performed in units called 'iterations'.\n",
    "After each iteration the policy is updated.\n",
    "At the beginning of an iteration data is collected by playing `num_episodes_per_iteration` games, which\n",
    "are also called episodes. \n",
    "If an episodes runs for more than `max_steps_per_episode` steps, the episode is stopped and a new episode\n",
    "is started (by calling the gym environments `reset()` method).\n",
    "\n",
    "To see how the policy changes during training we evaluate the policy every `num_iterations_between_eval`\n",
    "iterations. Thus during evaluation the policy is not modified, we just use it to collect statistics\n",
    "data like the average sum of rewards or the average number of steps per episode.\n",
    "\n",
    "Thus call to `agent.train(...)`results in a loop like this:\n",
    "\n",
    "````\n",
    "    initialize policy (typicallsy with random values)\n",
    "    for i in range(num_iterations) \n",
    "        play num_episodes_per_iteration episodes using the current policy and store all steps\n",
    "            if an episode runs for more than max_steps_per_episode steps stop the episode and start a new one\n",
    "        update the current policy num_epochs_per_iteration times using the collected steps\n",
    "\n",
    "        every num_iterations_between_eval iterations:\n",
    "            play num_eval_episodes episodes and collect statistics like avg reward and avg steps\n",
    "````\n",
    "\n",
    "We can actually see how the train-loop runs:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install packages (gym, tfagents, tensorflow,....)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "if 'google.colab' in sys.modules:\n",
    "    !apt-get install xvfb >/dev/null\n",
    "    !pip install pyvirtualdisplay >/dev/null    \n",
    "    \n",
    "    from pyvirtualdisplay import Display\n",
    "    Display(visible=0, size=(960, 720)).start() \n",
    "else:\n",
    "    #  for local installation\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### install easyagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install easyagents >/dev/null    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.agents import PpoAgent\n",
    "from easyagents.callbacks import duration, log\n",
    "\n",
    "ppoAgent = PpoAgent('CartPole-v0')\n",
    "ppoAgent.train([duration.Fast()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.agents import PpoAgent\n",
    "from easyagents.callbacks import duration, log\n",
    "\n",
    "#ppoAgent = PpoAgent('CartPole-v0', backend='tensorforce')\n",
    "#ppoAgent.train([log.Agent(), duration.Fast()], default_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "While in tensorforce we also first do a sequence of agent and policy. Note that in contrast to tfagents we do not\n",
    "build up actor and critic policy networks but instead pass a network specification to the Agent.create call.\n",
    "Moreover tensorforce implements already the train loop through its Runner class. \n",
    "Thus we only see 1 call to runner.run instead of the many api calls for tfagents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Seeding\n",
    "\n",
    "To set a seed use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import easyagents\n",
    "\n",
    "easyagents.agents.seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once set, the seed is applied before each call to train. Let's validate this using our log.Agent callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.agents import PpoAgent\n",
    "from easyagents.callbacks import duration, log\n",
    "\n",
    "ppoAgent = PpoAgent('CartPole-v0')\n",
    "ppoAgent.train([log.Agent(), duration.Fast()], default_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that at the very beginning the calls to set the seeds for tensorflow, numpy and python.\n",
    "\n",
    "## Iteration & Duration logging\n",
    "Use the log.Iteration() callback to log the training progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.agents import PpoAgent\n",
    "from easyagents.callbacks import duration, log\n",
    "\n",
    "ppoAgent = PpoAgent('CartPole-v0')\n",
    "ppoAgent.train([log.Iteration(), duration.Fast()], default_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The first line shows the result of an initial policy evaluation depicting the policy's performance before any \n",
    "training has happend. Policy evaluation happens every `num_iterations_between_eval` iterations and spans over\n",
    "`num_episodes_per_eval` episodes. For every evaluation period the result is logged again. The `steps_done`value is \n",
    "the number of training steps (excluding the steps taken during evaluation). \n",
    "To see the training duration configuration use the log.Duration() callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.agents import PpoAgent\n",
    "from easyagents.callbacks import duration, log\n",
    "\n",
    "ppoAgent = PpoAgent('CartPole-v0')\n",
    "ppoAgent.train([log.Duration(), duration.Fast()], default_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gym steps logging\n",
    "Use the log.Step() callback to investigate how the agent interacts with the gym environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.agents import PpoAgent\n",
    "from easyagents.callbacks import duration, log\n",
    "\n",
    "ppoAgent = PpoAgent('CartPole-v0')\n",
    "ppoAgent.train([log.Step(), duration.Fast()], default_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For each call to the gym environments step method you get a log entry, along with the action taken and current\n",
    "observation. Each entry starts with \n",
    "\n",
    "[{gym_env_id} {instance_id}:{episode_in_instance}:{step_in_episode}]\n",
    "\n",
    "followed by the id of the current training iteration as well as the current iteration step count.\n",
    "If in a evaluation period you get the same statistics for the current evaluation episode.\n",
    "\n",
    "You may easily implement other log callbacks to produce statistics specific to your problem domain.\n",
    "\n",
    "## Fixing jupyter output cell clearing\n",
    "It seems that jupyter / matplotlib backend changes its behaviour of outputing the current figure of an \n",
    "evaluated cell (if you can help here, please let use know by \n",
    "[creating an issue](https://github.com/christianhidber/easyagents/issues/new/choose)).\n",
    "\n",
    "Nonetheless you may directly control easyagents jupyter ouput cell clearing behaviour through the plot.Clear()\n",
    "callback:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.agents import PpoAgent\n",
    "from easyagents.callbacks import duration, plot\n",
    "\n",
    "ppoAgent = PpoAgent('CartPole-v0')\n",
    "ppoAgent.train([plot.Clear(on_train=False,on_play=False), duration.Fast()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If your plot gets \"doubled\" after cell evaluation set on_train / on_play to True, if it disappears to False. \n",
    "Once plot.Clear() is called, the behaviour stays the same across a upcoming plots."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zpzHtN3-kQ26",
    "w3OdHyWEEEwy",
    "bzoq0VM85p46"
   ],
   "include_colab_link": true,
   "name": "easyagents_logging.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}