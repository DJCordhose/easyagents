{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU7ylMh1kQ2y"
   },
   "source": [
    "# Berater Environment v13\n",
    "\n",
    "## Changes from v12 (work in progress)\n",
    "* migration to easyagents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym==0.10.11 > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3OdHyWEEEwy"
   },
   "source": [
    "# Define Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQyb_Aq8Kg9j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "from gym import spaces\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsJ6zcXvwN53"
   },
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-S4sZG5ZkQ3T"
   },
   "outputs": [],
   "source": [
    "def state_name_to_int(state):\n",
    "    state_name_map = {\n",
    "        'S': 0,\n",
    "        'A': 1,\n",
    "        'B': 2,\n",
    "        'C': 3,\n",
    "        'D': 4,\n",
    "        'E': 5,\n",
    "        'F': 6,\n",
    "        'G': 7,\n",
    "        'H': 8,\n",
    "        'K': 9,\n",
    "        'L': 10,\n",
    "        'M': 11,\n",
    "        'N': 12,\n",
    "        'O': 13\n",
    "    }\n",
    "    return state_name_map[state]\n",
    "\n",
    "def int_to_state_name(state_as_int):\n",
    "    state_map = {\n",
    "        0: 'S',\n",
    "        1: 'A',\n",
    "        2: 'B',\n",
    "        3: 'C',\n",
    "        4: 'D',\n",
    "        5: 'E',\n",
    "        6: 'F',\n",
    "        7: 'G',\n",
    "        8: 'H',\n",
    "        9: 'K',\n",
    "        10: 'L',\n",
    "        11: 'M',\n",
    "        12: 'N',\n",
    "        13: 'O'\n",
    "    }\n",
    "    return state_map[state_as_int]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-olom0nwiSX"
   },
   "source": [
    "### Berater Environment (OpenAI Gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3plH2u3Swotj"
   },
   "outputs": [],
   "source": [
    "class BeraterEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    The Berater Problem\n",
    "\n",
    "    Actions: \n",
    "    There are 4 discrete deterministic actions, each choosing one direction\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['ansi']}\n",
    "    \n",
    "    showStep = False\n",
    "    showDone = True\n",
    "    envEpisodeModulo = 100\n",
    "\n",
    "    def __init__(self):\n",
    "#         self.map = {\n",
    "#             'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
    "#             'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
    "#             'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
    "#             'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
    "#         }\n",
    "        self.map = {\n",
    "            'S': [('A', 300), ('B', 100), ('C', 200 )],\n",
    "            'A': [('S', 300), ('B', 100), ('E', 100 ), ('D', 100 )],\n",
    "            'B': [('S', 100), ('A', 100), ('C', 50 ), ('K', 200 )],\n",
    "            'C': [('S', 200), ('B', 50), ('M', 100 ), ('L', 200 )],\n",
    "            'D': [('A', 100), ('F', 50)],\n",
    "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
    "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
    "            'G': [('F', 200), ('O', 300)],\n",
    "            'H': [('E', 100), ('K', 300)],\n",
    "            'K': [('B', 200), ('H', 300)],\n",
    "            'L': [('C', 200), ('M', 50)],\n",
    "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
    "            'N': [('M', 100), ('O', 100)],\n",
    "            'O': [('N', 100), ('G', 300)]\n",
    "        }\n",
    "        max_paths = 4\n",
    "        self.action_space = spaces.Discrete(max_paths)\n",
    "      \n",
    "        positions = len(self.map)\n",
    "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
    "        # non existing path is -1000 and no position change\n",
    "        # look at what #getObservation returns if you are confused\n",
    "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
    "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
    "        self.observation_space = spaces.Box(low=low,\n",
    "                                             high=high,\n",
    "                                             dtype=np.float32)\n",
    "        self.reward_range = (-1, 1)\n",
    "\n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "\n",
    "        self.envReward = 0\n",
    "        self.envEpisodeCount = 0\n",
    "        self.envStepCount = 0\n",
    "\n",
    "        self.reset()\n",
    "        self.optimum = self.calculate_customers_reward()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def iterate_path(self, state, action):\n",
    "        paths = self.map[state]\n",
    "        if action < len(paths):\n",
    "          return paths[action]\n",
    "        else:\n",
    "          # sorry, no such action, stay where you are and pay a high penalty\n",
    "          return (state, 1000)\n",
    "      \n",
    "    def step(self, action):\n",
    "        if self.debugStep:\n",
    "          pdb.set_trace()\n",
    "        destination, cost = self.iterate_path(self.state, action)\n",
    "        lastState = self.state\n",
    "        customerReward = self.customer_reward[destination]\n",
    "        reward = (customerReward - cost) / self.optimum\n",
    "\n",
    "        self.state = destination\n",
    "        self.customer_visited(destination)\n",
    "        done = (destination == 'S' and self.all_customers_visited())\n",
    "        if self.stepCount >= 200:\n",
    "          if BeraterEnv.showDone:\n",
    "            print(\"Done: stepCount >= 200\")\n",
    "          done = True\n",
    "\n",
    "        stateAsInt = state_name_to_int(self.state)\n",
    "        self.totalReward += reward\n",
    "        self.stepCount += 1\n",
    "        self.envReward += reward\n",
    "        self.envStepCount += 1\n",
    "\n",
    "        if self.showStep:\n",
    "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
    "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
    "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
    "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
    "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
    "                   )\n",
    "\n",
    "        if done and not self.isDone:\n",
    "            self.envEpisodeCount += 1\n",
    "            if BeraterEnv.showDone:\n",
    "                episodes = BeraterEnv.envEpisodeModulo\n",
    "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
    "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
    "                print( \"Done: \" + \n",
    "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
    "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
    "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
    "                        )\n",
    "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
    "                    self.envReward = 0\n",
    "                    self.envStepCount = 0\n",
    "\n",
    "        self.isDone = done\n",
    "        observation = self.getObservation(stateAsInt)\n",
    "        info = {\"from\": self.state, \"to\": destination}\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def getObservation(self, position):\n",
    "        result = np.array([ position, \n",
    "                               self.getPathObservation(position, 0),\n",
    "                               self.getPathObservation(position, 1),\n",
    "                               self.getPathObservation(position, 2),\n",
    "                               self.getPathObservation(position, 3)\n",
    "                              ],\n",
    "                             dtype=np.float32)\n",
    "        all_rest_rewards = list(self.customer_reward.values())\n",
    "        result = np.append(result, all_rest_rewards)\n",
    "        return result\n",
    "\n",
    "    def getPathObservation(self, position, path):\n",
    "        source = int_to_state_name(position)\n",
    "        paths = self.map[self.state]\n",
    "        if path < len(paths):\n",
    "          target, cost = paths[path]\n",
    "          reward = self.customer_reward[target] \n",
    "          result = reward - cost\n",
    "        else:\n",
    "          result = -1000\n",
    "\n",
    "        return result\n",
    "\n",
    "    def customer_visited(self, customer):\n",
    "        self.customer_reward[customer] = 0\n",
    "\n",
    "    def all_customers_visited(self):\n",
    "        return self.calculate_customers_reward() == 0\n",
    "\n",
    "    def calculate_customers_reward(self):\n",
    "        sum = 0\n",
    "        for value in self.customer_reward.values():\n",
    "            sum += value\n",
    "        return sum\n",
    "\n",
    "      \n",
    "    def modulate_reward(self):\n",
    "      number_of_customers = len(self.map) - 1\n",
    "      number_per_consultant = int(number_of_customers/2)\n",
    "#       number_per_consultant = int(number_of_customers/1.5)\n",
    "      self.customer_reward = {\n",
    "          'S': 0\n",
    "      }\n",
    "      for customer_nr in range(1, number_of_customers + 1):\n",
    "        self.customer_reward[int_to_state_name(customer_nr)] = 0\n",
    "      \n",
    "      # every consultant only visits a few random customers\n",
    "      samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
    "      key_list = list(self.customer_reward.keys())\n",
    "      for sample in samples:\n",
    "        self.customer_reward[key_list[sample]] = 1000\n",
    "\n",
    "      \n",
    "    def reset(self):\n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "\n",
    "        self.modulate_reward()\n",
    "        self.state = 'S'\n",
    "        return self.getObservation(state_name_to_int(self.state))\n",
    "      \n",
    "    def render(self):\n",
    "      print(self.customer_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9J54w2URZIme"
   },
   "outputs": [],
   "source": [
    "BeraterEnv.showStep = False\n",
    "BeraterEnv.showDone = False\n",
    "BeraterEnv.debugStep = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYaTAvAyYO-U"
   },
   "source": [
    "### Register with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'isEnvRegistered' in locals():\n",
    "  env_name=\"Berater-v1\"\n",
    "  gym.envs.registration.register(id=env_name,entry_point=BeraterEnv,max_episode_steps=1000)\n",
    "  isEnvRegistered=True\n",
    "  print(\"Berater registered as '\" + env_name + \"'\")\n",
    "else:\n",
    "  print(\"Already registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sX8eJGcbOJ30"
   },
   "source": [
    "# Train policy with tfagents PpoAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzoq0VM85p46"
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install easyagents > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !apt-get install xvfb >/dev/null\n",
    "    !pip install pyvirtualdisplay >/dev/null    \n",
    "    \n",
    "    from pyvirtualdisplay import Display\n",
    "    Display(visible=0, size=(960, 720)).start()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dry run (short training, default logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDurationFast\n",
    "from easyagents.config import LoggingVerbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ofYknQFRkRT"
   },
   "outputs": [],
   "source": [
    "ppoAgent = PpoAgent(    gym_env_name = 'Berater-v1',\n",
    "                        training_duration=TrainingDurationFast() )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default training (self-defined network, default logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent = PpoAgent( gym_env_name = 'Berater-v1', fc_layers=(500,500,500) )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training (duration, learning rate, logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDuration\n",
    "from easyagents.config import Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_duration=TrainingDuration( num_iterations = 2000,\n",
    "                                    num_episodes_per_iteration = 10,\n",
    "                                    max_steps_per_episode = 1000,\n",
    "                                    num_epochs_per_iteration = 5,\n",
    "                                    num_iterations_between_eval = 10,\n",
    "                                    num_eval_episodes = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging=Logging(log_agent=True, log_gym_api=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent = PpoAgent(    gym_env_name = 'Berater-v1',\n",
    "                        fc_layers=(500,500,500), \n",
    "                        training_duration=training_duration,\n",
    "                        logging=logging,\n",
    "                        learning_rate=1e-4\n",
    "                   )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zpzHtN3-kQ26",
    "w3OdHyWEEEwy",
    "bzoq0VM85p46"
   ],
   "name": "190316-2_tfagents_berater-v12_ppo.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
