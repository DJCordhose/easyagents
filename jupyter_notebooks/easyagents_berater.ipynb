{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU7ylMh1kQ2y"
   },
   "source": [
    "# Berater Environment v13\n",
    "\n",
    "## Changes from v12 (work in progress)\n",
    "* migration to easyagents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.10.11 > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3OdHyWEEEwy"
   },
   "source": [
    "# Define Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQyb_Aq8Kg9j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "from gym import spaces\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsJ6zcXvwN53"
   },
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-S4sZG5ZkQ3T"
   },
   "outputs": [],
   "source": [
    "def state_name_to_int(state):\n",
    "    state_name_map = {\n",
    "        'S': 0,\n",
    "        'A': 1,\n",
    "        'B': 2,\n",
    "        'C': 3,\n",
    "        'D': 4,\n",
    "        'E': 5,\n",
    "        'F': 6,\n",
    "        'G': 7,\n",
    "        'H': 8,\n",
    "        'K': 9,\n",
    "        'L': 10,\n",
    "        'M': 11,\n",
    "        'N': 12,\n",
    "        'O': 13\n",
    "    }\n",
    "    return state_name_map[state]\n",
    "\n",
    "def int_to_state_name(state_as_int):\n",
    "    state_map = {\n",
    "        0: 'S',\n",
    "        1: 'A',\n",
    "        2: 'B',\n",
    "        3: 'C',\n",
    "        4: 'D',\n",
    "        5: 'E',\n",
    "        6: 'F',\n",
    "        7: 'G',\n",
    "        8: 'H',\n",
    "        9: 'K',\n",
    "        10: 'L',\n",
    "        11: 'M',\n",
    "        12: 'N',\n",
    "        13: 'O'\n",
    "    }\n",
    "    return state_map[state_as_int]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-olom0nwiSX"
   },
   "source": [
    "### Berater Environment (OpenAI Gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3plH2u3Swotj"
   },
   "outputs": [],
   "source": [
    "class BeraterEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    The Berater Problem\n",
    "\n",
    "    Actions: \n",
    "    There are 4 discrete deterministic actions, each choosing one direction\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['ansi']}\n",
    "    \n",
    "    showStep = False\n",
    "    showDone = True\n",
    "    envEpisodeModulo = 100\n",
    "\n",
    "    def __init__(self):\n",
    "#         self.map = {\n",
    "#             'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
    "#             'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
    "#             'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
    "#             'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
    "#         }\n",
    "        self.map = {\n",
    "            'S': [('A', 300), ('B', 100), ('C', 200 )],\n",
    "            'A': [('S', 300), ('B', 100), ('E', 100 ), ('D', 100 )],\n",
    "            'B': [('S', 100), ('A', 100), ('C', 50 ), ('K', 200 )],\n",
    "            'C': [('S', 200), ('B', 50), ('M', 100 ), ('L', 200 )],\n",
    "            'D': [('A', 100), ('F', 50)],\n",
    "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
    "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
    "            'G': [('F', 200), ('O', 300)],\n",
    "            'H': [('E', 100), ('K', 300)],\n",
    "            'K': [('B', 200), ('H', 300)],\n",
    "            'L': [('C', 200), ('M', 50)],\n",
    "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
    "            'N': [('M', 100), ('O', 100)],\n",
    "            'O': [('N', 100), ('G', 300)]\n",
    "        }\n",
    "        max_paths = 4\n",
    "        self.action_space = spaces.Discrete(max_paths)\n",
    "      \n",
    "        positions = len(self.map)\n",
    "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
    "        # non existing path is -1000 and no position change\n",
    "        # look at what #getObservation returns if you are confused\n",
    "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
    "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
    "        self.observation_space = spaces.Box(low=low,\n",
    "                                             high=high,\n",
    "                                             dtype=np.float32)\n",
    "        self.reward_range = (-1, 1)\n",
    "\n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "\n",
    "        self.envReward = 0\n",
    "        self.envEpisodeCount = 0\n",
    "        self.envStepCount = 0\n",
    "\n",
    "        self.reset()\n",
    "        self.optimum = self.calculate_customers_reward()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def iterate_path(self, state, action):\n",
    "        paths = self.map[state]\n",
    "        if action < len(paths):\n",
    "          return paths[action]\n",
    "        else:\n",
    "          # sorry, no such action, stay where you are and pay a high penalty\n",
    "          return (state, 1000)\n",
    "      \n",
    "    def step(self, action):\n",
    "        if self.debugStep:\n",
    "          pdb.set_trace()\n",
    "        destination, cost = self.iterate_path(self.state, action)\n",
    "        lastState = self.state\n",
    "        customerReward = self.customer_reward[destination]\n",
    "        reward = (customerReward - cost) / self.optimum\n",
    "\n",
    "        self.state = destination\n",
    "        self.customer_visited(destination)\n",
    "        done = (destination == 'S' and self.all_customers_visited())\n",
    "        if self.stepCount >= 200:\n",
    "          if BeraterEnv.showDone:\n",
    "            print(\"Done: stepCount >= 200\")\n",
    "          done = True\n",
    "\n",
    "        stateAsInt = state_name_to_int(self.state)\n",
    "        self.totalReward += reward\n",
    "        self.stepCount += 1\n",
    "        self.envReward += reward\n",
    "        self.envStepCount += 1\n",
    "\n",
    "        if self.showStep:\n",
    "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
    "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
    "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
    "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
    "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
    "                   )\n",
    "\n",
    "        if done and not self.isDone:\n",
    "            self.envEpisodeCount += 1\n",
    "            if BeraterEnv.showDone:\n",
    "                episodes = BeraterEnv.envEpisodeModulo\n",
    "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
    "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
    "                print( \"Done: \" + \n",
    "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
    "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
    "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
    "                        )\n",
    "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
    "                    self.envReward = 0\n",
    "                    self.envStepCount = 0\n",
    "\n",
    "        self.isDone = done\n",
    "        observation = self.getObservation(stateAsInt)\n",
    "        info = {\"from\": self.state, \"to\": destination}\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def getObservation(self, position):\n",
    "        result = np.array([ position, \n",
    "                               self.getPathObservation(position, 0),\n",
    "                               self.getPathObservation(position, 1),\n",
    "                               self.getPathObservation(position, 2),\n",
    "                               self.getPathObservation(position, 3)\n",
    "                              ],\n",
    "                             dtype=np.float32)\n",
    "        all_rest_rewards = list(self.customer_reward.values())\n",
    "        result = np.append(result, all_rest_rewards)\n",
    "        return result\n",
    "\n",
    "    def getPathObservation(self, position, path):\n",
    "        source = int_to_state_name(position)\n",
    "        paths = self.map[self.state]\n",
    "        if path < len(paths):\n",
    "          target, cost = paths[path]\n",
    "          reward = self.customer_reward[target] \n",
    "          result = reward - cost\n",
    "        else:\n",
    "          result = -1000\n",
    "\n",
    "        return result\n",
    "\n",
    "    def customer_visited(self, customer):\n",
    "        self.customer_reward[customer] = 0\n",
    "\n",
    "    def all_customers_visited(self):\n",
    "        return self.calculate_customers_reward() == 0\n",
    "\n",
    "    def calculate_customers_reward(self):\n",
    "        sum = 0\n",
    "        for value in self.customer_reward.values():\n",
    "            sum += value\n",
    "        return sum\n",
    "\n",
    "      \n",
    "    def modulate_reward(self):\n",
    "      number_of_customers = len(self.map) - 1\n",
    "      number_per_consultant = int(number_of_customers/2)\n",
    "#       number_per_consultant = int(number_of_customers/1.5)\n",
    "      self.customer_reward = {\n",
    "          'S': 0\n",
    "      }\n",
    "      for customer_nr in range(1, number_of_customers + 1):\n",
    "        self.customer_reward[int_to_state_name(customer_nr)] = 0\n",
    "      \n",
    "      # every consultant only visits a few random customers\n",
    "      samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
    "      key_list = list(self.customer_reward.keys())\n",
    "      for sample in samples:\n",
    "        self.customer_reward[key_list[sample]] = 1000\n",
    "\n",
    "      \n",
    "    def reset(self):\n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "\n",
    "        self.modulate_reward()\n",
    "        self.state = 'S'\n",
    "        return self.getObservation(state_name_to_int(self.state))\n",
    "      \n",
    "    def render(self):\n",
    "      print(self.customer_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9J54w2URZIme"
   },
   "outputs": [],
   "source": [
    "BeraterEnv.showStep = False\n",
    "BeraterEnv.showDone = False\n",
    "BeraterEnv.debugStep = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYaTAvAyYO-U"
   },
   "source": [
    "### Register with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already registered\n"
     ]
    }
   ],
   "source": [
    "if not 'isEnvRegistered' in locals():\n",
    "  env_name=\"Berater-v1\"\n",
    "  gym.envs.registration.register(id=env_name,entry_point=BeraterEnv,max_episode_steps=1000)\n",
    "  isEnvRegistered=True\n",
    "  print(\"Berater registered as '\" + env_name + \"'\")\n",
    "else:\n",
    "  print(\"Already registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sX8eJGcbOJ30"
   },
   "source": [
    "# Train policy with tfagents PpoAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzoq0VM85p46"
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install easyagents > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -i https://test.pypi.org/simple/ easyagents > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dry run (very short, no logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDurationFast\n",
    "from easyagents.config import LoggingSilent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ofYknQFRkRT"
   },
   "outputs": [],
   "source": [
    "ppoAgent = PpoAgent(    gym_env_name = 'Berater-v1',\n",
    "                        training_duration=TrainingDurationFast(),\n",
    "                        logging = LoggingSilent() )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiP6UgA65163"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_average_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Short training (with chosen policy network layers, training durations & default logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_duration=TrainingDuration( num_iterations = 50,\n",
    "                                    num_episodes_per_iteration = 10,\n",
    "                                    max_steps_per_episode = 1000,\n",
    "                                    num_epochs_per_iteration = 5,\n",
    "                                    num_iterations_between_eval = 10,\n",
    "                                    num_eval_episodes = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0626 21:55:31.814436 12772 agents.py:60] gym_env_name=Berater-v1 fc_layers=(500, 500, 500)\n",
      "I0626 21:55:31.815778 12772 agents.py:69] executing: tf.compat.v1.enable_v2_behavior()\n",
      "I0626 21:55:31.816433 12772 agents.py:69] executing: tf.enable_eager_execution()\n",
      "I0626 21:55:31.817433 12772 agents.py:69] executing: tf.compat.v1.set_random_seed(0)\n",
      "I0626 21:55:31.828431 12772 agents.py:69] Creating environment:\n",
      "I0626 21:55:31.829433 12772 agents.py:69]    executing tf_py_environment.TFPyEnvironment( suite_gym.load )\n",
      "I0626 21:55:31.848441 12772 agents.py:69] Creating agent:\n",
      "I0626 21:55:31.849441 12772 agents.py:69]   creating  tf.compat.v1.train.AdamOptimizer( ... )\n",
      "I0626 21:55:31.862470 12772 agents.py:69]   creating  PPOAgent( ... )\n",
      "I0626 21:55:31.879480 12772 agents.py:69]   executing tf_agent.initialize()\n",
      "I0626 21:55:31.881434 12772 agents.py:69] Creating data collection:\n",
      "I0626 21:55:31.883462 12772 agents.py:69]   creating TFUniformReplayBuffer()\n",
      "I0626 21:55:31.902435 12772 agents.py:69]   creating DynamicEpisodeDriver()\n",
      "I0626 21:55:31.903436 12772 agents.py:69] Starting training:\n",
      "I0626 21:55:31.905580 12772 agents.py:69] executing compute_avg_return(...)\n",
      "I0626 21:55:31.905580 12772 agents.py:69]    executing tf_py_environment.TFPyEnvironment( suite_gym.load )\n",
      "I0626 21:55:31.910638 12772 logenv.py:72] #EnvId ResetCount.Steps [R=sumRewards]\n",
      "I0626 21:55:31.911630 12772 logenv.py:75] #1   0.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:55:33.389572 12772 logenv.py:75] #1   1.201 [R=  -3.2] game over\n",
      "I0626 21:55:34.968370 12772 logenv.py:75] #1   2.201 [R=  -3.3] game over\n",
      "I0626 21:55:36.521655 12772 logenv.py:75] #1   3.201 [R=  -3.0] game over\n",
      "I0626 21:55:38.094494 12772 logenv.py:75] #1   4.201 [R=  -3.2] game over\n",
      "I0626 21:55:39.829188 12772 logenv.py:75] #1   5.201 [R=  -3.2] game over\n",
      "I0626 21:55:41.331356 12772 logenv.py:75] #1   6.201 [R=  -3.0] game over\n",
      "I0626 21:55:42.842510 12772 logenv.py:75] #1   7.201 [R=  -3.2] game over\n",
      "I0626 21:55:44.262789 12772 logenv.py:75] #1   8.201 [R=  -3.2] game over\n",
      "I0626 21:55:46.075535 12772 logenv.py:75] #1   9.201 [R=  -3.2] game over\n",
      "I0626 21:55:47.810787 12772 logenv.py:75] #1  10.201 [R=  -3.2] game over\n",
      "I0626 21:55:47.812811 12772 agents.py:69] completed compute_avg_return(...) = -3.267\n",
      "I0626 21:55:47.814751 12772 agents.py:69] training 1 of 50: executing collect_driver.run()\n",
      "I0626 21:55:48.274494 17180 logenv.py:72] #EnvId ResetCount.Steps [R=sumRewards]\n",
      "I0626 21:55:48.282889 17180 logenv.py:75] #0   0.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:55:48.353419 15108 logenv.py:75] #0   1.66  [R=  -4.2] game over\n",
      "I0626 21:55:48.435143 10184 logenv.py:75] #0   2.70  [R=  -4.9] game over\n",
      "I0626 21:55:48.506519 15108 logenv.py:75] #0   3.59  [R=  -1.8] game over\n",
      "I0626 21:55:48.615656 15108 logenv.py:75] #0   4.101 [R=  -5.9] game over\n",
      "I0626 21:55:48.688384 14320 logenv.py:75] #0   5.64  [R=  -4.1] game over\n",
      "I0626 21:55:48.817382 17180 logenv.py:75] #0   6.117 [R=  -5.8] game over\n",
      "I0626 21:55:48.930040 17180 logenv.py:75] #0   7.105 [R=  -5.1] game over\n",
      "I0626 21:55:49.030262  5292 logenv.py:75] #0   8.88  [R=  -5.8] game over\n",
      "I0626 21:55:49.257621 17180 logenv.py:75] #0   9.201 [R= -13.8] game over\n",
      "I0626 21:55:49.380497 10184 logenv.py:75] #0  10.108 [R=  -6.7] game over\n",
      "I0626 21:55:49.389362 12772 agents.py:69] training 1 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:55:49.393510 12772 agents.py:69] training 1 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:00.924364 12772 agents.py:69] training 1 of 50: completed tf_agent.train(...) = 3993.260 [loss]\n",
      "I0626 21:56:00.926277 12772 agents.py:69] training 1 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:00.927850 12772 agents.py:69] training 2 of 50: executing collect_driver.run()\n",
      "I0626 21:56:00.929375 10184 logenv.py:75] #0  11.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:01.066155  5292 logenv.py:75] #0  12.139 [R= -10.8] game over\n",
      "I0626 21:56:01.249363 17180 logenv.py:75] #0  13.159 [R= -12.9] game over\n",
      "I0626 21:56:01.366969 10184 logenv.py:75] #0  14.105 [R=  -6.6] game over\n",
      "I0626 21:56:01.460242 14320 logenv.py:75] #0  15.84  [R=  -7.0] game over\n",
      "I0626 21:56:01.551264  5292 logenv.py:75] #0  16.76  [R=  -5.2] game over\n",
      "I0626 21:56:01.737406  5292 logenv.py:75] #0  17.192 [R= -10.2] game over\n",
      "I0626 21:56:01.955105 17180 logenv.py:75] #0  18.201 [R= -19.2] game over\n",
      "I0626 21:56:02.029461 18188 logenv.py:75] #0  19.65  [R=  -3.5] game over\n",
      "I0626 21:56:02.188450 15108 logenv.py:75] #0  20.146 [R= -11.0] game over\n",
      "I0626 21:56:02.312452  7092 logenv.py:75] #0  21.113 [R=  -7.0] game over\n",
      "I0626 21:56:02.323791 12772 agents.py:69] training 2 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:02.328602 12772 agents.py:69] training 2 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:09.066914 12772 agents.py:69] training 2 of 50: completed tf_agent.train(...) = 7359.441 [loss]\n",
      "I0626 21:56:09.067915 12772 agents.py:69] training 2 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:09.070005 12772 agents.py:69] training 3 of 50: executing collect_driver.run()\n",
      "I0626 21:56:09.071914 10184 logenv.py:75] #0  22.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:09.162128  5292 logenv.py:75] #0  23.88  [R=  -5.6] game over\n",
      "I0626 21:56:09.325193 17180 logenv.py:75] #0  24.150 [R=  -9.6] game over\n",
      "I0626 21:56:09.386697  5292 logenv.py:75] #0  25.50  [R=  -2.6] game over\n",
      "I0626 21:56:09.527025 18188 logenv.py:75] #0  26.125 [R=  -9.1] game over\n",
      "I0626 21:56:09.595386 17180 logenv.py:75] #0  27.58  [R=  -2.8] game over\n",
      "I0626 21:56:09.700975  5292 logenv.py:75] #0  28.99  [R=  -7.5] game over\n",
      "I0626 21:56:09.816673 15108 logenv.py:75] #0  29.103 [R=  -4.8] game over\n",
      "I0626 21:56:10.076111  5292 logenv.py:75] #0  30.201 [R= -11.9] game over\n",
      "I0626 21:56:10.197114 15108 logenv.py:75] #0  31.123 [R=  -8.1] game over\n",
      "I0626 21:56:10.395925  5292 logenv.py:75] #0  32.201 [R= -13.7] game over\n",
      "I0626 21:56:10.402928 12772 agents.py:69] training 3 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:10.406926 12772 agents.py:69] training 3 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:16.322461 12772 agents.py:69] training 3 of 50: completed tf_agent.train(...) = 3219.281 [loss]\n",
      "I0626 21:56:16.323461 12772 agents.py:69] training 3 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:16.325462 12772 agents.py:69] training 4 of 50: executing collect_driver.run()\n",
      "I0626 21:56:16.326460 17180 logenv.py:75] #0  33.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:16.355463 15108 logenv.py:75] #0  34.36  [R=  -1.1] game over\n",
      "I0626 21:56:16.422462 14320 logenv.py:75] #0  35.74  [R=  -3.9] game over\n",
      "I0626 21:56:16.481462 17180 logenv.py:75] #0  36.77  [R=  -4.8] game over\n",
      "I0626 21:56:16.573497 17500 logenv.py:75] #0  37.114 [R=  -7.2] game over\n",
      "I0626 21:56:16.644463 17500 logenv.py:75] #0  38.79  [R=  -4.0] game over\n",
      "I0626 21:56:16.746464 17180 logenv.py:75] #0  39.130 [R=  -9.5] game over\n",
      "I0626 21:56:16.776499 17180 logenv.py:75] #0  40.34  [R=  -1.1] game over\n",
      "I0626 21:56:16.825548 15108 logenv.py:75] #0  41.57  [R=  -3.6] game over\n",
      "I0626 21:56:16.893460 17500 logenv.py:75] #0  42.89  [R=  -5.6] game over\n",
      "I0626 21:56:16.959599 18188 logenv.py:75] #0  43.68  [R=  -4.4] game over\n",
      "I0626 21:56:16.972879 12772 agents.py:69] training 4 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:16.978466 12772 agents.py:69] training 4 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:18.319956 12772 agents.py:69] training 4 of 50: completed tf_agent.train(...) = 994.499 [loss]\n",
      "I0626 21:56:18.320953 12772 agents.py:69] training 4 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:18.322959 12772 agents.py:69] training 5 of 50: executing collect_driver.run()\n",
      "I0626 21:56:18.324952  7092 logenv.py:75] #0  44.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:18.408471 17500 logenv.py:75] #0  45.145 [R=  -9.2] game over\n",
      "I0626 21:56:18.468531 17180 logenv.py:75] #0  46.89  [R=  -5.0] game over\n",
      "I0626 21:56:18.536277 17500 logenv.py:75] #0  47.103 [R=  -6.1] game over\n",
      "I0626 21:56:18.668897 17180 logenv.py:75] #0  48.201 [R= -12.7] game over\n",
      "I0626 21:56:18.772535 17180 logenv.py:75] #0  49.171 [R= -10.0] game over\n",
      "I0626 21:56:18.864031 17180 logenv.py:75] #0  50.140 [R= -11.0] game over\n",
      "I0626 21:56:18.919034 17180 logenv.py:75] #0  51.76  [R=  -4.0] game over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0626 21:56:19.014696 17180 logenv.py:75] #0  52.162 [R= -13.8] game over\n",
      "I0626 21:56:19.072192 17180 logenv.py:75] #0  53.85  [R=  -3.9] game over\n",
      "I0626 21:56:19.151181 17180 logenv.py:75] #0  54.100 [R=  -4.3] game over\n",
      "I0626 21:56:19.159415 12772 agents.py:69] training 5 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:19.162410 12772 agents.py:69] training 5 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:20.582983 12772 agents.py:69] training 5 of 50: completed tf_agent.train(...) = 1658.831 [loss]\n",
      "I0626 21:56:20.584200 12772 agents.py:69] training 5 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:20.584983 12772 agents.py:69] training 6 of 50: executing collect_driver.run()\n",
      "I0626 21:56:20.585985  5292 logenv.py:75] #0  55.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:20.708983  7092 logenv.py:75] #0  56.197 [R= -12.9] game over\n",
      "I0626 21:56:20.822090  7092 logenv.py:75] #0  57.201 [R= -14.0] game over\n",
      "I0626 21:56:20.877991  7092 logenv.py:75] #0  58.94  [R=  -5.8] game over\n",
      "I0626 21:56:21.008991  7092 logenv.py:75] #0  59.201 [R= -12.1] game over\n",
      "I0626 21:56:21.088250  7092 logenv.py:75] #0  60.108 [R=  -6.0] game over\n",
      "I0626 21:56:21.250622  7092 logenv.py:75] #0  61.107 [R=  -6.3] game over\n",
      "I0626 21:56:21.307032  7092 logenv.py:75] #0  62.38  [R=  -0.8] game over\n",
      "I0626 21:56:21.396733  7092 logenv.py:75] #0  63.87  [R=  -4.6] game over\n",
      "I0626 21:56:21.520357  7092 logenv.py:75] #0  64.201 [R= -11.0] game over\n",
      "I0626 21:56:21.577435  7092 logenv.py:75] #0  65.93  [R=  -7.0] game over\n",
      "I0626 21:56:21.584440 12772 agents.py:69] training 6 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:21.586438 12772 agents.py:69] training 6 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:23.321145 12772 agents.py:69] training 6 of 50: completed tf_agent.train(...) = 982.234 [loss]\n",
      "I0626 21:56:23.322973 12772 agents.py:69] training 6 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:23.323503 12772 agents.py:69] training 7 of 50: executing collect_driver.run()\n",
      "I0626 21:56:23.325579 18188 logenv.py:75] #0  66.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:23.376246 18188 logenv.py:75] #0  67.75  [R=  -4.8] game over\n",
      "I0626 21:56:23.461780 18188 logenv.py:75] #0  68.149 [R=  -8.2] game over\n",
      "I0626 21:56:23.565573 18188 logenv.py:75] #0  69.189 [R= -10.6] game over\n",
      "I0626 21:56:23.620713 18188 logenv.py:75] #0  70.77  [R=  -3.4] game over\n",
      "I0626 21:56:23.695897 18188 logenv.py:75] #0  71.120 [R=  -6.9] game over\n",
      "I0626 21:56:23.787608 18188 logenv.py:75] #0  72.161 [R=  -9.5] game over\n",
      "I0626 21:56:23.887852 18188 logenv.py:75] #0  73.147 [R=  -8.3] game over\n",
      "I0626 21:56:23.954231 18188 logenv.py:75] #0  74.114 [R=  -6.2] game over\n",
      "I0626 21:56:24.026960 18188 logenv.py:75] #0  75.128 [R=  -7.7] game over\n",
      "I0626 21:56:24.116400 18188 logenv.py:75] #0  76.115 [R=  -6.8] game over\n",
      "I0626 21:56:24.123580 12772 agents.py:69] training 7 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:24.126397 12772 agents.py:69] training 7 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:25.999944 12772 agents.py:69] training 7 of 50: completed tf_agent.train(...) = 984.229 [loss]\n",
      "I0626 21:56:26.001446 12772 agents.py:69] training 7 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:26.003448 12772 agents.py:69] training 8 of 50: executing collect_driver.run()\n",
      "I0626 21:56:26.004446  7092 logenv.py:75] #0  77.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:26.079448  7092 logenv.py:75] #0  78.111 [R=  -6.9] game over\n",
      "I0626 21:56:26.139607  7092 logenv.py:75] #0  79.103 [R=  -3.4] game over\n",
      "I0626 21:56:26.198566  7092 logenv.py:75] #0  80.87  [R=  -5.0] game over\n",
      "I0626 21:56:26.219673  7092 logenv.py:75] #0  81.29  [R=  -0.9] game over\n",
      "I0626 21:56:26.263568  7092 logenv.py:75] #0  82.49  [R=  -2.2] game over\n",
      "I0626 21:56:26.338760  7092 logenv.py:75] #0  83.114 [R=  -5.2] game over\n",
      "I0626 21:56:26.381763  7092 logenv.py:75] #0  84.60  [R=  -2.7] game over\n",
      "I0626 21:56:26.502812  7092 logenv.py:75] #0  85.187 [R= -10.5] game over\n",
      "I0626 21:56:26.592029  7092 logenv.py:75] #0  86.82  [R=  -3.0] game over\n",
      "I0626 21:56:26.787283  7092 logenv.py:75] #0  87.201 [R=  -9.1] game over\n",
      "I0626 21:56:26.794689 12772 agents.py:69] training 8 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:26.797655 12772 agents.py:69] training 8 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:28.227832 12772 agents.py:69] training 8 of 50: completed tf_agent.train(...) = 850.108 [loss]\n",
      "I0626 21:56:28.229100 12772 agents.py:69] training 8 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:28.229831 12772 agents.py:69] training 9 of 50: executing collect_driver.run()\n",
      "I0626 21:56:28.231834  5292 logenv.py:75] #0  88.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:28.277875  5292 logenv.py:75] #0  89.73  [R=  -3.5] game over\n",
      "I0626 21:56:28.312918  5292 logenv.py:75] #0  90.56  [R=  -1.9] game over\n",
      "I0626 21:56:28.412209  5292 logenv.py:75] #0  91.201 [R=  -9.1] game over\n",
      "I0626 21:56:28.450541  5292 logenv.py:75] #0  92.66  [R=  -2.4] game over\n",
      "I0626 21:56:28.483733  5292 logenv.py:75] #0  93.58  [R=  -1.3] game over\n",
      "I0626 21:56:28.589798  5292 logenv.py:75] #0  94.201 [R= -10.1] game over\n",
      "I0626 21:56:28.662385  5292 logenv.py:75] #0  95.112 [R=  -4.2] game over\n",
      "I0626 21:56:28.782695  5292 logenv.py:75] #0  96.201 [R=  -9.3] game over\n",
      "I0626 21:56:28.853713  5292 logenv.py:75] #0  97.125 [R=  -6.2] game over\n",
      "I0626 21:56:28.910684  5292 logenv.py:75] #0  98.91  [R=  -4.4] game over\n",
      "I0626 21:56:28.916757 12772 agents.py:69] training 9 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:28.920773 12772 agents.py:69] training 9 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:30.363106 12772 agents.py:69] training 9 of 50: completed tf_agent.train(...) = 830.298 [loss]\n",
      "I0626 21:56:30.364105 12772 agents.py:69] training 9 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:30.365107 12772 agents.py:69] training 10 of 50: executing collect_driver.run()\n",
      "I0626 21:56:30.367109 17500 logenv.py:75] #0  99.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:30.418778 17500 logenv.py:75] #0 100.84  [R=  -2.2] game over\n",
      "I0626 21:56:30.455776 17500 logenv.py:75] #0 101.55  [R=  -0.8] game over\n",
      "I0626 21:56:30.528916 17500 logenv.py:75] #0 102.111 [R=  -3.3] game over\n",
      "I0626 21:56:30.600918 17500 logenv.py:75] #0 103.116 [R=  -3.8] game over\n",
      "I0626 21:56:30.658920 17500 logenv.py:75] #0 104.94  [R=  -2.9] game over\n",
      "I0626 21:56:30.776780 17500 logenv.py:75] #0 105.201 [R=  -7.4] game over\n",
      "I0626 21:56:30.871228 17500 logenv.py:75] #0 106.173 [R=  -5.6] game over\n",
      "I0626 21:56:30.982740 17500 logenv.py:75] #0 107.189 [R=  -8.0] game over\n",
      "I0626 21:56:31.061619 17500 logenv.py:75] #0 108.115 [R=  -2.6] game over\n",
      "I0626 21:56:31.307997 17500 logenv.py:75] #0 109.186 [R= -10.1] game over\n",
      "I0626 21:56:31.320753 12772 agents.py:69] training 10 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:31.326742 12772 agents.py:69] training 10 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:33.045807 12772 agents.py:69] training 10 of 50: completed tf_agent.train(...) = 500.319 [loss]\n",
      "I0626 21:56:33.046391 12772 agents.py:69] training 10 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:33.048440 12772 agents.py:69] executing compute_avg_return(...)\n",
      "I0626 21:56:34.126744 12772 logenv.py:75] #1  11.201 [R=  -3.2] game over\n",
      "I0626 21:56:35.749964 12772 logenv.py:75] #1  12.201 [R=  -5.2] game over\n",
      "I0626 21:56:36.695016 12772 logenv.py:75] #1  13.201 [R=  -3.2] game over\n",
      "I0626 21:56:37.926201 12772 logenv.py:75] #1  14.201 [R=  -3.0] game over\n",
      "I0626 21:56:39.222898 12772 logenv.py:75] #1  15.201 [R=  -3.2] game over\n",
      "I0626 21:56:40.503449 12772 logenv.py:75] #1  16.201 [R=  -3.0] game over\n",
      "I0626 21:56:41.768489 12772 logenv.py:75] #1  17.201 [R=  -3.2] game over\n",
      "I0626 21:56:42.927502 12772 logenv.py:75] #1  18.201 [R=  -3.0] game over\n",
      "I0626 21:56:43.815914 12772 logenv.py:75] #1  19.201 [R=  -3.2] game over\n",
      "I0626 21:56:45.207032 12772 logenv.py:75] #1  20.201 [R=  -3.3] game over\n",
      "I0626 21:56:45.209399 12772 agents.py:69] completed compute_avg_return(...) = -3.432\n",
      "I0626 21:56:45.210090 12772 agents.py:69] training 11 of 50: executing collect_driver.run()\n",
      "I0626 21:56:45.212105  5292 logenv.py:75] #0 110.0   [R=   0.0] executing reset(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0626 21:56:45.292976  5292 logenv.py:75] #0 111.117 [R=  -3.9] game over\n",
      "I0626 21:56:45.403972  5292 logenv.py:75] #0 112.192 [R=  -4.4] game over\n",
      "I0626 21:56:45.492792  5292 logenv.py:75] #0 113.156 [R=  -4.3] game over\n",
      "I0626 21:56:45.551025  5292 logenv.py:75] #0 114.84  [R=  -2.1] game over\n",
      "I0626 21:56:45.576712  5292 logenv.py:75] #0 115.41  [R=  -0.9] game over\n",
      "I0626 21:56:45.656719  5292 logenv.py:75] #0 116.138 [R=  -3.7] game over\n",
      "I0626 21:56:45.684289  5292 logenv.py:75] #0 117.39  [R=  -0.7] game over\n",
      "I0626 21:56:45.732401  5292 logenv.py:75] #0 118.79  [R=  -2.3] game over\n",
      "I0626 21:56:45.852010  5292 logenv.py:75] #0 119.201 [R=  -5.7] game over\n",
      "I0626 21:56:45.870047  5292 logenv.py:75] #0 120.26  [R=  -0.1] game over\n",
      "I0626 21:56:45.876016 12772 agents.py:69] training 11 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:45.879020 12772 agents.py:69] training 11 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:47.324054 12772 agents.py:69] training 11 of 50: completed tf_agent.train(...) = 295.860 [loss]\n",
      "I0626 21:56:47.324977 12772 agents.py:69] training 11 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:47.326979 12772 agents.py:69] training 12 of 50: executing collect_driver.run()\n",
      "I0626 21:56:47.327979  7092 logenv.py:75] #0 121.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:47.385062  7092 logenv.py:75] #0 122.82  [R=  -1.5] game over\n",
      "I0626 21:56:47.433078  7092 logenv.py:75] #0 123.85  [R=  -2.2] game over\n",
      "I0626 21:56:47.491980  7092 logenv.py:75] #0 124.88  [R=  -2.4] game over\n",
      "I0626 21:56:47.522980  7092 logenv.py:75] #0 125.44  [R=  -0.6] game over\n",
      "I0626 21:56:47.569984  7092 logenv.py:75] #0 126.73  [R=  -1.1] game over\n",
      "I0626 21:56:47.645520  7092 logenv.py:75] #0 127.98  [R=  -3.1] game over\n",
      "I0626 21:56:47.730106  7092 logenv.py:75] #0 128.123 [R=  -2.9] game over\n",
      "I0626 21:56:47.801369  7092 logenv.py:75] #0 129.102 [R=  -2.2] game over\n",
      "I0626 21:56:47.863470  7092 logenv.py:75] #0 130.73  [R=  -1.1] game over\n",
      "I0626 21:56:47.991826  7092 logenv.py:75] #0 131.172 [R=  -5.3] game over\n",
      "I0626 21:56:48.000572 12772 agents.py:69] training 12 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:48.004570 12772 agents.py:69] training 12 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:49.598378 12772 agents.py:69] training 12 of 50: completed tf_agent.train(...) = 306.974 [loss]\n",
      "I0626 21:56:49.599402 12772 agents.py:69] training 12 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:49.601380 12772 agents.py:69] training 13 of 50: executing collect_driver.run()\n",
      "I0626 21:56:49.602379 10184 logenv.py:75] #0 132.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:49.705259 10184 logenv.py:75] #0 133.201 [R=  -5.7] game over\n",
      "I0626 21:56:49.889451 10184 logenv.py:75] #0 134.201 [R=  -4.7] game over\n",
      "I0626 21:56:50.038456 10184 logenv.py:75] #0 135.201 [R=  -5.2] game over\n",
      "I0626 21:56:50.189990 10184 logenv.py:75] #0 136.201 [R=  -5.5] game over\n",
      "I0626 21:56:50.261771 10184 logenv.py:75] #0 137.95  [R=  -3.6] game over\n",
      "I0626 21:56:50.369908 10184 logenv.py:75] #0 138.184 [R=  -4.0] game over\n",
      "I0626 21:56:50.464089 10184 logenv.py:75] #0 139.153 [R=  -3.1] game over\n",
      "I0626 21:56:50.482752 10184 logenv.py:75] #0 140.21  [R=   0.0] game over\n",
      "I0626 21:56:50.549469 10184 logenv.py:75] #0 141.86  [R=  -1.9] game over\n",
      "I0626 21:56:50.693305 10184 logenv.py:75] #0 142.201 [R=  -5.2] game over\n",
      "I0626 21:56:50.700299 12772 agents.py:69] training 13 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:50.703303 12772 agents.py:69] training 13 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:52.244037 12772 agents.py:69] training 13 of 50: completed tf_agent.train(...) = 263.517 [loss]\n",
      "I0626 21:56:52.245408 12772 agents.py:69] training 13 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:52.247421 12772 agents.py:69] training 14 of 50: executing collect_driver.run()\n",
      "I0626 21:56:52.249419 15108 logenv.py:75] #0 143.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:52.329470 15108 logenv.py:75] #0 144.112 [R=  -2.1] game over\n",
      "I0626 21:56:52.389468 15108 logenv.py:75] #0 145.75  [R=  -2.1] game over\n",
      "I0626 21:56:52.542463 15108 logenv.py:75] #0 146.201 [R=  -5.2] game over\n",
      "I0626 21:56:52.661445 15108 logenv.py:75] #0 147.176 [R=  -3.1] game over\n",
      "I0626 21:56:52.782428 15108 logenv.py:75] #0 148.171 [R=  -3.9] game over\n",
      "I0626 21:56:52.833423 15108 logenv.py:75] #0 149.67  [R=  -0.7] game over\n",
      "I0626 21:56:52.895537 15108 logenv.py:75] #0 150.67  [R=  -0.8] game over\n",
      "I0626 21:56:52.982537 15108 logenv.py:75] #0 151.132 [R=  -3.1] game over\n",
      "I0626 21:56:53.093801 15108 logenv.py:75] #0 152.103 [R=  -1.5] game over\n",
      "I0626 21:56:53.142673 15108 logenv.py:75] #0 153.61  [R=  -1.2] game over\n",
      "I0626 21:56:53.150675 12772 agents.py:69] training 14 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:53.153532 12772 agents.py:69] training 14 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:54.749450 12772 agents.py:69] training 14 of 50: completed tf_agent.train(...) = 151.610 [loss]\n",
      "I0626 21:56:54.751450 12772 agents.py:69] training 14 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:54.753451 12772 agents.py:69] training 15 of 50: executing collect_driver.run()\n",
      "I0626 21:56:54.754452 17500 logenv.py:75] #0 154.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:54.851453 17500 logenv.py:75] #0 155.185 [R=  -3.8] game over\n",
      "I0626 21:56:54.909582 17500 logenv.py:75] #0 156.65  [R=  -1.0] game over\n",
      "I0626 21:56:55.001600 17500 logenv.py:75] #0 157.94  [R=  -1.6] game over\n",
      "I0626 21:56:55.024564 17500 logenv.py:75] #0 158.29  [R=   0.3] game over\n",
      "I0626 21:56:55.122643 17500 logenv.py:75] #0 159.74  [R=  -1.5] game over\n",
      "I0626 21:56:55.186642 17500 logenv.py:75] #0 160.83  [R=  -1.2] game over\n",
      "I0626 21:56:55.243646 17500 logenv.py:75] #0 161.69  [R=  -0.5] game over\n",
      "I0626 21:56:55.359723 17500 logenv.py:75] #0 162.145 [R=  -3.1] game over\n",
      "I0626 21:56:55.379069 17500 logenv.py:75] #0 163.16  [R=   0.7] game over\n",
      "I0626 21:56:55.476741 17500 logenv.py:75] #0 164.166 [R=  -3.0] game over\n",
      "I0626 21:56:55.482722 12772 agents.py:69] training 15 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:55.485759 12772 agents.py:69] training 15 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:56.979094 12772 agents.py:69] training 15 of 50: completed tf_agent.train(...) = 193.721 [loss]\n",
      "I0626 21:56:56.980595 12772 agents.py:69] training 15 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:56.981604 12772 agents.py:69] training 16 of 50: executing collect_driver.run()\n",
      "I0626 21:56:56.983608 15108 logenv.py:75] #0 165.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:57.034826 15108 logenv.py:75] #0 166.61  [R=  -0.7] game over\n",
      "I0626 21:56:57.098491 15108 logenv.py:75] #0 167.84  [R=  -1.3] game over\n",
      "I0626 21:56:57.132920 15108 logenv.py:75] #0 168.48  [R=  -0.2] game over\n",
      "I0626 21:56:57.205643 15108 logenv.py:75] #0 169.80  [R=  -1.2] game over\n",
      "I0626 21:56:57.323237 15108 logenv.py:75] #0 170.142 [R=  -1.9] game over\n",
      "I0626 21:56:57.398790 15108 logenv.py:75] #0 171.107 [R=  -3.5] game over\n",
      "I0626 21:56:57.451921 15108 logenv.py:75] #0 172.78  [R=  -1.4] game over\n",
      "I0626 21:56:57.473117 15108 logenv.py:75] #0 173.29  [R=  -0.1] game over\n",
      "I0626 21:56:57.510277 15108 logenv.py:75] #0 174.60  [R=  -0.6] game over\n",
      "I0626 21:56:57.538773 15108 logenv.py:75] #0 175.35  [R=   0.4] game over\n",
      "I0626 21:56:57.544776 12772 agents.py:69] training 16 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:57.548241 12772 agents.py:69] training 16 of 50: executing tf_agent.train(...)\n",
      "I0626 21:56:59.217693 12772 agents.py:69] training 16 of 50: completed tf_agent.train(...) = 143.094 [loss]\n",
      "I0626 21:56:59.218616 12772 agents.py:69] training 16 of 50: executing replay_buffer.clear()\n",
      "I0626 21:56:59.220617 12772 agents.py:69] training 17 of 50: executing collect_driver.run()\n",
      "I0626 21:56:59.221784 17500 logenv.py:75] #0 176.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:56:59.264616 17500 logenv.py:75] #0 177.46  [R=  -0.2] game over\n",
      "I0626 21:56:59.378694 17500 logenv.py:75] #0 178.186 [R=  -3.9] game over\n",
      "I0626 21:56:59.421720 17500 logenv.py:75] #0 179.50  [R=  -1.0] game over\n",
      "I0626 21:56:59.473362 17500 logenv.py:75] #0 180.58  [R=  -1.4] game over\n",
      "I0626 21:56:59.507364 17500 logenv.py:75] #0 181.47  [R=   0.1] game over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0626 21:56:59.548985 17500 logenv.py:75] #0 182.57  [R=  -0.3] game over\n",
      "I0626 21:56:59.585175 17500 logenv.py:75] #0 183.53  [R=  -0.3] game over\n",
      "I0626 21:56:59.625078 17500 logenv.py:75] #0 184.55  [R=  -0.5] game over\n",
      "I0626 21:56:59.679160 17500 logenv.py:75] #0 185.81  [R=  -1.9] game over\n",
      "I0626 21:56:59.711414 17500 logenv.py:75] #0 186.47  [R=  -0.1] game over\n",
      "I0626 21:56:59.717171 12772 agents.py:69] training 17 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:56:59.721122 12772 agents.py:69] training 17 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:01.216181 12772 agents.py:69] training 17 of 50: completed tf_agent.train(...) = 176.472 [loss]\n",
      "I0626 21:57:01.218185 12772 agents.py:69] training 17 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:01.220180 12772 agents.py:69] training 18 of 50: executing collect_driver.run()\n",
      "I0626 21:57:01.222183 10184 logenv.py:75] #0 187.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:01.280802 10184 logenv.py:75] #0 188.54  [R=  -0.5] game over\n",
      "I0626 21:57:01.319988 10184 logenv.py:75] #0 189.45  [R=  -0.3] game over\n",
      "I0626 21:57:01.349374 10184 logenv.py:75] #0 190.38  [R=   0.2] game over\n",
      "I0626 21:57:01.385167 10184 logenv.py:75] #0 191.46  [R=  -0.5] game over\n",
      "I0626 21:57:01.420133 10184 logenv.py:75] #0 192.33  [R=   0.2] game over\n",
      "I0626 21:57:01.482798 10184 logenv.py:75] #0 193.52  [R=  -0.3] game over\n",
      "I0626 21:57:01.611793 10184 logenv.py:75] #0 194.188 [R=  -3.6] game over\n",
      "I0626 21:57:01.634850 10184 logenv.py:75] #0 195.33  [R=   0.3] game over\n",
      "I0626 21:57:01.656305 10184 logenv.py:75] #0 196.25  [R=  -0.0] game over\n",
      "I0626 21:57:01.728191 10184 logenv.py:75] #0 197.46  [R=  -0.3] game over\n",
      "I0626 21:57:01.738198 12772 agents.py:69] training 18 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:01.742351 12772 agents.py:69] training 18 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:03.128556 12772 agents.py:69] training 18 of 50: completed tf_agent.train(...) = 197.008 [loss]\n",
      "I0626 21:57:03.129470 12772 agents.py:69] training 18 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:03.131469 12772 agents.py:69] training 19 of 50: executing collect_driver.run()\n",
      "I0626 21:57:03.132485 14320 logenv.py:75] #0 198.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:03.150470 14320 logenv.py:75] #0 199.20  [R=   0.4] game over\n",
      "I0626 21:57:03.172680 14320 logenv.py:75] #0 200.28  [R=  -0.0] game over\n",
      "I0626 21:57:03.200566 14320 logenv.py:75] #0 201.36  [R=   0.3] game over\n",
      "I0626 21:57:03.266603 14320 logenv.py:75] #0 202.81  [R=  -0.4] game over\n",
      "I0626 21:57:03.446492 14320 logenv.py:75] #0 203.148 [R=  -2.1] game over\n",
      "I0626 21:57:03.472488 14320 logenv.py:75] #0 204.41  [R=  -1.0] game over\n",
      "I0626 21:57:03.555452 14320 logenv.py:75] #0 205.116 [R=  -2.1] game over\n",
      "I0626 21:57:03.587435 14320 logenv.py:75] #0 206.40  [R=  -0.1] game over\n",
      "I0626 21:57:03.626439 14320 logenv.py:75] #0 207.39  [R=   0.1] game over\n",
      "I0626 21:57:03.695525 14320 logenv.py:75] #0 208.100 [R=  -1.7] game over\n",
      "I0626 21:57:03.701527 12772 agents.py:69] training 19 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:03.703524 12772 agents.py:69] training 19 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:05.252166 12772 agents.py:69] training 19 of 50: completed tf_agent.train(...) = 102.973 [loss]\n",
      "I0626 21:57:05.253253 12772 agents.py:69] training 19 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:05.255166 12772 agents.py:69] training 20 of 50: executing collect_driver.run()\n",
      "I0626 21:57:05.256169 14320 logenv.py:75] #0 209.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:05.307164 14320 logenv.py:75] #0 210.68  [R=  -0.8] game over\n",
      "I0626 21:57:05.339761 14320 logenv.py:75] #0 211.40  [R=   0.3] game over\n",
      "I0626 21:57:05.404396 14320 logenv.py:75] #0 212.77  [R=  -0.4] game over\n",
      "I0626 21:57:05.441296 14320 logenv.py:75] #0 213.40  [R=  -0.0] game over\n",
      "I0626 21:57:05.463764 14320 logenv.py:75] #0 214.21  [R=   0.6] game over\n",
      "I0626 21:57:05.496257 14320 logenv.py:75] #0 215.39  [R=   0.3] game over\n",
      "I0626 21:57:05.534999 14320 logenv.py:75] #0 216.40  [R=   0.2] game over\n",
      "I0626 21:57:05.619764 14320 logenv.py:75] #0 217.113 [R=  -1.9] game over\n",
      "I0626 21:57:05.651085 14320 logenv.py:75] #0 218.38  [R=  -0.4] game over\n",
      "I0626 21:57:05.692191 14320 logenv.py:75] #0 219.50  [R=   0.0] game over\n",
      "I0626 21:57:05.698188 12772 agents.py:69] training 20 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:05.701186 12772 agents.py:69] training 20 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:07.165365 12772 agents.py:69] training 20 of 50: completed tf_agent.train(...) = 63.626 [loss]\n",
      "I0626 21:57:07.166323 12772 agents.py:69] training 20 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:07.168323 12772 agents.py:69] executing compute_avg_return(...)\n",
      "I0626 21:57:08.355747 12772 logenv.py:75] #1  21.201 [R=  -3.2] game over\n",
      "I0626 21:57:09.536494 12772 logenv.py:75] #1  22.201 [R=  -3.0] game over\n",
      "I0626 21:57:10.965881 12772 logenv.py:75] #1  23.201 [R=  -2.4] game over\n",
      "I0626 21:57:11.958222 12772 logenv.py:75] #1  24.201 [R=  -2.6] game over\n",
      "I0626 21:57:12.853158 12772 logenv.py:75] #1  25.201 [R=  -3.0] game over\n",
      "I0626 21:57:13.795655 12772 logenv.py:75] #1  26.201 [R=  -2.4] game over\n",
      "I0626 21:57:15.303241 12772 logenv.py:75] #1  27.201 [R=  -1.4] game over\n",
      "I0626 21:57:16.746091 12772 logenv.py:75] #1  28.201 [R=  -2.8] game over\n",
      "I0626 21:57:18.128230 12772 logenv.py:75] #1  29.201 [R=  -2.5] game over\n",
      "I0626 21:57:19.436938 12772 logenv.py:75] #1  30.201 [R=  -2.5] game over\n",
      "I0626 21:57:19.438276 12772 agents.py:69] completed compute_avg_return(...) = -2.698\n",
      "I0626 21:57:19.438972 12772 agents.py:69] training 21 of 50: executing collect_driver.run()\n",
      "I0626 21:57:19.440944 14320 logenv.py:75] #0 220.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:19.462835 14320 logenv.py:75] #0 221.31  [R=   0.5] game over\n",
      "I0626 21:57:19.491864 14320 logenv.py:75] #0 222.46  [R=   0.1] game over\n",
      "I0626 21:57:19.517367 14320 logenv.py:75] #0 223.19  [R=   0.6] game over\n",
      "I0626 21:57:19.571097 14320 logenv.py:75] #0 224.81  [R=  -0.9] game over\n",
      "I0626 21:57:19.612601 14320 logenv.py:75] #0 225.69  [R=  -0.9] game over\n",
      "I0626 21:57:19.630545 14320 logenv.py:75] #0 226.24  [R=   0.5] game over\n",
      "I0626 21:57:19.663435 14320 logenv.py:75] #0 227.48  [R=   0.0] game over\n",
      "I0626 21:57:19.681289 14320 logenv.py:75] #0 228.22  [R=   0.2] game over\n",
      "I0626 21:57:19.748851 14320 logenv.py:75] #0 229.87  [R=  -0.7] game over\n",
      "I0626 21:57:19.771794 14320 logenv.py:75] #0 230.26  [R=   0.3] game over\n",
      "I0626 21:57:19.778802 12772 agents.py:69] training 21 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:19.783799 12772 agents.py:69] training 21 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:21.088847 12772 agents.py:69] training 21 of 50: completed tf_agent.train(...) = 68.167 [loss]\n",
      "I0626 21:57:21.090194 12772 agents.py:69] training 21 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:21.091782 12772 agents.py:69] training 22 of 50: executing collect_driver.run()\n",
      "I0626 21:57:21.092676  7092 logenv.py:75] #0 231.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:21.120467  7092 logenv.py:75] #0 232.31  [R=   0.3] game over\n",
      "I0626 21:57:21.137443  7092 logenv.py:75] #0 233.14  [R=   0.7] game over\n",
      "I0626 21:57:21.167483  7092 logenv.py:75] #0 234.42  [R=   0.3] game over\n",
      "I0626 21:57:21.199788  7092 logenv.py:75] #0 235.47  [R=  -0.0] game over\n",
      "I0626 21:57:21.219636  7092 logenv.py:75] #0 236.24  [R=   0.5] game over\n",
      "I0626 21:57:21.259048  7092 logenv.py:75] #0 237.50  [R=  -0.0] game over\n",
      "I0626 21:57:21.310068  7092 logenv.py:75] #0 238.62  [R=  -0.4] game over\n",
      "I0626 21:57:21.343962  7092 logenv.py:75] #0 239.38  [R=   0.2] game over\n",
      "I0626 21:57:21.379996  7092 logenv.py:75] #0 240.33  [R=   0.4] game over\n",
      "I0626 21:57:21.398267  7092 logenv.py:75] #0 241.20  [R=   0.5] game over\n",
      "I0626 21:57:21.405070 12772 agents.py:69] training 22 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:21.407106 12772 agents.py:69] training 22 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:22.678739 12772 agents.py:69] training 22 of 50: completed tf_agent.train(...) = 67.392 [loss]\n",
      "I0626 21:57:22.678739 12772 agents.py:69] training 22 of 50: executing replay_buffer.clear()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0626 21:57:22.680741 12772 agents.py:69] training 23 of 50: executing collect_driver.run()\n",
      "I0626 21:57:22.682906 10184 logenv.py:75] #0 242.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:22.709786 10184 logenv.py:75] #0 243.31  [R=   0.4] game over\n",
      "I0626 21:57:22.731740 10184 logenv.py:75] #0 244.24  [R=   0.4] game over\n",
      "I0626 21:57:22.761740 10184 logenv.py:75] #0 245.40  [R=  -0.6] game over\n",
      "I0626 21:57:22.784744 10184 logenv.py:75] #0 246.30  [R=   0.2] game over\n",
      "I0626 21:57:22.817740 10184 logenv.py:75] #0 247.37  [R=   0.2] game over\n",
      "I0626 21:57:22.850165 10184 logenv.py:75] #0 248.44  [R=   0.0] game over\n",
      "I0626 21:57:22.868094 10184 logenv.py:75] #0 249.20  [R=   0.6] game over\n",
      "I0626 21:57:22.913755 10184 logenv.py:75] #0 250.61  [R=  -0.2] game over\n",
      "I0626 21:57:22.941400 10184 logenv.py:75] #0 251.35  [R=   0.4] game over\n",
      "I0626 21:57:22.971271 10184 logenv.py:75] #0 252.35  [R=  -0.1] game over\n",
      "I0626 21:57:22.977359 12772 agents.py:69] training 23 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:22.980360 12772 agents.py:69] training 23 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:24.477582 12772 agents.py:69] training 23 of 50: completed tf_agent.train(...) = 45.343 [loss]\n",
      "I0626 21:57:24.478583 12772 agents.py:69] training 23 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:24.479584 12772 agents.py:69] training 24 of 50: executing collect_driver.run()\n",
      "I0626 21:57:24.480654  7092 logenv.py:75] #0 253.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:24.499042  7092 logenv.py:75] #0 254.14  [R=   0.7] game over\n",
      "I0626 21:57:24.520935  7092 logenv.py:75] #0 255.32  [R=   0.4] game over\n",
      "I0626 21:57:24.539901  7092 logenv.py:75] #0 256.21  [R=   0.4] game over\n",
      "I0626 21:57:24.561769  7092 logenv.py:75] #0 257.28  [R=   0.4] game over\n",
      "I0626 21:57:24.581996  7092 logenv.py:75] #0 258.24  [R=   0.4] game over\n",
      "I0626 21:57:24.602417  7092 logenv.py:75] #0 259.24  [R=   0.4] game over\n",
      "I0626 21:57:24.630598  7092 logenv.py:75] #0 260.37  [R=   0.3] game over\n",
      "I0626 21:57:24.652724  7092 logenv.py:75] #0 261.24  [R=   0.5] game over\n",
      "I0626 21:57:24.685659  7092 logenv.py:75] #0 262.47  [R=  -0.1] game over\n",
      "I0626 21:57:24.794649  7092 logenv.py:75] #0 263.156 [R=  -2.0] game over\n",
      "I0626 21:57:24.801660 12772 agents.py:69] training 24 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:24.805997 12772 agents.py:69] training 24 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:26.156307 12772 agents.py:69] training 24 of 50: completed tf_agent.train(...) = 110.269 [loss]\n",
      "I0626 21:57:26.157959 12772 agents.py:69] training 24 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:26.159970 12772 agents.py:69] training 25 of 50: executing collect_driver.run()\n",
      "I0626 21:57:26.160959  5292 logenv.py:75] #0 264.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:26.198020  5292 logenv.py:75] #0 265.51  [R=  -0.1] game over\n",
      "I0626 21:57:26.227453  5292 logenv.py:75] #0 266.42  [R=  -0.0] game over\n",
      "I0626 21:57:26.247451  5292 logenv.py:75] #0 267.25  [R=   0.5] game over\n",
      "I0626 21:57:26.270629  5292 logenv.py:75] #0 268.27  [R=   0.4] game over\n",
      "I0626 21:57:26.299009  5292 logenv.py:75] #0 269.40  [R=   0.2] game over\n",
      "I0626 21:57:26.319649  5292 logenv.py:75] #0 270.24  [R=   0.5] game over\n",
      "I0626 21:57:26.354250  5292 logenv.py:75] #0 271.56  [R=   0.0] game over\n",
      "I0626 21:57:26.397074  5292 logenv.py:75] #0 272.60  [R=  -0.6] game over\n",
      "I0626 21:57:26.445339  5292 logenv.py:75] #0 273.45  [R=   0.3] game over\n",
      "I0626 21:57:26.508634  5292 logenv.py:75] #0 274.70  [R=  -0.4] game over\n",
      "I0626 21:57:26.516505 12772 agents.py:69] training 25 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:26.520356 12772 agents.py:69] training 25 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:27.801517 12772 agents.py:69] training 25 of 50: completed tf_agent.train(...) = 35.513 [loss]\n",
      "I0626 21:57:27.802471 12772 agents.py:69] training 25 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:27.803655 12772 agents.py:69] training 26 of 50: executing collect_driver.run()\n",
      "I0626 21:57:27.805473 14320 logenv.py:75] #0 275.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:27.827471 14320 logenv.py:75] #0 276.23  [R=   0.5] game over\n",
      "I0626 21:57:27.888516 14320 logenv.py:75] #0 277.95  [R=  -0.9] game over\n",
      "I0626 21:57:27.911478 14320 logenv.py:75] #0 278.31  [R=   0.2] game over\n",
      "I0626 21:57:28.021368 14320 logenv.py:75] #0 279.170 [R=  -2.0] game over\n",
      "I0626 21:57:28.069593 14320 logenv.py:75] #0 280.82  [R=  -0.8] game over\n",
      "I0626 21:57:28.103283 14320 logenv.py:75] #0 281.42  [R=   0.0] game over\n",
      "I0626 21:57:28.191057 14320 logenv.py:75] #0 282.140 [R=  -1.5] game over\n",
      "I0626 21:57:28.211892 14320 logenv.py:75] #0 283.27  [R=   0.5] game over\n",
      "I0626 21:57:28.229891 14320 logenv.py:75] #0 284.16  [R=   0.7] game over\n",
      "I0626 21:57:28.290892 14320 logenv.py:75] #0 285.75  [R=  -0.4] game over\n",
      "I0626 21:57:28.298464 12772 agents.py:69] training 26 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:28.300896 12772 agents.py:69] training 26 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:29.872875 12772 agents.py:69] training 26 of 50: completed tf_agent.train(...) = 88.979 [loss]\n",
      "I0626 21:57:29.873875 12772 agents.py:69] training 26 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:29.874875 12772 agents.py:69] training 27 of 50: executing collect_driver.run()\n",
      "I0626 21:57:29.876148 15108 logenv.py:75] #0 286.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:29.894296 15108 logenv.py:75] #0 287.21  [R=   0.4] game over\n",
      "I0626 21:57:29.924319 15108 logenv.py:75] #0 288.50  [R=   0.0] game over\n",
      "I0626 21:57:29.954320 15108 logenv.py:75] #0 289.34  [R=   0.3] game over\n",
      "I0626 21:57:29.970518 15108 logenv.py:75] #0 290.20  [R=   0.5] game over\n",
      "I0626 21:57:30.011330 15108 logenv.py:75] #0 291.53  [R=  -0.0] game over\n",
      "I0626 21:57:30.037440 15108 logenv.py:75] #0 292.29  [R=   0.4] game over\n",
      "I0626 21:57:30.062442 15108 logenv.py:75] #0 293.31  [R=  -0.0] game over\n",
      "I0626 21:57:30.091013 15108 logenv.py:75] #0 294.33  [R=   0.4] game over\n",
      "I0626 21:57:30.126481 15108 logenv.py:75] #0 295.54  [R=  -0.2] game over\n",
      "I0626 21:57:30.158513 15108 logenv.py:75] #0 296.37  [R=   0.2] game over\n",
      "I0626 21:57:30.165522 12772 agents.py:69] training 27 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:30.168516 12772 agents.py:69] training 27 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:31.667816 12772 agents.py:69] training 27 of 50: completed tf_agent.train(...) = 28.719 [loss]\n",
      "I0626 21:57:31.669295 12772 agents.py:69] training 27 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:31.670814 12772 agents.py:69] training 28 of 50: executing collect_driver.run()\n",
      "I0626 21:57:31.672812  7092 logenv.py:75] #0 297.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:31.699185  7092 logenv.py:75] #0 298.27  [R=   0.2] game over\n",
      "I0626 21:57:31.725189  7092 logenv.py:75] #0 299.37  [R=   0.4] game over\n",
      "I0626 21:57:31.747044  7092 logenv.py:75] #0 300.29  [R=   0.4] game over\n",
      "I0626 21:57:31.760266  7092 logenv.py:75] #0 301.10  [R=   0.8] game over\n",
      "I0626 21:57:31.779111  7092 logenv.py:75] #0 302.20  [R=   0.5] game over\n",
      "I0626 21:57:31.804226  7092 logenv.py:75] #0 303.29  [R=   0.3] game over\n",
      "I0626 21:57:31.834199  7092 logenv.py:75] #0 304.34  [R=  -0.1] game over\n",
      "I0626 21:57:31.871320  7092 logenv.py:75] #0 305.42  [R=   0.1] game over\n",
      "I0626 21:57:31.908150  7092 logenv.py:75] #0 306.45  [R=   0.1] game over\n",
      "I0626 21:57:31.956486  7092 logenv.py:75] #0 307.55  [R=   0.1] game over\n",
      "I0626 21:57:31.963523 12772 agents.py:69] training 28 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:31.966490 12772 agents.py:69] training 28 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:33.359369 12772 agents.py:69] training 28 of 50: completed tf_agent.train(...) = 34.989 [loss]\n",
      "I0626 21:57:33.361154 12772 agents.py:69] training 28 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:33.362152 12772 agents.py:69] training 29 of 50: executing collect_driver.run()\n",
      "I0626 21:57:33.364204  5292 logenv.py:75] #0 308.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:33.386029  5292 logenv.py:75] #0 309.13  [R=   0.7] game over\n",
      "I0626 21:57:33.407588  5292 logenv.py:75] #0 310.27  [R=   0.5] game over\n",
      "I0626 21:57:33.426856  5292 logenv.py:75] #0 311.21  [R=   0.6] game over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0626 21:57:33.467038  5292 logenv.py:75] #0 312.40  [R=   0.1] game over\n",
      "I0626 21:57:33.491038  5292 logenv.py:75] #0 313.19  [R=   0.5] game over\n",
      "I0626 21:57:33.507117  5292 logenv.py:75] #0 314.18  [R=   0.6] game over\n",
      "I0626 21:57:33.542924  5292 logenv.py:75] #0 315.37  [R=   0.1] game over\n",
      "I0626 21:57:33.560215  5292 logenv.py:75] #0 316.15  [R=   0.6] game over\n",
      "I0626 21:57:33.580464  5292 logenv.py:75] #0 317.21  [R=   0.6] game over\n",
      "I0626 21:57:33.618464  5292 logenv.py:75] #0 318.29  [R=   0.4] game over\n",
      "I0626 21:57:33.626199 12772 agents.py:69] training 29 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:33.630106 12772 agents.py:69] training 29 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:35.409479 12772 agents.py:69] training 29 of 50: completed tf_agent.train(...) = 52.842 [loss]\n",
      "I0626 21:57:35.410477 12772 agents.py:69] training 29 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:35.411477 12772 agents.py:69] training 30 of 50: executing collect_driver.run()\n",
      "I0626 21:57:35.413476 18188 logenv.py:75] #0 319.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:35.428483 18188 logenv.py:75] #0 320.15  [R=   0.7] game over\n",
      "I0626 21:57:35.455197 18188 logenv.py:75] #0 321.31  [R=   0.4] game over\n",
      "I0626 21:57:35.474711 18188 logenv.py:75] #0 322.18  [R=   0.6] game over\n",
      "I0626 21:57:35.493350 18188 logenv.py:75] #0 323.17  [R=   0.6] game over\n",
      "I0626 21:57:35.510293 18188 logenv.py:75] #0 324.13  [R=   0.7] game over\n",
      "I0626 21:57:35.557065 18188 logenv.py:75] #0 325.60  [R=  -0.1] game over\n",
      "I0626 21:57:35.584884 18188 logenv.py:75] #0 326.35  [R=   0.2] game over\n",
      "I0626 21:57:35.610820 18188 logenv.py:75] #0 327.25  [R=   0.5] game over\n",
      "I0626 21:57:35.643684 18188 logenv.py:75] #0 328.41  [R=   0.1] game over\n",
      "I0626 21:57:35.714729 18188 logenv.py:75] #0 329.97  [R=  -0.8] game over\n",
      "I0626 21:57:35.722262 12772 agents.py:69] training 30 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:35.724325 12772 agents.py:69] training 30 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:37.028866 12772 agents.py:69] training 30 of 50: completed tf_agent.train(...) = 51.497 [loss]\n",
      "I0626 21:57:37.029602 12772 agents.py:69] training 30 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:37.031604 12772 agents.py:69] executing compute_avg_return(...)\n",
      "I0626 21:57:38.288186 12772 logenv.py:75] #1  31.201 [R=  -2.6] game over\n",
      "I0626 21:57:38.338422 12772 logenv.py:75] #1  32.11  [R=   0.8] game over\n",
      "I0626 21:57:38.418778 12772 logenv.py:75] #1  33.17  [R=   0.7] game over\n",
      "I0626 21:57:39.278717 12772 logenv.py:75] #1  34.201 [R=  -3.2] game over\n",
      "I0626 21:57:40.242601 12772 logenv.py:75] #1  35.201 [R=  -1.3] game over\n",
      "I0626 21:57:41.096566 12772 logenv.py:75] #1  36.201 [R=  -2.8] game over\n",
      "I0626 21:57:42.656546 12772 logenv.py:75] #1  37.201 [R=  -1.0] game over\n",
      "I0626 21:57:43.579228 12772 logenv.py:75] #1  38.201 [R=  -1.0] game over\n",
      "I0626 21:57:43.668387 12772 logenv.py:75] #1  39.11  [R=   0.7] game over\n",
      "I0626 21:57:45.091537 12772 logenv.py:75] #1  40.201 [R=  -3.2] game over\n",
      "I0626 21:57:45.093505 12772 agents.py:69] completed compute_avg_return(...) = -1.401\n",
      "I0626 21:57:45.093505 12772 agents.py:69] training 31 of 50: executing collect_driver.run()\n",
      "I0626 21:57:45.095591 14320 logenv.py:75] #0 330.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:45.112553 14320 logenv.py:75] #0 331.13  [R=   0.7] game over\n",
      "I0626 21:57:45.147439 14320 logenv.py:75] #0 332.37  [R=   0.1] game over\n",
      "I0626 21:57:45.187537 14320 logenv.py:75] #0 333.48  [R=   0.2] game over\n",
      "I0626 21:57:45.299084 14320 logenv.py:75] #0 334.130 [R=  -1.2] game over\n",
      "I0626 21:57:45.314059 14320 logenv.py:75] #0 335.14  [R=   0.7] game over\n",
      "I0626 21:57:45.344734 14320 logenv.py:75] #0 336.39  [R=   0.2] game over\n",
      "I0626 21:57:45.367977 14320 logenv.py:75] #0 337.27  [R=   0.3] game over\n",
      "I0626 21:57:45.383595 14320 logenv.py:75] #0 338.17  [R=   0.6] game over\n",
      "I0626 21:57:45.396917 14320 logenv.py:75] #0 339.13  [R=   0.7] game over\n",
      "I0626 21:57:45.445301 14320 logenv.py:75] #0 340.81  [R=  -0.6] game over\n",
      "I0626 21:57:45.451679 12772 agents.py:69] training 31 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:45.455703 12772 agents.py:69] training 31 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:46.874381 12772 agents.py:69] training 31 of 50: completed tf_agent.train(...) = 80.151 [loss]\n",
      "I0626 21:57:46.876377 12772 agents.py:69] training 31 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:46.878382 12772 agents.py:69] training 32 of 50: executing collect_driver.run()\n",
      "I0626 21:57:46.879475  5292 logenv.py:75] #0 341.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:46.910223  5292 logenv.py:75] #0 342.32  [R=   0.4] game over\n",
      "I0626 21:57:46.941679  5292 logenv.py:75] #0 343.48  [R=   0.2] game over\n",
      "I0626 21:57:46.956738  5292 logenv.py:75] #0 344.17  [R=   0.6] game over\n",
      "I0626 21:57:46.981517  5292 logenv.py:75] #0 345.36  [R=   0.3] game over\n",
      "I0626 21:57:47.008979  5292 logenv.py:75] #0 346.40  [R=   0.2] game over\n",
      "I0626 21:57:47.031877  5292 logenv.py:75] #0 347.33  [R=   0.3] game over\n",
      "I0626 21:57:47.052142  5292 logenv.py:75] #0 348.21  [R=   0.6] game over\n",
      "I0626 21:57:47.080657  5292 logenv.py:75] #0 349.27  [R=   0.4] game over\n",
      "I0626 21:57:47.115008  5292 logenv.py:75] #0 350.42  [R=   0.3] game over\n",
      "I0626 21:57:47.146952  5292 logenv.py:75] #0 351.44  [R=   0.2] game over\n",
      "I0626 21:57:47.153680 12772 agents.py:69] training 32 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:47.155724 12772 agents.py:69] training 32 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:48.507788 12772 agents.py:69] training 32 of 50: completed tf_agent.train(...) = 33.438 [loss]\n",
      "I0626 21:57:48.508887 12772 agents.py:69] training 32 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:48.509795 12772 agents.py:69] training 33 of 50: executing collect_driver.run()\n",
      "I0626 21:57:48.511789 17500 logenv.py:75] #0 352.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:48.531819 17500 logenv.py:75] #0 353.23  [R=   0.6] game over\n",
      "I0626 21:57:48.558841 17500 logenv.py:75] #0 354.34  [R=   0.4] game over\n",
      "I0626 21:57:48.572950 17500 logenv.py:75] #0 355.17  [R=   0.6] game over\n",
      "I0626 21:57:48.588798 17500 logenv.py:75] #0 356.16  [R=   0.6] game over\n",
      "I0626 21:57:48.611225 17500 logenv.py:75] #0 357.35  [R=   0.3] game over\n",
      "I0626 21:57:48.636257 17500 logenv.py:75] #0 358.29  [R=   0.4] game over\n",
      "I0626 21:57:48.661264 17500 logenv.py:75] #0 359.36  [R=   0.3] game over\n",
      "I0626 21:57:48.688167 17500 logenv.py:75] #0 360.44  [R=   0.1] game over\n",
      "I0626 21:57:48.701500 17500 logenv.py:75] #0 361.12  [R=   0.7] game over\n",
      "I0626 21:57:48.724468 17500 logenv.py:75] #0 362.31  [R=   0.4] game over\n",
      "I0626 21:57:48.731452 12772 agents.py:69] training 33 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:48.733455 12772 agents.py:69] training 33 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:50.139668 12772 agents.py:69] training 33 of 50: completed tf_agent.train(...) = 34.619 [loss]\n",
      "I0626 21:57:50.140671 12772 agents.py:69] training 33 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:50.141668 12772 agents.py:69] training 34 of 50: executing collect_driver.run()\n",
      "I0626 21:57:50.143670 15108 logenv.py:75] #0 363.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:50.162756 15108 logenv.py:75] #0 364.28  [R=   0.4] game over\n",
      "I0626 21:57:50.190937 15108 logenv.py:75] #0 365.28  [R=   0.5] game over\n",
      "I0626 21:57:50.248668 15108 logenv.py:75] #0 366.72  [R=  -0.4] game over\n",
      "I0626 21:57:50.279820 15108 logenv.py:75] #0 367.36  [R=   0.3] game over\n",
      "I0626 21:57:50.314669 15108 logenv.py:75] #0 368.38  [R=   0.3] game over\n",
      "I0626 21:57:50.335673 15108 logenv.py:75] #0 369.19  [R=   0.6] game over\n",
      "I0626 21:57:50.363671 15108 logenv.py:75] #0 370.32  [R=   0.4] game over\n",
      "I0626 21:57:50.392677 15108 logenv.py:75] #0 371.28  [R=   0.4] game over\n",
      "I0626 21:57:50.453141 15108 logenv.py:75] #0 372.87  [R=  -0.5] game over\n",
      "I0626 21:57:50.484193 15108 logenv.py:75] #0 373.43  [R=   0.2] game over\n",
      "I0626 21:57:50.490079 12772 agents.py:69] training 34 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:50.493079 12772 agents.py:69] training 34 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:51.908434 12772 agents.py:69] training 34 of 50: completed tf_agent.train(...) = 35.057 [loss]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0626 21:57:51.908434 12772 agents.py:69] training 34 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:51.910434 12772 agents.py:69] training 35 of 50: executing collect_driver.run()\n",
      "I0626 21:57:51.911440 10184 logenv.py:75] #0 374.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:51.947437 10184 logenv.py:75] #0 375.53  [R=  -0.2] game over\n",
      "I0626 21:57:51.961589 10184 logenv.py:75] #0 376.18  [R=   0.6] game over\n",
      "I0626 21:57:51.975592 10184 logenv.py:75] #0 377.15  [R=   0.7] game over\n",
      "I0626 21:57:52.007985 10184 logenv.py:75] #0 378.49  [R=  -0.0] game over\n",
      "I0626 21:57:52.053587 10184 logenv.py:75] #0 379.44  [R=  -0.1] game over\n",
      "I0626 21:57:52.112339 10184 logenv.py:75] #0 380.80  [R=  -0.4] game over\n",
      "I0626 21:57:52.130111 10184 logenv.py:75] #0 381.18  [R=   0.6] game over\n",
      "I0626 21:57:52.154790 10184 logenv.py:75] #0 382.31  [R=   0.5] game over\n",
      "I0626 21:57:52.175053 10184 logenv.py:75] #0 383.22  [R=   0.6] game over\n",
      "I0626 21:57:52.194803 10184 logenv.py:75] #0 384.19  [R=   0.6] game over\n",
      "I0626 21:57:52.200786 12772 agents.py:69] training 35 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:52.203481 12772 agents.py:69] training 35 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:53.463712 12772 agents.py:69] training 35 of 50: completed tf_agent.train(...) = 37.506 [loss]\n",
      "I0626 21:57:53.464679 12772 agents.py:69] training 35 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:53.465682 12772 agents.py:69] training 36 of 50: executing collect_driver.run()\n",
      "I0626 21:57:53.467681 14320 logenv.py:75] #0 385.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:53.484066 14320 logenv.py:75] #0 386.19  [R=   0.7] game over\n",
      "I0626 21:57:53.508064 14320 logenv.py:75] #0 387.29  [R=   0.4] game over\n",
      "I0626 21:57:53.540066 14320 logenv.py:75] #0 388.42  [R=   0.2] game over\n",
      "I0626 21:57:53.596066 14320 logenv.py:75] #0 389.87  [R=  -0.6] game over\n",
      "I0626 21:57:53.614065 14320 logenv.py:75] #0 390.22  [R=   0.5] game over\n",
      "I0626 21:57:53.627069 14320 logenv.py:75] #0 391.10  [R=   0.8] game over\n",
      "I0626 21:57:53.734682 14320 logenv.py:75] #0 392.133 [R=  -1.5] game over\n",
      "I0626 21:57:53.764683 14320 logenv.py:75] #0 393.43  [R=   0.2] game over\n",
      "I0626 21:57:53.783001 14320 logenv.py:75] #0 394.19  [R=   0.6] game over\n",
      "I0626 21:57:53.797032 14320 logenv.py:75] #0 395.13  [R=   0.7] game over\n",
      "I0626 21:57:53.804198 12772 agents.py:69] training 36 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:53.807000 12772 agents.py:69] training 36 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:55.101089 12772 agents.py:69] training 36 of 50: completed tf_agent.train(...) = 94.410 [loss]\n",
      "I0626 21:57:55.102098 12772 agents.py:69] training 36 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:55.103136 12772 agents.py:69] training 37 of 50: executing collect_driver.run()\n",
      "I0626 21:57:55.104102  7092 logenv.py:75] #0 396.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:55.129052  7092 logenv.py:75] #0 397.24  [R=   0.4] game over\n",
      "I0626 21:57:55.143226  7092 logenv.py:75] #0 398.11  [R=   0.7] game over\n",
      "I0626 21:57:55.168622  7092 logenv.py:75] #0 399.35  [R=   0.3] game over\n",
      "I0626 21:57:55.228319  7092 logenv.py:75] #0 400.72  [R=  -0.6] game over\n",
      "I0626 21:57:55.251163  7092 logenv.py:75] #0 401.27  [R=   0.5] game over\n",
      "I0626 21:57:55.273164  7092 logenv.py:75] #0 402.27  [R=   0.1] game over\n",
      "I0626 21:57:55.287962  7092 logenv.py:75] #0 403.9   [R=   0.8] game over\n",
      "I0626 21:57:55.319283  7092 logenv.py:75] #0 404.36  [R=   0.3] game over\n",
      "I0626 21:57:55.339230  7092 logenv.py:75] #0 405.25  [R=   0.5] game over\n",
      "I0626 21:57:55.353192  7092 logenv.py:75] #0 406.18  [R=   0.5] game over\n",
      "I0626 21:57:55.361190 12772 agents.py:69] training 37 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:55.364559 12772 agents.py:69] training 37 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:56.631806 12772 agents.py:69] training 37 of 50: completed tf_agent.train(...) = 39.842 [loss]\n",
      "I0626 21:57:56.633786 12772 agents.py:69] training 37 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:56.635916 12772 agents.py:69] training 38 of 50: executing collect_driver.run()\n",
      "I0626 21:57:56.637911 18188 logenv.py:75] #0 407.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:56.715883 18188 logenv.py:75] #0 408.94  [R=  -0.6] game over\n",
      "I0626 21:57:56.738713 18188 logenv.py:75] #0 409.22  [R=   0.5] game over\n",
      "I0626 21:57:56.758326 18188 logenv.py:75] #0 410.21  [R=   0.6] game over\n",
      "I0626 21:57:56.807129 18188 logenv.py:75] #0 411.79  [R=  -1.1] game over\n",
      "I0626 21:57:56.825167 18188 logenv.py:75] #0 412.25  [R=   0.5] game over\n",
      "I0626 21:57:56.856283 18188 logenv.py:75] #0 413.46  [R=   0.1] game over\n",
      "I0626 21:57:56.898224 18188 logenv.py:75] #0 414.65  [R=  -0.3] game over\n",
      "I0626 21:57:56.913846 18188 logenv.py:75] #0 415.15  [R=   0.7] game over\n",
      "I0626 21:57:56.934322 18188 logenv.py:75] #0 416.24  [R=   0.5] game over\n",
      "I0626 21:57:56.953323 18188 logenv.py:75] #0 417.25  [R=   0.4] game over\n",
      "I0626 21:57:56.961320 12772 agents.py:69] training 38 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:56.963320 12772 agents.py:69] training 38 of 50: executing tf_agent.train(...)\n",
      "I0626 21:57:58.453753 12772 agents.py:69] training 38 of 50: completed tf_agent.train(...) = 66.275 [loss]\n",
      "I0626 21:57:58.454744 12772 agents.py:69] training 38 of 50: executing replay_buffer.clear()\n",
      "I0626 21:57:58.455677 12772 agents.py:69] training 39 of 50: executing collect_driver.run()\n",
      "I0626 21:57:58.457680 14320 logenv.py:75] #0 418.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:57:58.493678 14320 logenv.py:75] #0 419.50  [R=   0.1] game over\n",
      "I0626 21:57:58.511680 14320 logenv.py:75] #0 420.17  [R=   0.6] game over\n",
      "I0626 21:57:58.534680 14320 logenv.py:75] #0 421.24  [R=   0.6] game over\n",
      "I0626 21:57:58.556682 14320 logenv.py:75] #0 422.22  [R=   0.4] game over\n",
      "I0626 21:57:58.574682 14320 logenv.py:75] #0 423.19  [R=   0.6] game over\n",
      "I0626 21:57:58.590680 14320 logenv.py:75] #0 424.16  [R=   0.7] game over\n",
      "I0626 21:57:58.628681 14320 logenv.py:75] #0 425.59  [R=  -0.5] game over\n",
      "I0626 21:57:58.659685 14320 logenv.py:75] #0 426.39  [R=   0.3] game over\n",
      "I0626 21:57:58.672682 14320 logenv.py:75] #0 427.12  [R=   0.7] game over\n",
      "I0626 21:57:58.694683 14320 logenv.py:75] #0 428.25  [R=   0.4] game over\n",
      "I0626 21:57:58.700679 12772 agents.py:69] training 39 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:57:58.703691 12772 agents.py:69] training 39 of 50: executing tf_agent.train(...)\n",
      "I0626 21:58:00.127409 12772 agents.py:69] training 39 of 50: completed tf_agent.train(...) = 63.093 [loss]\n",
      "I0626 21:58:00.128412 12772 agents.py:69] training 39 of 50: executing replay_buffer.clear()\n",
      "I0626 21:58:00.129530 12772 agents.py:69] training 40 of 50: executing collect_driver.run()\n",
      "I0626 21:58:00.130542 17180 logenv.py:75] #0 429.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:58:00.156380 17180 logenv.py:75] #0 430.27  [R=   0.5] game over\n",
      "I0626 21:58:00.171379 17180 logenv.py:75] #0 431.16  [R=   0.7] game over\n",
      "I0626 21:58:00.188383 17180 logenv.py:75] #0 432.15  [R=   0.7] game over\n",
      "I0626 21:58:00.210375 17180 logenv.py:75] #0 433.26  [R=   0.6] game over\n",
      "I0626 21:58:00.231298 17180 logenv.py:75] #0 434.29  [R=   0.5] game over\n",
      "I0626 21:58:00.246685 17180 logenv.py:75] #0 435.18  [R=   0.6] game over\n",
      "I0626 21:58:00.298713 17180 logenv.py:75] #0 436.50  [R=   0.0] game over\n",
      "I0626 21:58:00.334795 17180 logenv.py:75] #0 437.29  [R=   0.3] game over\n",
      "I0626 21:58:00.354794 17180 logenv.py:75] #0 438.24  [R=   0.4] game over\n",
      "I0626 21:58:00.377560 17180 logenv.py:75] #0 439.24  [R=   0.5] game over\n",
      "I0626 21:58:00.383566 12772 agents.py:69] training 40 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:58:00.386569 12772 agents.py:69] training 40 of 50: executing tf_agent.train(...)\n",
      "I0626 21:58:01.704542 12772 agents.py:69] training 40 of 50: completed tf_agent.train(...) = 32.239 [loss]\n",
      "I0626 21:58:01.705523 12772 agents.py:69] training 40 of 50: executing replay_buffer.clear()\n",
      "I0626 21:58:01.707612 12772 agents.py:69] executing compute_avg_return(...)\n",
      "I0626 21:58:02.570436 12772 logenv.py:75] #1  41.201 [R=  -2.7] game over\n",
      "I0626 21:58:02.655982 12772 logenv.py:75] #1  42.17  [R=   0.7] game over\n",
      "I0626 21:58:03.740788 12772 logenv.py:75] #1  43.201 [R=  -2.7] game over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0626 21:58:03.809182 12772 logenv.py:75] #1  44.14  [R=   0.6] game over\n",
      "I0626 21:58:04.784944 12772 logenv.py:75] #1  45.201 [R=  -1.4] game over\n",
      "I0626 21:58:04.845947 12772 logenv.py:75] #1  46.13  [R=   0.7] game over\n",
      "I0626 21:58:06.336557 12772 logenv.py:75] #1  47.201 [R=  -2.6] game over\n",
      "I0626 21:58:07.335880 12772 logenv.py:75] #1  48.201 [R=  -1.4] game over\n",
      "I0626 21:58:08.307960 12772 logenv.py:75] #1  49.201 [R=  -2.7] game over\n",
      "I0626 21:58:09.230682 12772 logenv.py:75] #1  50.201 [R=  -2.8] game over\n",
      "I0626 21:58:09.231681 12772 agents.py:69] completed compute_avg_return(...) = -1.553\n",
      "I0626 21:58:09.232680 12772 agents.py:69] training 41 of 50: executing collect_driver.run()\n",
      "I0626 21:58:09.235687  5292 logenv.py:75] #0 440.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:58:09.318962  5292 logenv.py:75] #0 441.117 [R=  -1.2] game over\n",
      "I0626 21:58:09.336671  5292 logenv.py:75] #0 442.22  [R=   0.6] game over\n",
      "I0626 21:58:09.355924  5292 logenv.py:75] #0 443.22  [R=   0.6] game over\n",
      "I0626 21:58:09.375972  5292 logenv.py:75] #0 444.23  [R=   0.4] game over\n",
      "I0626 21:58:09.407924  5292 logenv.py:75] #0 445.35  [R=   0.2] game over\n",
      "I0626 21:58:09.442263  5292 logenv.py:75] #0 446.29  [R=   0.5] game over\n",
      "I0626 21:58:09.462950  5292 logenv.py:75] #0 447.15  [R=   0.6] game over\n",
      "I0626 21:58:09.494911  5292 logenv.py:75] #0 448.28  [R=   0.5] game over\n",
      "I0626 21:58:09.514912  5292 logenv.py:75] #0 449.12  [R=   0.7] game over\n",
      "I0626 21:58:09.542915  5292 logenv.py:75] #0 450.21  [R=   0.6] game over\n",
      "I0626 21:58:09.550297 12772 agents.py:69] training 41 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:58:09.553296 12772 agents.py:69] training 41 of 50: executing tf_agent.train(...)\n",
      "I0626 21:58:10.888468 12772 agents.py:69] training 41 of 50: completed tf_agent.train(...) = 86.551 [loss]\n",
      "I0626 21:58:10.889047 12772 agents.py:69] training 41 of 50: executing replay_buffer.clear()\n",
      "I0626 21:58:10.891046 12772 agents.py:69] training 42 of 50: executing collect_driver.run()\n",
      "I0626 21:58:10.892048  5292 logenv.py:75] #0 451.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:58:10.915129  5292 logenv.py:75] #0 452.30  [R=   0.4] game over\n",
      "I0626 21:58:10.937672  5292 logenv.py:75] #0 453.34  [R=   0.2] game over\n",
      "I0626 21:58:10.954373  5292 logenv.py:75] #0 454.23  [R=   0.4] game over\n",
      "I0626 21:58:10.972235  5292 logenv.py:75] #0 455.24  [R=   0.5] game over\n",
      "I0626 21:58:10.995205  5292 logenv.py:75] #0 456.38  [R=   0.2] game over\n",
      "I0626 21:58:11.036312  5292 logenv.py:75] #0 457.72  [R=  -0.5] game over\n",
      "I0626 21:58:11.067310  5292 logenv.py:75] #0 458.24  [R=   0.5] game over\n",
      "I0626 21:58:11.083480  5292 logenv.py:75] #0 459.14  [R=   0.7] game over\n",
      "I0626 21:58:11.104477  5292 logenv.py:75] #0 460.22  [R=   0.5] game over\n",
      "I0626 21:58:11.132477  5292 logenv.py:75] #0 461.27  [R=   0.5] game over\n",
      "I0626 21:58:11.139482 12772 agents.py:69] training 42 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:58:11.142478 12772 agents.py:69] training 42 of 50: executing tf_agent.train(...)\n",
      "I0626 21:58:12.618312 12772 agents.py:69] training 42 of 50: completed tf_agent.train(...) = 39.487 [loss]\n",
      "I0626 21:58:12.619339 12772 agents.py:69] training 42 of 50: executing replay_buffer.clear()\n",
      "I0626 21:58:12.620315 12772 agents.py:69] training 43 of 50: executing collect_driver.run()\n",
      "I0626 21:58:12.621317  7092 logenv.py:75] #0 462.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:58:12.639330  7092 logenv.py:75] #0 463.19  [R=   0.6] game over\n",
      "I0626 21:58:12.660384  7092 logenv.py:75] #0 464.25  [R=   0.4] game over\n",
      "I0626 21:58:12.689316  7092 logenv.py:75] #0 465.31  [R=   0.4] game over\n",
      "I0626 21:58:12.754317  7092 logenv.py:75] #0 466.80  [R=  -0.8] game over\n",
      "I0626 21:58:12.769311  7092 logenv.py:75] #0 467.16  [R=   0.6] game over\n",
      "I0626 21:58:12.812367  7092 logenv.py:75] #0 468.61  [R=  -0.3] game over\n",
      "I0626 21:58:12.831367  7092 logenv.py:75] #0 469.20  [R=   0.5] game over\n",
      "I0626 21:58:12.848369  7092 logenv.py:75] #0 470.14  [R=   0.6] game over\n",
      "I0626 21:58:12.863364  7092 logenv.py:75] #0 471.17  [R=   0.6] game over\n",
      "I0626 21:58:12.885369  7092 logenv.py:75] #0 472.25  [R=   0.5] game over\n",
      "I0626 21:58:12.894756 12772 agents.py:69] training 43 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:58:12.898573 12772 agents.py:69] training 43 of 50: executing tf_agent.train(...)\n",
      "I0626 21:58:12.993113 12772 agents.py:69] training 43 of 50: completed tf_agent.train(...) = 64.283 [loss]\n",
      "I0626 21:58:12.994112 12772 agents.py:69] training 43 of 50: executing replay_buffer.clear()\n",
      "I0626 21:58:12.995112 12772 agents.py:69] training 44 of 50: executing collect_driver.run()\n",
      "I0626 21:58:12.996114  7092 logenv.py:75] #0 473.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:58:13.009112  7092 logenv.py:75] #0 474.16  [R=   0.6] game over\n",
      "I0626 21:58:13.040110  7092 logenv.py:75] #0 475.47  [R=  -0.4] game over\n",
      "I0626 21:58:13.090113  7092 logenv.py:75] #0 476.52  [R=  -0.2] game over\n",
      "I0626 21:58:13.117116  7092 logenv.py:75] #0 477.31  [R=   0.3] game over\n",
      "I0626 21:58:13.142242  7092 logenv.py:75] #0 478.21  [R=   0.5] game over\n",
      "I0626 21:58:13.183115  7092 logenv.py:75] #0 479.51  [R=  -0.0] game over\n",
      "I0626 21:58:13.199254  7092 logenv.py:75] #0 480.16  [R=   0.6] game over\n",
      "I0626 21:58:13.224256  7092 logenv.py:75] #0 481.21  [R=   0.5] game over\n",
      "I0626 21:58:13.253257  7092 logenv.py:75] #0 482.30  [R=   0.4] game over\n",
      "I0626 21:58:13.304006  7092 logenv.py:75] #0 483.53  [R=  -0.1] game over\n",
      "I0626 21:58:13.311996 12772 agents.py:69] training 44 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:58:13.315999 12772 agents.py:69] training 44 of 50: executing tf_agent.train(...)\n",
      "I0626 21:58:14.682315 12772 agents.py:69] training 44 of 50: completed tf_agent.train(...) = 42.039 [loss]\n",
      "I0626 21:58:14.682915 12772 agents.py:69] training 44 of 50: executing replay_buffer.clear()\n",
      "I0626 21:58:14.684993 12772 agents.py:69] training 45 of 50: executing collect_driver.run()\n",
      "I0626 21:58:14.687000  7092 logenv.py:75] #0 484.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:58:14.704035  7092 logenv.py:75] #0 485.13  [R=   0.8] game over\n",
      "I0626 21:58:14.722990  7092 logenv.py:75] #0 486.22  [R=   0.5] game over\n",
      "I0626 21:58:14.764335  7092 logenv.py:75] #0 487.53  [R=  -0.1] game over\n",
      "I0626 21:58:14.778334  7092 logenv.py:75] #0 488.13  [R=   0.7] game over\n",
      "I0626 21:58:14.791104  7092 logenv.py:75] #0 489.12  [R=   0.7] game over\n",
      "I0626 21:58:14.829114  7092 logenv.py:75] #0 490.55  [R=  -0.0] game over\n",
      "I0626 21:58:14.846370  7092 logenv.py:75] #0 491.16  [R=   0.7] game over\n",
      "I0626 21:58:14.861348  7092 logenv.py:75] #0 492.19  [R=   0.7] game over\n",
      "I0626 21:58:14.885113  7092 logenv.py:75] #0 493.16  [R=   0.6] game over\n",
      "I0626 21:58:14.909113  7092 logenv.py:75] #0 494.20  [R=   0.6] game over\n",
      "I0626 21:58:14.916118 12772 agents.py:69] training 45 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:58:14.919115 12772 agents.py:69] training 45 of 50: executing tf_agent.train(...)\n",
      "I0626 21:58:16.448423 12772 agents.py:69] training 45 of 50: completed tf_agent.train(...) = 37.118 [loss]\n",
      "I0626 21:58:16.449434 12772 agents.py:69] training 45 of 50: executing replay_buffer.clear()\n",
      "I0626 21:58:16.451435 12772 agents.py:69] training 46 of 50: executing collect_driver.run()\n",
      "I0626 21:58:16.453436 15108 logenv.py:75] #0 495.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:58:16.481266 15108 logenv.py:75] #0 496.24  [R=   0.4] game over\n",
      "I0626 21:58:16.494192 15108 logenv.py:75] #0 497.9   [R=   0.8] game over\n",
      "I0626 21:58:16.516833 15108 logenv.py:75] #0 498.15  [R=   0.7] game over\n",
      "I0626 21:58:16.540986 15108 logenv.py:75] #0 499.19  [R=   0.6] game over\n",
      "I0626 21:58:16.571455 15108 logenv.py:75] #0 500.31  [R=   0.2] game over\n",
      "I0626 21:58:16.599539 15108 logenv.py:75] #0 501.32  [R=   0.3] game over\n",
      "I0626 21:58:16.679434 15108 logenv.py:75] #0 502.117 [R=  -1.6] game over\n",
      "I0626 21:58:16.698573 15108 logenv.py:75] #0 503.21  [R=   0.4] game over\n",
      "I0626 21:58:16.740023 15108 logenv.py:75] #0 504.36  [R=   0.4] game over\n",
      "I0626 21:58:16.767015 15108 logenv.py:75] #0 505.25  [R=   0.3] game over\n",
      "I0626 21:58:16.774596 12772 agents.py:69] training 46 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:58:16.776594 12772 agents.py:69] training 46 of 50: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0626 21:58:18.093643 12772 agents.py:69] training 46 of 50: completed tf_agent.train(...) = 122.134 [loss]\n",
      "I0626 21:58:18.094641 12772 agents.py:69] training 46 of 50: executing replay_buffer.clear()\n",
      "I0626 21:58:18.095645 12772 agents.py:69] training 47 of 50: executing collect_driver.run()\n",
      "I0626 21:58:18.097708 17180 logenv.py:75] #0 506.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:58:18.110642 17180 logenv.py:75] #0 507.16  [R=   0.7] game over\n",
      "I0626 21:58:18.142179 17180 logenv.py:75] #0 508.47  [R=  -0.4] game over\n",
      "I0626 21:58:18.156644 17180 logenv.py:75] #0 509.15  [R=   0.7] game over\n",
      "I0626 21:58:18.169642 17180 logenv.py:75] #0 510.14  [R=   0.7] game over\n",
      "I0626 21:58:18.186644 17180 logenv.py:75] #0 511.22  [R=   0.5] game over\n",
      "I0626 21:58:18.201713 17180 logenv.py:75] #0 512.12  [R=   0.8] game over\n",
      "I0626 21:58:18.222710 17180 logenv.py:75] #0 513.22  [R=   0.5] game over\n",
      "I0626 21:58:18.244711 17180 logenv.py:75] #0 514.31  [R=   0.3] game over\n",
      "I0626 21:58:18.275774 17180 logenv.py:75] #0 515.48  [R=  -0.1] game over\n",
      "I0626 21:58:18.291709 17180 logenv.py:75] #0 516.18  [R=   0.5] game over\n",
      "I0626 21:58:18.298778 12772 agents.py:69] training 47 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:58:18.300781 12772 agents.py:69] training 47 of 50: executing tf_agent.train(...)\n",
      "I0626 21:58:19.794255 12772 agents.py:69] training 47 of 50: completed tf_agent.train(...) = 60.423 [loss]\n",
      "I0626 21:58:19.795255 12772 agents.py:69] training 47 of 50: executing replay_buffer.clear()\n",
      "I0626 21:58:19.797257 12772 agents.py:69] training 48 of 50: executing collect_driver.run()\n",
      "I0626 21:58:19.798257  7092 logenv.py:75] #0 517.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:58:19.824043  7092 logenv.py:75] #0 518.22  [R=   0.5] game over\n",
      "I0626 21:58:19.841192  7092 logenv.py:75] #0 519.15  [R=   0.6] game over\n",
      "I0626 21:58:19.860963  7092 logenv.py:75] #0 520.14  [R=   0.7] game over\n",
      "I0626 21:58:19.879632  7092 logenv.py:75] #0 521.20  [R=   0.6] game over\n",
      "I0626 21:58:19.906357  7092 logenv.py:75] #0 522.24  [R=   0.5] game over\n",
      "I0626 21:58:19.941455  7092 logenv.py:75] #0 523.37  [R=   0.3] game over\n",
      "I0626 21:58:19.962041  7092 logenv.py:75] #0 524.23  [R=   0.5] game over\n",
      "I0626 21:58:19.980131  7092 logenv.py:75] #0 525.18  [R=   0.6] game over\n",
      "I0626 21:58:20.012618  7092 logenv.py:75] #0 526.31  [R=   0.1] game over\n",
      "I0626 21:58:20.045641  7092 logenv.py:75] #0 527.43  [R=   0.2] game over\n",
      "I0626 21:58:20.053640 12772 agents.py:69] training 48 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:58:20.056640 12772 agents.py:69] training 48 of 50: executing tf_agent.train(...)\n",
      "I0626 21:58:21.483345 12772 agents.py:69] training 48 of 50: completed tf_agent.train(...) = 27.590 [loss]\n",
      "I0626 21:58:21.484374 12772 agents.py:69] training 48 of 50: executing replay_buffer.clear()\n",
      "I0626 21:58:21.485340 12772 agents.py:69] training 49 of 50: executing collect_driver.run()\n",
      "I0626 21:58:21.486344 15108 logenv.py:75] #0 528.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:58:21.514316 15108 logenv.py:75] #0 529.30  [R=   0.3] game over\n",
      "I0626 21:58:21.581359 15108 logenv.py:75] #0 530.88  [R=  -0.6] game over\n",
      "I0626 21:58:21.601886 15108 logenv.py:75] #0 531.22  [R=   0.6] game over\n",
      "I0626 21:58:21.656294 15108 logenv.py:75] #0 532.46  [R=   0.2] game over\n",
      "I0626 21:58:21.759383 15108 logenv.py:75] #0 533.120 [R=  -1.1] game over\n",
      "I0626 21:58:21.787379 15108 logenv.py:75] #0 534.23  [R=   0.4] game over\n",
      "I0626 21:58:21.818451 15108 logenv.py:75] #0 535.35  [R=   0.3] game over\n",
      "I0626 21:58:21.841383 15108 logenv.py:75] #0 536.22  [R=   0.5] game over\n",
      "I0626 21:58:21.857380 15108 logenv.py:75] #0 537.14  [R=   0.7] game over\n",
      "I0626 21:58:21.879149 15108 logenv.py:75] #0 538.20  [R=   0.6] game over\n",
      "I0626 21:58:21.885201 12772 agents.py:69] training 49 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:58:21.888675 12772 agents.py:69] training 49 of 50: executing tf_agent.train(...)\n",
      "I0626 21:58:23.438575 12772 agents.py:69] training 49 of 50: completed tf_agent.train(...) = 88.980 [loss]\n",
      "I0626 21:58:23.439567 12772 agents.py:69] training 49 of 50: executing replay_buffer.clear()\n",
      "I0626 21:58:23.440604 12772 agents.py:69] training 50 of 50: executing collect_driver.run()\n",
      "I0626 21:58:23.442567 17180 logenv.py:75] #0 539.0   [R=   0.0] executing reset(...)\n",
      "I0626 21:58:23.474146 17180 logenv.py:75] #0 540.47  [R=  -0.0] game over\n",
      "I0626 21:58:23.487187 17180 logenv.py:75] #0 541.15  [R=   0.7] game over\n",
      "I0626 21:58:23.498866 17180 logenv.py:75] #0 542.12  [R=   0.7] game over\n",
      "I0626 21:58:23.512347 17180 logenv.py:75] #0 543.12  [R=   0.6] game over\n",
      "I0626 21:58:23.536992 17180 logenv.py:75] #0 544.17  [R=   0.6] game over\n",
      "I0626 21:58:23.564171 17180 logenv.py:75] #0 545.19  [R=   0.6] game over\n",
      "I0626 21:58:23.578703 17180 logenv.py:75] #0 546.10  [R=   0.8] game over\n",
      "I0626 21:58:23.613996 17180 logenv.py:75] #0 547.32  [R=   0.4] game over\n",
      "I0626 21:58:23.634683 17180 logenv.py:75] #0 548.18  [R=   0.6] game over\n",
      "I0626 21:58:23.654466 17180 logenv.py:75] #0 549.20  [R=   0.5] game over\n",
      "I0626 21:58:23.661465 12772 agents.py:69] training 50 of 50: executing replay_buffer.gather_all()\n",
      "I0626 21:58:23.664155 12772 agents.py:69] training 50 of 50: executing tf_agent.train(...)\n",
      "I0626 21:58:24.984697 12772 agents.py:69] training 50 of 50: completed tf_agent.train(...) = 37.693 [loss]\n",
      "I0626 21:58:24.986671 12772 agents.py:69] training 50 of 50: executing replay_buffer.clear()\n",
      "I0626 21:58:24.987634 12772 agents.py:69] executing compute_avg_return(...)\n",
      "I0626 21:58:26.508157 12772 logenv.py:75] #1  51.201 [R=  -3.0] game over\n",
      "I0626 21:58:27.389276 12772 logenv.py:75] #1  52.201 [R=  -2.5] game over\n",
      "I0626 21:58:27.467068 12772 logenv.py:75] #1  53.14  [R=   0.7] game over\n",
      "I0626 21:58:28.656136 12772 logenv.py:75] #1  54.201 [R=  -2.9] game over\n",
      "I0626 21:58:30.146238 12772 logenv.py:75] #1  55.201 [R=  -2.8] game over\n",
      "I0626 21:58:31.383033 12772 logenv.py:75] #1  56.201 [R=  -3.0] game over\n",
      "I0626 21:58:32.484781 12772 logenv.py:75] #1  57.201 [R=  -2.7] game over\n",
      "I0626 21:58:33.935298 12772 logenv.py:75] #1  58.201 [R=  -2.9] game over\n",
      "I0626 21:58:34.047863 12772 logenv.py:75] #1  59.10  [R=   0.7] game over\n",
      "I0626 21:58:34.199820 12772 logenv.py:75] #1  60.16  [R=   0.6] game over\n",
      "I0626 21:58:34.204836 12772 agents.py:69] completed compute_avg_return(...) = -1.897\n"
     ]
    }
   ],
   "source": [
    "ppoAgent = PpoAgent(    gym_env_name = 'Berater-v1',\n",
    "                        fc_layers=(500,500,500), \n",
    "                        training_duration=training_duration )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full training (full duration, learning rate, reduced logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDuration\n",
    "from easyagents.config import Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_duration=TrainingDuration( num_iterations = 2000,\n",
    "                                    num_episodes_per_iteration = 10,\n",
    "                                    max_steps_per_episode = 1000,\n",
    "                                    num_epochs_per_iteration = 5,\n",
    "                                    num_iterations_between_eval = 10,\n",
    "                                    num_eval_episodes = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging=Logging(log_agent=True, log_gym_env=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent = PpoAgent(    gym_env_name = 'Berater-v1',\n",
    "                        fc_layers=(500,500,500), \n",
    "                        training_duration=training_duration,\n",
    "                        logging=logging,\n",
    "                        learning_rate=1e-4\n",
    "                   )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zpzHtN3-kQ26",
    "w3OdHyWEEEwy",
    "bzoq0VM85p46"
   ],
   "name": "190316-2_tfagents_berater-v12_ppo.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
