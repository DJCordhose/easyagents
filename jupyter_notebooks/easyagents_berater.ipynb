{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU7ylMh1kQ2y"
   },
   "source": [
    "# Berater Environment v13\n",
    "\n",
    "## Changes from v12 (work in progress)\n",
    "* migration to easyagents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.10.11 > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3OdHyWEEEwy"
   },
   "source": [
    "# Define Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQyb_Aq8Kg9j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "from gym import spaces\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsJ6zcXvwN53"
   },
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-S4sZG5ZkQ3T"
   },
   "outputs": [],
   "source": [
    "def state_name_to_int(state):\n",
    "    state_name_map = {\n",
    "        'S': 0,\n",
    "        'A': 1,\n",
    "        'B': 2,\n",
    "        'C': 3,\n",
    "        'D': 4,\n",
    "        'E': 5,\n",
    "        'F': 6,\n",
    "        'G': 7,\n",
    "        'H': 8,\n",
    "        'K': 9,\n",
    "        'L': 10,\n",
    "        'M': 11,\n",
    "        'N': 12,\n",
    "        'O': 13\n",
    "    }\n",
    "    return state_name_map[state]\n",
    "\n",
    "def int_to_state_name(state_as_int):\n",
    "    state_map = {\n",
    "        0: 'S',\n",
    "        1: 'A',\n",
    "        2: 'B',\n",
    "        3: 'C',\n",
    "        4: 'D',\n",
    "        5: 'E',\n",
    "        6: 'F',\n",
    "        7: 'G',\n",
    "        8: 'H',\n",
    "        9: 'K',\n",
    "        10: 'L',\n",
    "        11: 'M',\n",
    "        12: 'N',\n",
    "        13: 'O'\n",
    "    }\n",
    "    return state_map[state_as_int]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-olom0nwiSX"
   },
   "source": [
    "### Berater Environment (OpenAI Gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3plH2u3Swotj"
   },
   "outputs": [],
   "source": [
    "class BeraterEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    The Berater Problem\n",
    "\n",
    "    Actions: \n",
    "    There are 4 discrete deterministic actions, each choosing one direction\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['ansi']}\n",
    "    \n",
    "    showStep = False\n",
    "    showDone = True\n",
    "    envEpisodeModulo = 100\n",
    "\n",
    "    def __init__(self):\n",
    "#         self.map = {\n",
    "#             'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
    "#             'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
    "#             'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
    "#             'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
    "#         }\n",
    "        self.map = {\n",
    "            'S': [('A', 300), ('B', 100), ('C', 200 )],\n",
    "            'A': [('S', 300), ('B', 100), ('E', 100 ), ('D', 100 )],\n",
    "            'B': [('S', 100), ('A', 100), ('C', 50 ), ('K', 200 )],\n",
    "            'C': [('S', 200), ('B', 50), ('M', 100 ), ('L', 200 )],\n",
    "            'D': [('A', 100), ('F', 50)],\n",
    "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
    "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
    "            'G': [('F', 200), ('O', 300)],\n",
    "            'H': [('E', 100), ('K', 300)],\n",
    "            'K': [('B', 200), ('H', 300)],\n",
    "            'L': [('C', 200), ('M', 50)],\n",
    "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
    "            'N': [('M', 100), ('O', 100)],\n",
    "            'O': [('N', 100), ('G', 300)]\n",
    "        }\n",
    "        max_paths = 4\n",
    "        self.action_space = spaces.Discrete(max_paths)\n",
    "      \n",
    "        positions = len(self.map)\n",
    "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
    "        # non existing path is -1000 and no position change\n",
    "        # look at what #getObservation returns if you are confused\n",
    "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
    "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
    "        self.observation_space = spaces.Box(low=low,\n",
    "                                             high=high,\n",
    "                                             dtype=np.float32)\n",
    "        self.reward_range = (-1, 1)\n",
    "\n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "\n",
    "        self.envReward = 0\n",
    "        self.envEpisodeCount = 0\n",
    "        self.envStepCount = 0\n",
    "\n",
    "        self.reset()\n",
    "        self.optimum = self.calculate_customers_reward()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def iterate_path(self, state, action):\n",
    "        paths = self.map[state]\n",
    "        if action < len(paths):\n",
    "          return paths[action]\n",
    "        else:\n",
    "          # sorry, no such action, stay where you are and pay a high penalty\n",
    "          return (state, 1000)\n",
    "      \n",
    "    def step(self, action):\n",
    "        if self.debugStep:\n",
    "          pdb.set_trace()\n",
    "        destination, cost = self.iterate_path(self.state, action)\n",
    "        lastState = self.state\n",
    "        customerReward = self.customer_reward[destination]\n",
    "        reward = (customerReward - cost) / self.optimum\n",
    "\n",
    "        self.state = destination\n",
    "        self.customer_visited(destination)\n",
    "        done = (destination == 'S' and self.all_customers_visited())\n",
    "        if self.stepCount >= 200:\n",
    "          if BeraterEnv.showDone:\n",
    "            print(\"Done: stepCount >= 200\")\n",
    "          done = True\n",
    "\n",
    "        stateAsInt = state_name_to_int(self.state)\n",
    "        self.totalReward += reward\n",
    "        self.stepCount += 1\n",
    "        self.envReward += reward\n",
    "        self.envStepCount += 1\n",
    "\n",
    "        if self.showStep:\n",
    "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
    "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
    "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
    "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
    "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
    "                   )\n",
    "\n",
    "        if done and not self.isDone:\n",
    "            self.envEpisodeCount += 1\n",
    "            if BeraterEnv.showDone:\n",
    "                episodes = BeraterEnv.envEpisodeModulo\n",
    "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
    "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
    "                print( \"Done: \" + \n",
    "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
    "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
    "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
    "                        )\n",
    "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
    "                    self.envReward = 0\n",
    "                    self.envStepCount = 0\n",
    "\n",
    "        self.isDone = done\n",
    "        observation = self.getObservation(stateAsInt)\n",
    "        info = {\"from\": self.state, \"to\": destination}\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def getObservation(self, position):\n",
    "        result = np.array([ position, \n",
    "                               self.getPathObservation(position, 0),\n",
    "                               self.getPathObservation(position, 1),\n",
    "                               self.getPathObservation(position, 2),\n",
    "                               self.getPathObservation(position, 3)\n",
    "                              ],\n",
    "                             dtype=np.float32)\n",
    "        all_rest_rewards = list(self.customer_reward.values())\n",
    "        result = np.append(result, all_rest_rewards)\n",
    "        return result\n",
    "\n",
    "    def getPathObservation(self, position, path):\n",
    "        source = int_to_state_name(position)\n",
    "        paths = self.map[self.state]\n",
    "        if path < len(paths):\n",
    "          target, cost = paths[path]\n",
    "          reward = self.customer_reward[target] \n",
    "          result = reward - cost\n",
    "        else:\n",
    "          result = -1000\n",
    "\n",
    "        return result\n",
    "\n",
    "    def customer_visited(self, customer):\n",
    "        self.customer_reward[customer] = 0\n",
    "\n",
    "    def all_customers_visited(self):\n",
    "        return self.calculate_customers_reward() == 0\n",
    "\n",
    "    def calculate_customers_reward(self):\n",
    "        sum = 0\n",
    "        for value in self.customer_reward.values():\n",
    "            sum += value\n",
    "        return sum\n",
    "\n",
    "      \n",
    "    def modulate_reward(self):\n",
    "      number_of_customers = len(self.map) - 1\n",
    "      number_per_consultant = int(number_of_customers/2)\n",
    "#       number_per_consultant = int(number_of_customers/1.5)\n",
    "      self.customer_reward = {\n",
    "          'S': 0\n",
    "      }\n",
    "      for customer_nr in range(1, number_of_customers + 1):\n",
    "        self.customer_reward[int_to_state_name(customer_nr)] = 0\n",
    "      \n",
    "      # every consultant only visits a few random customers\n",
    "      samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
    "      key_list = list(self.customer_reward.keys())\n",
    "      for sample in samples:\n",
    "        self.customer_reward[key_list[sample]] = 1000\n",
    "\n",
    "      \n",
    "    def reset(self):\n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "\n",
    "        self.modulate_reward()\n",
    "        self.state = 'S'\n",
    "        return self.getObservation(state_name_to_int(self.state))\n",
    "      \n",
    "    def render(self):\n",
    "      print(self.customer_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9J54w2URZIme"
   },
   "outputs": [],
   "source": [
    "BeraterEnv.showStep = False\n",
    "BeraterEnv.showDone = False\n",
    "BeraterEnv.debugStep = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYaTAvAyYO-U"
   },
   "source": [
    "### Register with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berater registered as 'Berater-v1'\n"
     ]
    }
   ],
   "source": [
    "if not 'isEnvRegistered' in locals():\n",
    "  env_name=\"Berater-v1\"\n",
    "  gym.envs.registration.register(id=env_name,entry_point=BeraterEnv,max_episode_steps=1000)\n",
    "  isEnvRegistered=True\n",
    "  print(\"Berater registered as '\" + env_name + \"'\")\n",
    "else:\n",
    "  print(\"Already registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sX8eJGcbOJ30"
   },
   "source": [
    "# Train policy with tfagents PpoAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzoq0VM85p46"
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install easyagents > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -i https://test.pypi.org/simple/ easyagents > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDuration\n",
    "from easyagents.config import TrainingDurationFast\n",
    "from easyagents.config import Logging\n",
    "from easyagents.config import LoggingSilent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dry run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (very short, no logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDurationFast\n",
    "from easyagents.config import LoggingSilent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ofYknQFRkRT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_agent.py: creating PPOAgent(...)\n",
      "ppo_agent.py: executing _train(...)\n",
      "ppo_agent.py: epoch=0\n",
      "ppo_agent.py: executing _train(...)\n",
      "ppo_agent.py: epoch=0\n",
      "ppo_agent.py: executing _train(...)\n",
      "ppo_agent.py: epoch=0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-06f04fa1d798>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                         \u001b[0mtraining_duration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrainingDurationFast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                         logging = LoggingSilent() )\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mppoAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\easyagents\\tfagents.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logCall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" executing tf_agent.train(...)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mexperience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrajectories\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logCall\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34mf'{msg} completed tf_agent.train(...) = {total_loss.numpy():.3f} [loss]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    402\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1388\u001b[0m         ag_ctx.Status.ENABLED if self._autograph else ag_ctx.Status.DISABLED)\n\u001b[0;32m   1389\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_status\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1390\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1391\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1697\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1698\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 1699\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1701\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1639\u001b[0m         relaxed_arg_shapes)\n\u001b[0;32m   1640\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 1641\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   1642\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   1595\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1596\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1597\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   1598\u001b[0m         self._function_attributes)\n\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    730\u001b[0m                                           converted_func)\n\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tf_agents\\agents\\tf_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, experience, weights)\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[0mloss_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m       \u001b[0mloss_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLossInfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tf_agents\\agents\\ppo\\ppo_agent.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, experience, weights)\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;31m# Tuple is used for py3, where zip is a generator producing values once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables_to_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gradient_clipping\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m           grads_and_vars = eager_utils.clip_gradient_norms(\n\u001b[0;32m    536\u001b[0m               grads_and_vars, self._gradient_clipping)\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1000\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1386\u001b[0m   \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1388\u001b[1;33m     \u001b[0mgrad_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1389\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6402\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m   6403\u001b[0m         \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6404\u001b[1;33m                   name=name)\n\u001b[0m\u001b[0;32m   6405\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6406\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    791\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[0;32m    792\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    794\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    467\u001b[0m     return super(FuncGraph, self).create_op(\n\u001b[0;32m    468\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m         compute_device=compute_device)\n\u001b[0m\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3297\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3298\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3299\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3300\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3301\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1712\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1713\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1714\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1716\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1546\u001b[0m     \u001b[1;31m# TODO(skyewm): this creates and deletes a new TF_Status for every attr.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1547\u001b[0m     \u001b[1;31m# It might be worth creating a convenient way to re-use the same status.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1548\u001b[1;33m     \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_SetAttrValueProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserialized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppoAgent = PpoAgent(    gym_env_name = 'Berater-v1',\n",
    "                        training_duration=TrainingDurationFast(),\n",
    "                        logging = LoggingSilent() )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiP6UgA65163"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_average_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. First training (with chosen policy network layers, training durations & default logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_duration=TrainingDuration( num_iterations = 50,\n",
    "                                    num_episodes_per_iteration = 10,\n",
    "                                    max_steps_per_episode = 1000,\n",
    "                                    num_epochs_per_iteration = 5,\n",
    "                                    num_iterations_between_eval = 10,\n",
    "                                    num_eval_episodes = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:23:32.575278 22948 agents.py:60] gym_env_name=Berater-v1 fc_layers=(500, 500, 500)\n",
      "I0624 23:23:32.576375 22948 agents.py:69] executing: tf.compat.v1.enable_v2_behavior()\n",
      "I0624 23:23:32.578297 22948 agents.py:69] executing: tf.enable_eager_execution()\n",
      "I0624 23:23:32.579704 22948 agents.py:69] executing: tf.compat.v1.set_random_seed(0)\n",
      "I0624 23:23:32.580279 22948 agents.py:69] Creating environment:\n",
      "I0624 23:23:32.581384 22948 agents.py:69]    executing tf_py_environment.TFPyEnvironment( suite_gym.load )\n",
      "I0624 23:23:32.585394 22948 agents.py:69] Creating agent:\n",
      "I0624 23:23:32.586099 22948 agents.py:69]   creating  tf.compat.v1.train.AdamOptimizer( ... )\n",
      "I0624 23:23:32.592112 22948 agents.py:69]   creating  PPOAgent( ... )\n",
      "I0624 23:23:32.604208 22948 agents.py:69]   executing tf_agent.initialize()\n",
      "I0624 23:23:32.605191 22948 agents.py:69] Creating data collection:\n",
      "I0624 23:23:32.605191 22948 agents.py:69]   creating TFUniformReplayBuffer()\n",
      "I0624 23:23:32.628186 22948 agents.py:69]   creating DynamicEpisodeDriver()\n",
      "I0624 23:23:32.629596 22948 agents.py:69] Starting training:\n",
      "I0624 23:23:32.631100 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:23:32.632097 22948 agents.py:69]    executing tf_py_environment.TFPyEnvironment( suite_gym.load )\n",
      "I0624 23:23:32.639233 22948 logenv.py:72] #EnvId ResetCount.Steps [R=sumRewards]\n",
      "I0624 23:23:32.640664 22948 logenv.py:75] #5   0.0   [R=   0.0] executing reset(...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_agent.py: creating PPOAgent(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:23:34.208884 22948 logenv.py:75] #5   1.201 [R=  -3.2] game over\n",
      "I0624 23:23:35.837425 22948 logenv.py:75] #5   2.201 [R=  -3.0] game over\n",
      "I0624 23:23:37.374526 22948 logenv.py:75] #5   3.201 [R=  -3.3] game over\n",
      "I0624 23:23:38.975446 22948 logenv.py:75] #5   4.201 [R=  -3.2] game over\n",
      "I0624 23:23:40.563221 22948 logenv.py:75] #5   5.201 [R=  -3.2] game over\n",
      "I0624 23:23:42.135339 22948 logenv.py:75] #5   6.201 [R=  -3.3] game over\n",
      "I0624 23:23:43.604231 22948 logenv.py:75] #5   7.201 [R=  -1.7] game over\n",
      "I0624 23:23:45.161143 22948 logenv.py:75] #5   8.201 [R=  -2.8] game over\n",
      "I0624 23:23:46.647231 22948 logenv.py:75] #5   9.201 [R=  -3.2] game over\n",
      "I0624 23:23:48.122108 22948 logenv.py:75] #5  10.201 [R=  -3.3] game over\n",
      "I0624 23:23:48.125166 22948 agents.py:69] completed compute_avg_return(...) = -3.100\n",
      "I0624 23:23:48.126106 22948 agents.py:69] training 1 of 50: executing collect_driver.run()\n",
      "I0624 23:23:48.595163 15100 logenv.py:72] #EnvId ResetCount.Steps [R=sumRewards]\n",
      "I0624 23:23:48.603162 15100 logenv.py:75] #4   0.0   [R=   0.0] executing reset(...)\n",
      "I0624 23:23:48.675511 21464 logenv.py:75] #4   1.66  [R=  -4.5] game over\n",
      "I0624 23:23:48.760904  4208 logenv.py:75] #4   2.70  [R=  -4.4] game over\n",
      "I0624 23:23:48.853657 11732 logenv.py:75] #4   3.80  [R=  -3.4] game over\n",
      "I0624 23:23:48.946495 15100 logenv.py:75] #4   4.80  [R=  -4.7] game over\n",
      "I0624 23:23:49.173611  4208 logenv.py:75] #4   5.182 [R= -12.1] game over\n",
      "I0624 23:23:49.290460 21064 logenv.py:75] #4   6.105 [R=  -5.1] game over\n",
      "I0624 23:23:49.446550 21064 logenv.py:75] #4   7.130 [R=  -8.1] game over\n",
      "I0624 23:23:49.650613 21064 logenv.py:75] #4   8.165 [R= -10.7] game over\n",
      "I0624 23:23:49.774726 21464 logenv.py:75] #4   9.102 [R=  -7.2] game over\n",
      "I0624 23:23:49.892302 11732 logenv.py:75] #4  10.95  [R=  -6.5] game over\n",
      "I0624 23:23:49.902297 22948 agents.py:69] training 1 of 50: executing replay_buffer.gather_all()\n",
      "I0624 23:23:49.906488 22948 agents.py:69] training 1 of 50: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_agent.py: executing _train(...)\n",
      "ppo_agent.py: epoch=0\n",
      "ppo_agent.py: epoch=1\n",
      "ppo_agent.py: epoch=2\n",
      "ppo_agent.py: epoch=3\n",
      "ppo_agent.py: epoch=4\n",
      "ppo_agent.py: executing _train(...)\n",
      "ppo_agent.py: epoch=0\n",
      "ppo_agent.py: epoch=1\n",
      "ppo_agent.py: epoch=2\n",
      "ppo_agent.py: epoch=3\n",
      "ppo_agent.py: epoch=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:24:01.268658 22948 agents.py:69] training 1 of 50: completed tf_agent.train(...) = 4479.675 [loss]\n",
      "I0624 23:24:01.270255 22948 agents.py:69] training 1 of 50: executing replay_buffer.clear()\n",
      "I0624 23:24:01.271238 22948 agents.py:69] training 2 of 50: executing collect_driver.run()\n",
      "I0624 23:24:01.273164  3344 logenv.py:75] #4  11.0   [R=   0.0] executing reset(...)\n",
      "I0624 23:24:01.513897  3344 logenv.py:75] #4  12.201 [R= -14.4] game over\n",
      "I0624 23:24:01.729017  2520 logenv.py:75] #4  13.179 [R= -12.4] game over\n",
      "I0624 23:24:01.794328 21464 logenv.py:75] #4  14.52  [R=  -3.0] game over\n",
      "I0624 23:24:01.903247 10108 logenv.py:75] #4  15.90  [R=  -5.1] game over\n",
      "I0624 23:24:01.984466 21464 logenv.py:75] #4  16.62  [R=  -3.2] game over\n",
      "I0624 23:24:02.130287 10108 logenv.py:75] #4  17.127 [R=  -7.4] game over\n",
      "I0624 23:24:02.337376 21464 logenv.py:75] #4  18.172 [R= -17.0] game over\n",
      "I0624 23:24:02.495942 10108 logenv.py:75] #4  19.128 [R=  -8.1] game over\n",
      "I0624 23:24:02.743566 10108 logenv.py:75] #4  20.201 [R= -14.3] game over\n",
      "I0624 23:24:02.896385  3344 logenv.py:75] #4  21.121 [R= -10.1] game over\n",
      "I0624 23:24:02.906557 22948 agents.py:69] training 2 of 50: executing replay_buffer.gather_all()\n",
      "I0624 23:24:02.911385 22948 agents.py:69] training 2 of 50: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_agent.py: executing _train(...)\n",
      "ppo_agent.py: epoch=0\n",
      "ppo_agent.py: epoch=1\n",
      "ppo_agent.py: epoch=2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-0fd38f182951>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                         \u001b[0mfc_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                         training_duration=training_duration )\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mppoAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\easyagents\\tfagents.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logCall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" executing tf_agent.train(...)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mexperience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrajectories\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logCall\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34mf'{msg} completed tf_agent.train(...) = {total_loss.numpy():.3f} [loss]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    402\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1388\u001b[0m         ag_ctx.Status.ENABLED if self._autograph else ag_ctx.Status.DISABLED)\n\u001b[0;32m   1389\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_status\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1390\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1391\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1697\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1698\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 1699\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1701\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1639\u001b[0m         relaxed_arg_shapes)\n\u001b[0;32m   1640\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 1641\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   1642\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   1595\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1596\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1597\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   1598\u001b[0m         self._function_attributes)\n\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    730\u001b[0m                                           converted_func)\n\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tf_agents\\agents\\tf_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, experience, weights)\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[0mloss_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m       \u001b[0mloss_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLossInfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tf_agents\\agents\\ppo\\ppo_agent.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, experience, weights)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         variables_to_train = (\n\u001b[1;32m--> 529\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_actor_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m             self._value_net.trainable_weights)\n\u001b[0;32m    531\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables_to_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tf_agents\\agents\\ppo\\ppo_agent.py\u001b[0m in \u001b[0;36mget_epoch_loss\u001b[1;34m(self, time_steps, actions, act_log_probs, returns, normalized_advantages, action_distribution_parameters, weights, train_step, debug_summaries)\u001b[0m\n\u001b[0;32m    344\u001b[0m     distribution_step = self._collect_policy.distribution(\n\u001b[0;32m    345\u001b[0m         time_steps, policy_state)\n\u001b[1;32m--> 346\u001b[1;33m     \u001b[1;31m# TODO(eholly): Rename policy distributions to something clear and uniform.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m     \u001b[0mcurrent_policy_distribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36mdistribution\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memit_log_probability\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m       \u001b[1;31m# This here is set only for compatibility with info_spec in constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tf_agents\\agents\\ppo\\ppo_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;31m# Actor network outputs nested structure of distributions or actions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     actions_or_distributions, policy_state = self._apply_actor_network(\n\u001b[1;32m--> 133\u001b[1;33m         time_step, policy_state)\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_to_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_or_distribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tf_agents\\agents\\ppo\\ppo_policy.py\u001b[0m in \u001b[0;36m_apply_actor_network\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m    119\u001b[0m                               time_step.discount, observation)\n\u001b[0;32m    120\u001b[0m     return self._actor_network(\n\u001b[1;32m--> 121\u001b[1;33m         time_step.observation, time_step.step_type, network_state=policy_state)\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tf_agents\\networks\\network.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_tensor_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    667\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m                   \u001b[0mforce_conversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                   \u001b[0moptional_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptional_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m               ), args, kwargs)\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, owner, options, args, kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tf_agents\\networks\\actor_distribution_network.py\u001b[0m in \u001b[0;36mtf__call\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mstates_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConversionOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_conversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_convert_user_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstates_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstates_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m       \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mlp_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m       \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unflatten'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_squash\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConversionOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_conversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_convert_user_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m       \u001b[0moutput_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'map_structure'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConversionOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_conversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_convert_user_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mproj_net\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproj_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConversionOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_conversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_convert_user_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouter_rank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_projection_networks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\u001b[0m in \u001b[0;36mfor_stmt\u001b[1;34m(iter_, extra_test, body, init_state)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcustom_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextra_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_py_for_stmt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\u001b[0m in \u001b[0;36m_py_for_stmt\u001b[1;34m(iter_, extra_test, body, init_state)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mextra_test\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextra_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m       \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tf_agents\\networks\\actor_distribution_network.py\u001b[0m in \u001b[0;36mloop_body\u001b[1;34m(loop_vars, states_1)\u001b[0m\n\u001b[0;32m     19\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mstates_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConversionOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_conversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_convert_user_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstates_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstates_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m       \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mlp_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, owner, options, args, kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforce_conversion\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted_for_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[0m_attach_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    667\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1048\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[1;34m(value, bias, data_format, name)\u001b[0m\n\u001b[0;32m   2662\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2664\u001b[1;33m       \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"bias\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2665\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[0;32m   1098\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[0;32m   1099\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[1;32m-> 1100\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1156\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1237\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_dense_var_to_tensor\u001b[1;34m(var, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m   1588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1589\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_dense_var_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1590\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_var_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_dense_var_to_tensor\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1543\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1544\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1545\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1547\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__iadd__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_other\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mvalue\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_existing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_graph_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    937\u001b[0m     \u001b[0mvariable_accessed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     result = gen_resource_variable_ops.read_variable_op(self._handle,\n\u001b[1;32m--> 939\u001b[1;33m                                                         self._dtype)\n\u001b[0m\u001b[0;32m    940\u001b[0m     \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    640\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m--> 642\u001b[1;33m         \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n\u001b[0m\u001b[0;32m    643\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    791\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[0;32m    792\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    794\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    467\u001b[0m     return super(FuncGraph, self).create_op(\n\u001b[0;32m    468\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m         compute_device=compute_device)\n\u001b[0m\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3297\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3298\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3299\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3300\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3301\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1712\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1713\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1714\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1716\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppoAgent = PpoAgent(    gym_env_name = 'Berater-v1',\n",
    "                        fc_layers=(500,500,500), \n",
    "                        training_duration=training_duration )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full training (full duration, learning rate, reduced logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDuration\n",
    "from easyagents.config import Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_duration=TrainingDuration( num_iterations = 2000,\n",
    "                                    num_episodes_per_iteration = 10,\n",
    "                                    max_steps_per_episode = 1000,\n",
    "                                    num_epochs_per_iteration = 5,\n",
    "                                    num_iterations_between_eval = 10,\n",
    "                                    num_eval_episodes = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging=Logging(log_agent=True, log_gym_env=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:29:41.855004 22948 agents.py:60] gym_env_name=Berater-v1 fc_layers=(500, 500, 500)\n",
      "I0624 23:29:41.856101 22948 agents.py:69] executing: tf.compat.v1.enable_v2_behavior()\n",
      "I0624 23:29:41.857016 22948 agents.py:69] executing: tf.enable_eager_execution()\n",
      "I0624 23:29:41.858005 22948 agents.py:69] executing: tf.compat.v1.set_random_seed(0)\n",
      "I0624 23:29:41.862042 22948 agents.py:69] Creating environment:\n",
      "I0624 23:29:41.863085 22948 agents.py:69]    executing tf_py_environment.TFPyEnvironment( suite_gym.load )\n",
      "I0624 23:29:41.867750 22948 agents.py:69] Creating agent:\n",
      "I0624 23:29:41.868785 22948 agents.py:69]   creating  tf.compat.v1.train.AdamOptimizer( ... )\n",
      "I0624 23:29:41.876369 22948 agents.py:69]   creating  PPOAgent( ... )\n",
      "I0624 23:29:41.888335 22948 agents.py:69]   executing tf_agent.initialize()\n",
      "I0624 23:29:41.889336 22948 agents.py:69] Creating data collection:\n",
      "I0624 23:29:41.889336 22948 agents.py:69]   creating TFUniformReplayBuffer()\n",
      "I0624 23:29:41.910668 22948 agents.py:69]   creating DynamicEpisodeDriver()\n",
      "I0624 23:29:41.911549 22948 agents.py:69] Starting training:\n",
      "I0624 23:29:41.913547 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:29:41.914549 22948 agents.py:69]    executing tf_py_environment.TFPyEnvironment( suite_gym.load )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_agent.py: creating PPOAgent(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:29:57.431318 22948 agents.py:69] completed compute_avg_return(...) = -3.888\n",
      "I0624 23:29:57.433272 22948 agents.py:69] training 1 of 2000: executing collect_driver.run()\n",
      "I0624 23:29:59.586188 22948 agents.py:69] training 1 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:29:59.591236 22948 agents.py:69] training 1 of 2000: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_agent.py: executing _train(...)\n",
      "ppo_agent.py: epoch=0\n",
      "ppo_agent.py: epoch=1\n",
      "ppo_agent.py: epoch=2\n",
      "ppo_agent.py: epoch=3\n",
      "ppo_agent.py: epoch=4\n",
      "ppo_agent.py: executing _train(...)\n",
      "ppo_agent.py: epoch=0\n",
      "ppo_agent.py: epoch=1\n",
      "ppo_agent.py: epoch=2\n",
      "ppo_agent.py: epoch=3\n",
      "ppo_agent.py: epoch=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:30:11.015911 22948 agents.py:69] training 1 of 2000: completed tf_agent.train(...) = 4128.440 [loss]\n",
      "I0624 23:30:11.016751 22948 agents.py:69] training 1 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:30:11.019767 22948 agents.py:69] training 2 of 2000: executing collect_driver.run()\n",
      "I0624 23:30:12.246863 22948 agents.py:69] training 2 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:30:12.249863 22948 agents.py:69] training 2 of 2000: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_agent.py: executing _train(...)\n",
      "ppo_agent.py: epoch=0\n",
      "ppo_agent.py: epoch=1\n",
      "ppo_agent.py: epoch=2\n",
      "ppo_agent.py: epoch=3\n",
      "ppo_agent.py: epoch=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:30:18.574217 22948 agents.py:69] training 2 of 2000: completed tf_agent.train(...) = 5826.175 [loss]\n",
      "I0624 23:30:18.576298 22948 agents.py:69] training 2 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:30:18.578305 22948 agents.py:69] training 3 of 2000: executing collect_driver.run()\n",
      "I0624 23:30:19.644215 22948 agents.py:69] training 3 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:30:19.650166 22948 agents.py:69] training 3 of 2000: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_agent.py: executing _train(...)\n",
      "ppo_agent.py: epoch=0\n",
      "ppo_agent.py: epoch=1\n",
      "ppo_agent.py: epoch=2\n",
      "ppo_agent.py: epoch=3\n",
      "ppo_agent.py: epoch=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:30:27.618613 22948 agents.py:69] training 3 of 2000: completed tf_agent.train(...) = 3630.698 [loss]\n",
      "I0624 23:30:27.620484 22948 agents.py:69] training 3 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:30:27.622563 22948 agents.py:69] training 4 of 2000: executing collect_driver.run()\n",
      "I0624 23:30:28.466400 22948 agents.py:69] training 4 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:30:28.470656 22948 agents.py:69] training 4 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:30:30.612318 22948 agents.py:69] training 4 of 2000: completed tf_agent.train(...) = 2066.112 [loss]\n",
      "I0624 23:30:30.615139 22948 agents.py:69] training 4 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:30:30.616261 22948 agents.py:69] training 5 of 2000: executing collect_driver.run()\n",
      "I0624 23:30:31.877067 22948 agents.py:69] training 5 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:30:31.880934 22948 agents.py:69] training 5 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:30:34.124992 22948 agents.py:69] training 5 of 2000: completed tf_agent.train(...) = 4471.945 [loss]\n",
      "I0624 23:30:34.127038 22948 agents.py:69] training 5 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:30:34.128527 22948 agents.py:69] training 6 of 2000: executing collect_driver.run()\n",
      "I0624 23:30:35.232539 22948 agents.py:69] training 6 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:30:35.237704 22948 agents.py:69] training 6 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:30:37.411507 22948 agents.py:69] training 6 of 2000: completed tf_agent.train(...) = 3080.956 [loss]\n",
      "I0624 23:30:37.413510 22948 agents.py:69] training 6 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:30:37.416476 22948 agents.py:69] training 7 of 2000: executing collect_driver.run()\n",
      "I0624 23:30:38.514307 22948 agents.py:69] training 7 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:30:38.518917 22948 agents.py:69] training 7 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:30:40.577728 22948 agents.py:69] training 7 of 2000: completed tf_agent.train(...) = 2541.774 [loss]\n",
      "I0624 23:30:40.578727 22948 agents.py:69] training 7 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:30:40.581733 22948 agents.py:69] training 8 of 2000: executing collect_driver.run()\n",
      "I0624 23:30:41.832924 22948 agents.py:69] training 8 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:30:41.837913 22948 agents.py:69] training 8 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:30:43.895989 22948 agents.py:69] training 8 of 2000: completed tf_agent.train(...) = 3226.341 [loss]\n",
      "I0624 23:30:43.898847 22948 agents.py:69] training 8 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:30:43.900851 22948 agents.py:69] training 9 of 2000: executing collect_driver.run()\n",
      "I0624 23:30:45.287056 22948 agents.py:69] training 9 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:30:45.292052 22948 agents.py:69] training 9 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:30:47.578154 22948 agents.py:69] training 9 of 2000: completed tf_agent.train(...) = 3558.501 [loss]\n",
      "I0624 23:30:47.580157 22948 agents.py:69] training 9 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:30:47.582187 22948 agents.py:69] training 10 of 2000: executing collect_driver.run()\n",
      "I0624 23:30:49.036216 22948 agents.py:69] training 10 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:30:49.038809 22948 agents.py:69] training 10 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:30:51.245012 22948 agents.py:69] training 10 of 2000: completed tf_agent.train(...) = 3194.312 [loss]\n",
      "I0624 23:30:51.247008 22948 agents.py:69] training 10 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:30:51.248074 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:31:06.144553 22948 agents.py:69] completed compute_avg_return(...) = -3.487\n",
      "I0624 23:31:06.145768 22948 agents.py:69] training 11 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:06.926177 22948 agents.py:69] training 11 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:06.929180 22948 agents.py:69] training 11 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:08.621640 22948 agents.py:69] training 11 of 2000: completed tf_agent.train(...) = 2786.988 [loss]\n",
      "I0624 23:31:08.622918 22948 agents.py:69] training 11 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:08.623639 22948 agents.py:69] training 12 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:09.406278 22948 agents.py:69] training 12 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:09.409275 22948 agents.py:69] training 12 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:11.012444 22948 agents.py:69] training 12 of 2000: completed tf_agent.train(...) = 2077.964 [loss]\n",
      "I0624 23:31:11.013444 22948 agents.py:69] training 12 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:11.014444 22948 agents.py:69] training 13 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:11.845993 22948 agents.py:69] training 13 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:11.849913 22948 agents.py:69] training 13 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:13.341684 22948 agents.py:69] training 13 of 2000: completed tf_agent.train(...) = 1627.137 [loss]\n",
      "I0624 23:31:13.342708 22948 agents.py:69] training 13 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:13.343681 22948 agents.py:69] training 14 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:14.016275 22948 agents.py:69] training 14 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:14.019274 22948 agents.py:69] training 14 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:15.374220 22948 agents.py:69] training 14 of 2000: completed tf_agent.train(...) = 1448.442 [loss]\n",
      "I0624 23:31:15.375253 22948 agents.py:69] training 14 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:15.376262 22948 agents.py:69] training 15 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:16.011423 22948 agents.py:69] training 15 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:16.013458 22948 agents.py:69] training 15 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:17.360447 22948 agents.py:69] training 15 of 2000: completed tf_agent.train(...) = 1178.359 [loss]\n",
      "I0624 23:31:17.361446 22948 agents.py:69] training 15 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:17.362446 22948 agents.py:69] training 16 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:17.728179 22948 agents.py:69] training 16 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:17.730351 22948 agents.py:69] training 16 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:18.955412 22948 agents.py:69] training 16 of 2000: completed tf_agent.train(...) = 284.171 [loss]\n",
      "I0624 23:31:18.956412 22948 agents.py:69] training 16 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:18.957521 22948 agents.py:69] training 17 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:19.385428 22948 agents.py:69] training 17 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:19.388385 22948 agents.py:69] training 17 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:20.638285 22948 agents.py:69] training 17 of 2000: completed tf_agent.train(...) = 704.158 [loss]\n",
      "I0624 23:31:20.640406 22948 agents.py:69] training 17 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:20.641454 22948 agents.py:69] training 18 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:21.142526 22948 agents.py:69] training 18 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:21.144561 22948 agents.py:69] training 18 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:22.511362 22948 agents.py:69] training 18 of 2000: completed tf_agent.train(...) = 288.619 [loss]\n",
      "I0624 23:31:22.512412 22948 agents.py:69] training 18 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:22.513419 22948 agents.py:69] training 19 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:23.040497 22948 agents.py:69] training 19 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:23.043791 22948 agents.py:69] training 19 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:24.355504 22948 agents.py:69] training 19 of 2000: completed tf_agent.train(...) = 316.396 [loss]\n",
      "I0624 23:31:24.356590 22948 agents.py:69] training 19 of 2000: executing replay_buffer.clear()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:31:24.358513 22948 agents.py:69] training 20 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:24.848925 22948 agents.py:69] training 20 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:24.852054 22948 agents.py:69] training 20 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:26.161526 22948 agents.py:69] training 20 of 2000: completed tf_agent.train(...) = 380.253 [loss]\n",
      "I0624 23:31:26.162526 22948 agents.py:69] training 20 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:26.164525 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:31:35.020688 22948 agents.py:69] completed compute_avg_return(...) = -3.267\n",
      "I0624 23:31:35.021529 22948 agents.py:69] training 21 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:35.349219 22948 agents.py:69] training 21 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:35.352220 22948 agents.py:69] training 21 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:36.614161 22948 agents.py:69] training 21 of 2000: completed tf_agent.train(...) = 159.947 [loss]\n",
      "I0624 23:31:36.616061 22948 agents.py:69] training 21 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:36.617064 22948 agents.py:69] training 22 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:36.979650 22948 agents.py:69] training 22 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:36.982664 22948 agents.py:69] training 22 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:38.236829 22948 agents.py:69] training 22 of 2000: completed tf_agent.train(...) = 205.248 [loss]\n",
      "I0624 23:31:38.237978 22948 agents.py:69] training 22 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:38.239774 22948 agents.py:69] training 23 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:38.598443 22948 agents.py:69] training 23 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:38.601308 22948 agents.py:69] training 23 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:39.856396 22948 agents.py:69] training 23 of 2000: completed tf_agent.train(...) = 177.057 [loss]\n",
      "I0624 23:31:39.857433 22948 agents.py:69] training 23 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:39.858398 22948 agents.py:69] training 24 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:40.119568 22948 agents.py:69] training 24 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:40.123574 22948 agents.py:69] training 24 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:41.439501 22948 agents.py:69] training 24 of 2000: completed tf_agent.train(...) = 107.806 [loss]\n",
      "I0624 23:31:41.440502 22948 agents.py:69] training 24 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:41.442502 22948 agents.py:69] training 25 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:42.012141 22948 agents.py:69] training 25 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:42.015418 22948 agents.py:69] training 25 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:43.311244 22948 agents.py:69] training 25 of 2000: completed tf_agent.train(...) = 113.104 [loss]\n",
      "I0624 23:31:43.312245 22948 agents.py:69] training 25 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:43.313311 22948 agents.py:69] training 26 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:43.614258 22948 agents.py:69] training 26 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:43.617381 22948 agents.py:69] training 26 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:44.823119 22948 agents.py:69] training 26 of 2000: completed tf_agent.train(...) = 145.788 [loss]\n",
      "I0624 23:31:44.824153 22948 agents.py:69] training 26 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:44.825138 22948 agents.py:69] training 27 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:45.168328 22948 agents.py:69] training 27 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:45.171328 22948 agents.py:69] training 27 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:45.316876 22948 agents.py:69] training 27 of 2000: completed tf_agent.train(...) = 108.770 [loss]\n",
      "I0624 23:31:45.317877 22948 agents.py:69] training 27 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:45.318875 22948 agents.py:69] training 28 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:45.743304 22948 agents.py:69] training 28 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:45.746305 22948 agents.py:69] training 28 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:47.067282 22948 agents.py:69] training 28 of 2000: completed tf_agent.train(...) = 100.187 [loss]\n",
      "I0624 23:31:47.068280 22948 agents.py:69] training 28 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:47.069281 22948 agents.py:69] training 29 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:47.550234 22948 agents.py:69] training 29 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:47.553233 22948 agents.py:69] training 29 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:49.082250 22948 agents.py:69] training 29 of 2000: completed tf_agent.train(...) = 232.241 [loss]\n",
      "I0624 23:31:49.083251 22948 agents.py:69] training 29 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:49.085251 22948 agents.py:69] training 30 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:49.350507 22948 agents.py:69] training 30 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:49.353538 22948 agents.py:69] training 30 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:31:50.534566 22948 agents.py:69] training 30 of 2000: completed tf_agent.train(...) = 118.115 [loss]\n",
      "I0624 23:31:50.535439 22948 agents.py:69] training 30 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:31:50.537082 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:31:59.099285 22948 agents.py:69] completed compute_avg_return(...) = -2.433\n",
      "I0624 23:31:59.100470 22948 agents.py:69] training 31 of 2000: executing collect_driver.run()\n",
      "I0624 23:31:59.456569 22948 agents.py:69] training 31 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:31:59.458566 22948 agents.py:69] training 31 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:00.727308 22948 agents.py:69] training 31 of 2000: completed tf_agent.train(...) = 128.821 [loss]\n",
      "I0624 23:32:00.728344 22948 agents.py:69] training 31 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:00.730355 22948 agents.py:69] training 32 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:01.033500 22948 agents.py:69] training 32 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:01.036519 22948 agents.py:69] training 32 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:02.271768 22948 agents.py:69] training 32 of 2000: completed tf_agent.train(...) = 130.504 [loss]\n",
      "I0624 23:32:02.272767 22948 agents.py:69] training 32 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:02.273768 22948 agents.py:69] training 33 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:02.546315 22948 agents.py:69] training 33 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:02.548565 22948 agents.py:69] training 33 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:03.749431 22948 agents.py:69] training 33 of 2000: completed tf_agent.train(...) = 70.466 [loss]\n",
      "I0624 23:32:03.750428 22948 agents.py:69] training 33 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:03.751429 22948 agents.py:69] training 34 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:04.055560 22948 agents.py:69] training 34 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:04.057592 22948 agents.py:69] training 34 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:05.349572 22948 agents.py:69] training 34 of 2000: completed tf_agent.train(...) = 81.392 [loss]\n",
      "I0624 23:32:05.350571 22948 agents.py:69] training 34 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:05.351572 22948 agents.py:69] training 35 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:05.684429 22948 agents.py:69] training 35 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:05.687512 22948 agents.py:69] training 35 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:06.957124 22948 agents.py:69] training 35 of 2000: completed tf_agent.train(...) = 46.266 [loss]\n",
      "I0624 23:32:06.958212 22948 agents.py:69] training 35 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:06.960124 22948 agents.py:69] training 36 of 2000: executing collect_driver.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:32:07.258520 22948 agents.py:69] training 36 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:07.261487 22948 agents.py:69] training 36 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:08.541453 22948 agents.py:69] training 36 of 2000: completed tf_agent.train(...) = 72.798 [loss]\n",
      "I0624 23:32:08.542457 22948 agents.py:69] training 36 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:08.543454 22948 agents.py:69] training 37 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:08.821324 22948 agents.py:69] training 37 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:08.824326 22948 agents.py:69] training 37 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:10.093856 22948 agents.py:69] training 37 of 2000: completed tf_agent.train(...) = 59.172 [loss]\n",
      "I0624 23:32:10.095476 22948 agents.py:69] training 37 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:10.096465 22948 agents.py:69] training 38 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:10.449200 22948 agents.py:69] training 38 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:10.452200 22948 agents.py:69] training 38 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:11.745445 22948 agents.py:69] training 38 of 2000: completed tf_agent.train(...) = 47.416 [loss]\n",
      "I0624 23:32:11.747435 22948 agents.py:69] training 38 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:11.748433 22948 agents.py:69] training 39 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:12.018401 22948 agents.py:69] training 39 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:12.021402 22948 agents.py:69] training 39 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:13.261426 22948 agents.py:69] training 39 of 2000: completed tf_agent.train(...) = 187.193 [loss]\n",
      "I0624 23:32:13.263429 22948 agents.py:69] training 39 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:13.265430 22948 agents.py:69] training 40 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:13.547421 22948 agents.py:69] training 40 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:13.550418 22948 agents.py:69] training 40 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:14.851378 22948 agents.py:69] training 40 of 2000: completed tf_agent.train(...) = 60.916 [loss]\n",
      "I0624 23:32:14.852429 22948 agents.py:69] training 40 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:14.853479 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:32:23.217476 22948 agents.py:69] completed compute_avg_return(...) = -2.041\n",
      "I0624 23:32:23.218692 22948 agents.py:69] training 41 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:23.459448 22948 agents.py:69] training 41 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:23.461445 22948 agents.py:69] training 41 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:24.756809 22948 agents.py:69] training 41 of 2000: completed tf_agent.train(...) = 69.983 [loss]\n",
      "I0624 23:32:24.757592 22948 agents.py:69] training 41 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:24.759590 22948 agents.py:69] training 42 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:25.054233 22948 agents.py:69] training 42 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:25.057320 22948 agents.py:69] training 42 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:26.341505 22948 agents.py:69] training 42 of 2000: completed tf_agent.train(...) = 58.591 [loss]\n",
      "I0624 23:32:26.343317 22948 agents.py:69] training 42 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:26.344316 22948 agents.py:69] training 43 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:26.801164 22948 agents.py:69] training 43 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:26.803166 22948 agents.py:69] training 43 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:28.166467 22948 agents.py:69] training 43 of 2000: completed tf_agent.train(...) = 152.026 [loss]\n",
      "I0624 23:32:28.167467 22948 agents.py:69] training 43 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:28.168471 22948 agents.py:69] training 44 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:28.454387 22948 agents.py:69] training 44 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:28.457388 22948 agents.py:69] training 44 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:29.713566 22948 agents.py:69] training 44 of 2000: completed tf_agent.train(...) = 57.312 [loss]\n",
      "I0624 23:32:29.715565 22948 agents.py:69] training 44 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:29.716568 22948 agents.py:69] training 45 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:30.004384 22948 agents.py:69] training 45 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:30.007381 22948 agents.py:69] training 45 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:31.244546 22948 agents.py:69] training 45 of 2000: completed tf_agent.train(...) = 51.384 [loss]\n",
      "I0624 23:32:31.246546 22948 agents.py:69] training 45 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:31.247545 22948 agents.py:69] training 46 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:31.488692 22948 agents.py:69] training 46 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:31.491776 22948 agents.py:69] training 46 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:33.053393 22948 agents.py:69] training 46 of 2000: completed tf_agent.train(...) = 48.598 [loss]\n",
      "I0624 23:32:33.056399 22948 agents.py:69] training 46 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:33.057389 22948 agents.py:69] training 47 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:33.447278 22948 agents.py:69] training 47 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:33.452265 22948 agents.py:69] training 47 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:35.519165 22948 agents.py:69] training 47 of 2000: completed tf_agent.train(...) = 41.706 [loss]\n",
      "I0624 23:32:35.521217 22948 agents.py:69] training 47 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:35.523083 22948 agents.py:69] training 48 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:35.818543 22948 agents.py:69] training 48 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:35.823547 22948 agents.py:69] training 48 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:37.831485 22948 agents.py:69] training 48 of 2000: completed tf_agent.train(...) = 74.229 [loss]\n",
      "I0624 23:32:37.834491 22948 agents.py:69] training 48 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:37.837486 22948 agents.py:69] training 49 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:38.223212 22948 agents.py:69] training 49 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:38.229213 22948 agents.py:69] training 49 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:38.389378 22948 agents.py:69] training 49 of 2000: completed tf_agent.train(...) = 50.743 [loss]\n",
      "I0624 23:32:38.392377 22948 agents.py:69] training 49 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:38.395067 22948 agents.py:69] training 50 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:38.836656 22948 agents.py:69] training 50 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:38.841015 22948 agents.py:69] training 50 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:40.918460 22948 agents.py:69] training 50 of 2000: completed tf_agent.train(...) = 37.507 [loss]\n",
      "I0624 23:32:40.920460 22948 agents.py:69] training 50 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:40.923457 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:32:51.440362 22948 agents.py:69] completed compute_avg_return(...) = -2.216\n",
      "I0624 23:32:51.441172 22948 agents.py:69] training 51 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:51.634874 22948 agents.py:69] training 51 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:51.636906 22948 agents.py:69] training 51 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:52.828362 22948 agents.py:69] training 51 of 2000: completed tf_agent.train(...) = 45.073 [loss]\n",
      "I0624 23:32:52.829370 22948 agents.py:69] training 51 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:52.831364 22948 agents.py:69] training 52 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:53.125664 22948 agents.py:69] training 52 of 2000: executing replay_buffer.gather_all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:32:53.128631 22948 agents.py:69] training 52 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:54.408232 22948 agents.py:69] training 52 of 2000: completed tf_agent.train(...) = 118.696 [loss]\n",
      "I0624 23:32:54.409232 22948 agents.py:69] training 52 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:54.410234 22948 agents.py:69] training 53 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:54.594535 22948 agents.py:69] training 53 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:54.597492 22948 agents.py:69] training 53 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:55.786458 22948 agents.py:69] training 53 of 2000: completed tf_agent.train(...) = 50.315 [loss]\n",
      "I0624 23:32:55.787443 22948 agents.py:69] training 53 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:55.789440 22948 agents.py:69] training 54 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:55.987658 22948 agents.py:69] training 54 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:55.990775 22948 agents.py:69] training 54 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:57.176417 22948 agents.py:69] training 54 of 2000: completed tf_agent.train(...) = 42.529 [loss]\n",
      "I0624 23:32:57.177588 22948 agents.py:69] training 54 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:57.178416 22948 agents.py:69] training 55 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:57.434370 22948 agents.py:69] training 55 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:57.437601 22948 agents.py:69] training 55 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:32:58.683772 22948 agents.py:69] training 55 of 2000: completed tf_agent.train(...) = 64.398 [loss]\n",
      "I0624 23:32:58.685315 22948 agents.py:69] training 55 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:32:58.686294 22948 agents.py:69] training 56 of 2000: executing collect_driver.run()\n",
      "I0624 23:32:59.001519 22948 agents.py:69] training 56 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:32:59.004599 22948 agents.py:69] training 56 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:00.301692 22948 agents.py:69] training 56 of 2000: completed tf_agent.train(...) = 48.600 [loss]\n",
      "I0624 23:33:00.302799 22948 agents.py:69] training 56 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:00.303695 22948 agents.py:69] training 57 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:00.483675 22948 agents.py:69] training 57 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:00.486630 22948 agents.py:69] training 57 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:01.675385 22948 agents.py:69] training 57 of 2000: completed tf_agent.train(...) = 39.002 [loss]\n",
      "I0624 23:33:01.676448 22948 agents.py:69] training 57 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:01.678376 22948 agents.py:69] training 58 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:01.951223 22948 agents.py:69] training 58 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:01.955173 22948 agents.py:69] training 58 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:03.198258 22948 agents.py:69] training 58 of 2000: completed tf_agent.train(...) = 61.911 [loss]\n",
      "I0624 23:33:03.199258 22948 agents.py:69] training 58 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:03.200257 22948 agents.py:69] training 59 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:03.365937 22948 agents.py:69] training 59 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:03.369216 22948 agents.py:69] training 59 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:04.605484 22948 agents.py:69] training 59 of 2000: completed tf_agent.train(...) = 53.268 [loss]\n",
      "I0624 23:33:04.606483 22948 agents.py:69] training 59 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:04.608032 22948 agents.py:69] training 60 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:04.779386 22948 agents.py:69] training 60 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:04.781385 22948 agents.py:69] training 60 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:04.867621 22948 agents.py:69] training 60 of 2000: completed tf_agent.train(...) = 38.466 [loss]\n",
      "I0624 23:33:04.868993 22948 agents.py:69] training 60 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:04.869865 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:33:13.732157 22948 agents.py:69] completed compute_avg_return(...) = -2.751\n",
      "I0624 23:33:13.733248 22948 agents.py:69] training 61 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:13.978419 22948 agents.py:69] training 61 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:13.981417 22948 agents.py:69] training 61 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:15.407438 22948 agents.py:69] training 61 of 2000: completed tf_agent.train(...) = 38.775 [loss]\n",
      "I0624 23:33:15.408495 22948 agents.py:69] training 61 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:15.410436 22948 agents.py:69] training 62 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:15.718313 22948 agents.py:69] training 62 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:15.719811 22948 agents.py:69] training 62 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:16.963986 22948 agents.py:69] training 62 of 2000: completed tf_agent.train(...) = 35.196 [loss]\n",
      "I0624 23:33:16.965258 22948 agents.py:69] training 62 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:16.968245 22948 agents.py:69] training 63 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:17.141398 22948 agents.py:69] training 63 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:17.144400 22948 agents.py:69] training 63 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:18.382137 22948 agents.py:69] training 63 of 2000: completed tf_agent.train(...) = 43.271 [loss]\n",
      "I0624 23:33:18.383448 22948 agents.py:69] training 63 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:18.384199 22948 agents.py:69] training 64 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:18.594715 22948 agents.py:69] training 64 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:18.597749 22948 agents.py:69] training 64 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:19.815231 22948 agents.py:69] training 64 of 2000: completed tf_agent.train(...) = 22.056 [loss]\n",
      "I0624 23:33:19.816215 22948 agents.py:69] training 64 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:19.818214 22948 agents.py:69] training 65 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:20.099457 22948 agents.py:69] training 65 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:20.103267 22948 agents.py:69] training 65 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:20.256381 22948 agents.py:69] training 65 of 2000: completed tf_agent.train(...) = 70.509 [loss]\n",
      "I0624 23:33:20.257316 22948 agents.py:69] training 65 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:20.259316 22948 agents.py:69] training 66 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:20.537005 22948 agents.py:69] training 66 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:20.539798 22948 agents.py:69] training 66 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:21.854434 22948 agents.py:69] training 66 of 2000: completed tf_agent.train(...) = 30.863 [loss]\n",
      "I0624 23:33:21.854434 22948 agents.py:69] training 66 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:21.856434 22948 agents.py:69] training 67 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:22.031251 22948 agents.py:69] training 67 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:22.034041 22948 agents.py:69] training 67 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:23.253390 22948 agents.py:69] training 67 of 2000: completed tf_agent.train(...) = 31.409 [loss]\n",
      "I0624 23:33:23.255371 22948 agents.py:69] training 67 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:23.256442 22948 agents.py:69] training 68 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:23.482022 22948 agents.py:69] training 68 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:23.485117 22948 agents.py:69] training 68 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:24.746668 22948 agents.py:69] training 68 of 2000: completed tf_agent.train(...) = 31.245 [loss]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:33:24.747669 22948 agents.py:69] training 68 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:24.749670 22948 agents.py:69] training 69 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:24.990671 22948 agents.py:69] training 69 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:24.993669 22948 agents.py:69] training 69 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:26.241714 22948 agents.py:69] training 69 of 2000: completed tf_agent.train(...) = 34.296 [loss]\n",
      "I0624 23:33:26.242713 22948 agents.py:69] training 69 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:26.243848 22948 agents.py:69] training 70 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:26.412421 22948 agents.py:69] training 70 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:26.414381 22948 agents.py:69] training 70 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:27.664072 22948 agents.py:69] training 70 of 2000: completed tf_agent.train(...) = 43.763 [loss]\n",
      "I0624 23:33:27.665071 22948 agents.py:69] training 70 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:27.667069 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:33:37.026561 22948 agents.py:69] completed compute_avg_return(...) = -2.557\n",
      "I0624 23:33:37.028213 22948 agents.py:69] training 71 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:37.298448 22948 agents.py:69] training 71 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:37.301413 22948 agents.py:69] training 71 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:37.511421 22948 agents.py:69] training 71 of 2000: completed tf_agent.train(...) = 28.400 [loss]\n",
      "I0624 23:33:37.513469 22948 agents.py:69] training 71 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:37.517394 22948 agents.py:69] training 72 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:37.907663 22948 agents.py:69] training 72 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:37.911659 22948 agents.py:69] training 72 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:40.170762 22948 agents.py:69] training 72 of 2000: completed tf_agent.train(...) = 34.498 [loss]\n",
      "I0624 23:33:40.171837 22948 agents.py:69] training 72 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:40.173765 22948 agents.py:69] training 73 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:40.515767 22948 agents.py:69] training 73 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:40.519777 22948 agents.py:69] training 73 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:42.093814 22948 agents.py:69] training 73 of 2000: completed tf_agent.train(...) = 86.016 [loss]\n",
      "I0624 23:33:42.095409 22948 agents.py:69] training 73 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:42.097397 22948 agents.py:69] training 74 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:42.312060 22948 agents.py:69] training 74 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:42.315066 22948 agents.py:69] training 74 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:43.754081 22948 agents.py:69] training 74 of 2000: completed tf_agent.train(...) = 33.782 [loss]\n",
      "I0624 23:33:43.755083 22948 agents.py:69] training 74 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:43.756160 22948 agents.py:69] training 75 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:43.978330 22948 agents.py:69] training 75 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:43.981294 22948 agents.py:69] training 75 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:45.318322 22948 agents.py:69] training 75 of 2000: completed tf_agent.train(...) = 31.565 [loss]\n",
      "I0624 23:33:45.318322 22948 agents.py:69] training 75 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:45.320323 22948 agents.py:69] training 76 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:45.558065 22948 agents.py:69] training 76 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:45.561068 22948 agents.py:69] training 76 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:47.211406 22948 agents.py:69] training 76 of 2000: completed tf_agent.train(...) = 32.934 [loss]\n",
      "I0624 23:33:47.212405 22948 agents.py:69] training 76 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:47.214438 22948 agents.py:69] training 77 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:47.577483 22948 agents.py:69] training 77 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:47.580537 22948 agents.py:69] training 77 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:49.131561 22948 agents.py:69] training 77 of 2000: completed tf_agent.train(...) = 83.725 [loss]\n",
      "I0624 23:33:49.132562 22948 agents.py:69] training 77 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:49.134562 22948 agents.py:69] training 78 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:49.394545 22948 agents.py:69] training 78 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:49.397605 22948 agents.py:69] training 78 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:50.892340 22948 agents.py:69] training 78 of 2000: completed tf_agent.train(...) = 31.271 [loss]\n",
      "I0624 23:33:50.893342 22948 agents.py:69] training 78 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:50.894340 22948 agents.py:69] training 79 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:51.054642 22948 agents.py:69] training 79 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:51.057851 22948 agents.py:69] training 79 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:52.434028 22948 agents.py:69] training 79 of 2000: completed tf_agent.train(...) = 32.601 [loss]\n",
      "I0624 23:33:52.435095 22948 agents.py:69] training 79 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:52.435653 22948 agents.py:69] training 80 of 2000: executing collect_driver.run()\n",
      "I0624 23:33:52.605144 22948 agents.py:69] training 80 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:33:52.607181 22948 agents.py:69] training 80 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:33:54.033684 22948 agents.py:69] training 80 of 2000: completed tf_agent.train(...) = 21.378 [loss]\n",
      "I0624 23:33:54.034683 22948 agents.py:69] training 80 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:33:54.036685 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:34:00.090335 22948 agents.py:69] completed compute_avg_return(...) = -1.296\n",
      "I0624 23:34:00.091122 22948 agents.py:69] training 81 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:00.235750 22948 agents.py:69] training 81 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:00.237750 22948 agents.py:69] training 81 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:01.554578 22948 agents.py:69] training 81 of 2000: completed tf_agent.train(...) = 30.468 [loss]\n",
      "I0624 23:34:01.555596 22948 agents.py:69] training 81 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:01.557609 22948 agents.py:69] training 82 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:01.746890 22948 agents.py:69] training 82 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:01.748890 22948 agents.py:69] training 82 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:03.055617 22948 agents.py:69] training 82 of 2000: completed tf_agent.train(...) = 38.566 [loss]\n",
      "I0624 23:34:03.057488 22948 agents.py:69] training 82 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:03.058490 22948 agents.py:69] training 83 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:03.235832 22948 agents.py:69] training 83 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:03.237775 22948 agents.py:69] training 83 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:04.534710 22948 agents.py:69] training 83 of 2000: completed tf_agent.train(...) = 21.266 [loss]\n",
      "I0624 23:34:04.536709 22948 agents.py:69] training 83 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:04.537711 22948 agents.py:69] training 84 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:04.751302 22948 agents.py:69] training 84 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:04.753300 22948 agents.py:69] training 84 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:06.057474 22948 agents.py:69] training 84 of 2000: completed tf_agent.train(...) = 25.239 [loss]\n",
      "I0624 23:34:06.058537 22948 agents.py:69] training 84 of 2000: executing replay_buffer.clear()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:34:06.059476 22948 agents.py:69] training 85 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:06.253449 22948 agents.py:69] training 85 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:06.256952 22948 agents.py:69] training 85 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:07.675308 22948 agents.py:69] training 85 of 2000: completed tf_agent.train(...) = 17.596 [loss]\n",
      "I0624 23:34:07.676433 22948 agents.py:69] training 85 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:07.678311 22948 agents.py:69] training 86 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:07.806330 22948 agents.py:69] training 86 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:07.809330 22948 agents.py:69] training 86 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:09.072879 22948 agents.py:69] training 86 of 2000: completed tf_agent.train(...) = 24.925 [loss]\n",
      "I0624 23:34:09.074880 22948 agents.py:69] training 86 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:09.077238 22948 agents.py:69] training 87 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:09.374198 22948 agents.py:69] training 87 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:09.378279 22948 agents.py:69] training 87 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:11.241826 22948 agents.py:69] training 87 of 2000: completed tf_agent.train(...) = 36.329 [loss]\n",
      "I0624 23:34:11.243440 22948 agents.py:69] training 87 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:11.243826 22948 agents.py:69] training 88 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:11.428007 22948 agents.py:69] training 88 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:11.431661 22948 agents.py:69] training 88 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:11.570622 22948 agents.py:69] training 88 of 2000: completed tf_agent.train(...) = 18.830 [loss]\n",
      "I0624 23:34:11.573620 22948 agents.py:69] training 88 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:11.575691 22948 agents.py:69] training 89 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:11.797872 22948 agents.py:69] training 89 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:11.802870 22948 agents.py:69] training 89 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:13.662286 22948 agents.py:69] training 89 of 2000: completed tf_agent.train(...) = 26.596 [loss]\n",
      "I0624 23:34:13.664271 22948 agents.py:69] training 89 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:13.666425 22948 agents.py:69] training 90 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:14.042053 22948 agents.py:69] training 90 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:14.047094 22948 agents.py:69] training 90 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:15.894441 22948 agents.py:69] training 90 of 2000: completed tf_agent.train(...) = 24.266 [loss]\n",
      "I0624 23:34:15.896436 22948 agents.py:69] training 90 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:15.898517 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:34:30.038444 22948 agents.py:69] completed compute_avg_return(...) = -3.077\n",
      "I0624 23:34:30.039407 22948 agents.py:69] training 91 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:30.402256 22948 agents.py:69] training 91 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:30.407387 22948 agents.py:69] training 91 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:32.099246 22948 agents.py:69] training 91 of 2000: completed tf_agent.train(...) = 18.018 [loss]\n",
      "I0624 23:34:32.100351 22948 agents.py:69] training 91 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:32.101257 22948 agents.py:69] training 92 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:32.283361 22948 agents.py:69] training 92 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:32.286194 22948 agents.py:69] training 92 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:33.576521 22948 agents.py:69] training 92 of 2000: completed tf_agent.train(...) = 43.052 [loss]\n",
      "I0624 23:34:33.577531 22948 agents.py:69] training 92 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:33.579547 22948 agents.py:69] training 93 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:33.935909 22948 agents.py:69] training 93 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:33.939914 22948 agents.py:69] training 93 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:35.595598 22948 agents.py:69] training 93 of 2000: completed tf_agent.train(...) = 70.025 [loss]\n",
      "I0624 23:34:35.596535 22948 agents.py:69] training 93 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:35.598558 22948 agents.py:69] training 94 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:35.883263 22948 agents.py:69] training 94 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:35.887267 22948 agents.py:69] training 94 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:37.513177 22948 agents.py:69] training 94 of 2000: completed tf_agent.train(...) = 16.947 [loss]\n",
      "I0624 23:34:37.514177 22948 agents.py:69] training 94 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:37.515291 22948 agents.py:69] training 95 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:37.678701 22948 agents.py:69] training 95 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:37.680433 22948 agents.py:69] training 95 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:39.080254 22948 agents.py:69] training 95 of 2000: completed tf_agent.train(...) = 21.905 [loss]\n",
      "I0624 23:34:39.081279 22948 agents.py:69] training 95 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:39.082254 22948 agents.py:69] training 96 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:39.395443 22948 agents.py:69] training 96 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:39.398431 22948 agents.py:69] training 96 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:41.133811 22948 agents.py:69] training 96 of 2000: completed tf_agent.train(...) = 22.539 [loss]\n",
      "I0624 23:34:41.134810 22948 agents.py:69] training 96 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:41.137813 22948 agents.py:69] training 97 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:41.471385 22948 agents.py:69] training 97 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:41.475392 22948 agents.py:69] training 97 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:42.986178 22948 agents.py:69] training 97 of 2000: completed tf_agent.train(...) = 39.161 [loss]\n",
      "I0624 23:34:42.986178 22948 agents.py:69] training 97 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:42.988179 22948 agents.py:69] training 98 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:43.139827 22948 agents.py:69] training 98 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:43.143584 22948 agents.py:69] training 98 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:43.240256 22948 agents.py:69] training 98 of 2000: completed tf_agent.train(...) = 11.432 [loss]\n",
      "I0624 23:34:43.241338 22948 agents.py:69] training 98 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:43.242336 22948 agents.py:69] training 99 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:43.374784 22948 agents.py:69] training 99 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:43.376523 22948 agents.py:69] training 99 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:44.902205 22948 agents.py:69] training 99 of 2000: completed tf_agent.train(...) = 17.486 [loss]\n",
      "I0624 23:34:44.903398 22948 agents.py:69] training 99 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:44.904164 22948 agents.py:69] training 100 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:45.083182 22948 agents.py:69] training 100 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:45.088146 22948 agents.py:69] training 100 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:46.658503 22948 agents.py:69] training 100 of 2000: completed tf_agent.train(...) = 39.051 [loss]\n",
      "I0624 23:34:46.659568 22948 agents.py:69] training 100 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:46.660660 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:34:56.945644 22948 agents.py:69] completed compute_avg_return(...) = -1.578\n",
      "I0624 23:34:56.947653 22948 agents.py:69] training 101 of 2000: executing collect_driver.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:34:57.193612 22948 agents.py:69] training 101 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:57.197609 22948 agents.py:69] training 101 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:34:59.097719 22948 agents.py:69] training 101 of 2000: completed tf_agent.train(...) = 14.413 [loss]\n",
      "I0624 23:34:59.099725 22948 agents.py:69] training 101 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:34:59.102722 22948 agents.py:69] training 102 of 2000: executing collect_driver.run()\n",
      "I0624 23:34:59.349222 22948 agents.py:69] training 102 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:34:59.354189 22948 agents.py:69] training 102 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:01.315632 22948 agents.py:69] training 102 of 2000: completed tf_agent.train(...) = 12.975 [loss]\n",
      "I0624 23:35:01.317691 22948 agents.py:69] training 102 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:01.318616 22948 agents.py:69] training 103 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:01.570825 22948 agents.py:69] training 103 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:01.576729 22948 agents.py:69] training 103 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:01.713555 22948 agents.py:69] training 103 of 2000: completed tf_agent.train(...) = 10.838 [loss]\n",
      "I0624 23:35:01.714429 22948 agents.py:69] training 103 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:01.715539 22948 agents.py:69] training 104 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:01.982570 22948 agents.py:69] training 104 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:01.986765 22948 agents.py:69] training 104 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:03.865286 22948 agents.py:69] training 104 of 2000: completed tf_agent.train(...) = 19.174 [loss]\n",
      "I0624 23:35:03.867283 22948 agents.py:69] training 104 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:03.869284 22948 agents.py:69] training 105 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:04.204426 22948 agents.py:69] training 105 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:04.207657 22948 agents.py:69] training 105 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:06.251509 22948 agents.py:69] training 105 of 2000: completed tf_agent.train(...) = 40.002 [loss]\n",
      "I0624 23:35:06.252510 22948 agents.py:69] training 105 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:06.254508 22948 agents.py:69] training 106 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:06.428528 22948 agents.py:69] training 106 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:06.431534 22948 agents.py:69] training 106 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:08.276655 22948 agents.py:69] training 106 of 2000: completed tf_agent.train(...) = 7.753 [loss]\n",
      "I0624 23:35:08.278653 22948 agents.py:69] training 106 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:08.280652 22948 agents.py:69] training 107 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:08.522584 22948 agents.py:69] training 107 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:08.527641 22948 agents.py:69] training 107 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:10.484704 22948 agents.py:69] training 107 of 2000: completed tf_agent.train(...) = 25.182 [loss]\n",
      "I0624 23:35:10.486824 22948 agents.py:69] training 107 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:10.487825 22948 agents.py:69] training 108 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:10.760532 22948 agents.py:69] training 108 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:10.764524 22948 agents.py:69] training 108 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:10.902384 22948 agents.py:69] training 108 of 2000: completed tf_agent.train(...) = 12.068 [loss]\n",
      "I0624 23:35:10.903610 22948 agents.py:69] training 108 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:10.905718 22948 agents.py:69] training 109 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:11.160418 22948 agents.py:69] training 109 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:11.163415 22948 agents.py:69] training 109 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:13.053597 22948 agents.py:69] training 109 of 2000: completed tf_agent.train(...) = 14.747 [loss]\n",
      "I0624 23:35:13.054309 22948 agents.py:69] training 109 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:13.056490 22948 agents.py:69] training 110 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:13.355412 22948 agents.py:69] training 110 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:13.360408 22948 agents.py:69] training 110 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:15.299432 22948 agents.py:69] training 110 of 2000: completed tf_agent.train(...) = 10.739 [loss]\n",
      "I0624 23:35:15.300432 22948 agents.py:69] training 110 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:15.303433 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:35:24.688771 22948 agents.py:69] completed compute_avg_return(...) = -1.259\n",
      "I0624 23:35:24.690719 22948 agents.py:69] training 111 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:24.953400 22948 agents.py:69] training 111 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:24.957357 22948 agents.py:69] training 111 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:27.012284 22948 agents.py:69] training 111 of 2000: completed tf_agent.train(...) = 7.817 [loss]\n",
      "I0624 23:35:27.015281 22948 agents.py:69] training 111 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:27.017278 22948 agents.py:69] training 112 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:27.285493 22948 agents.py:69] training 112 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:27.289677 22948 agents.py:69] training 112 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:29.300391 22948 agents.py:69] training 112 of 2000: completed tf_agent.train(...) = 8.368 [loss]\n",
      "I0624 23:35:29.302546 22948 agents.py:69] training 112 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:29.304508 22948 agents.py:69] training 113 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:29.537115 22948 agents.py:69] training 113 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:29.542124 22948 agents.py:69] training 113 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:31.568352 22948 agents.py:69] training 113 of 2000: completed tf_agent.train(...) = 6.187 [loss]\n",
      "I0624 23:35:31.570273 22948 agents.py:69] training 113 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:31.571392 22948 agents.py:69] training 114 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:31.833223 22948 agents.py:69] training 114 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:31.837917 22948 agents.py:69] training 114 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:33.830434 22948 agents.py:69] training 114 of 2000: completed tf_agent.train(...) = 11.636 [loss]\n",
      "I0624 23:35:33.832329 22948 agents.py:69] training 114 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:33.833437 22948 agents.py:69] training 115 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:34.128253 22948 agents.py:69] training 115 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:34.132377 22948 agents.py:69] training 115 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:36.076442 22948 agents.py:69] training 115 of 2000: completed tf_agent.train(...) = 30.375 [loss]\n",
      "I0624 23:35:36.078279 22948 agents.py:69] training 115 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:36.080399 22948 agents.py:69] training 116 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:36.306425 22948 agents.py:69] training 116 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:36.310197 22948 agents.py:69] training 116 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:38.283057 22948 agents.py:69] training 116 of 2000: completed tf_agent.train(...) = 4.683 [loss]\n",
      "I0624 23:35:38.286049 22948 agents.py:69] training 116 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:38.288052 22948 agents.py:69] training 117 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:38.566948 22948 agents.py:69] training 117 of 2000: executing replay_buffer.gather_all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:35:38.569967 22948 agents.py:69] training 117 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:38.737944 22948 agents.py:69] training 117 of 2000: completed tf_agent.train(...) = 11.656 [loss]\n",
      "I0624 23:35:38.739476 22948 agents.py:69] training 117 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:38.740942 22948 agents.py:69] training 118 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:38.991543 22948 agents.py:69] training 118 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:38.995601 22948 agents.py:69] training 118 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:39.161142 22948 agents.py:69] training 118 of 2000: completed tf_agent.train(...) = 24.511 [loss]\n",
      "I0624 23:35:39.163142 22948 agents.py:69] training 118 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:39.166148 22948 agents.py:69] training 119 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:39.355225 22948 agents.py:69] training 119 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:39.359210 22948 agents.py:69] training 119 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:41.397894 22948 agents.py:69] training 119 of 2000: completed tf_agent.train(...) = 7.347 [loss]\n",
      "I0624 23:35:41.399762 22948 agents.py:69] training 119 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:41.402765 22948 agents.py:69] training 120 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:41.657427 22948 agents.py:69] training 120 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:41.662383 22948 agents.py:69] training 120 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:43.581349 22948 agents.py:69] training 120 of 2000: completed tf_agent.train(...) = 4.301 [loss]\n",
      "I0624 23:35:43.582350 22948 agents.py:69] training 120 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:43.585357 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:35:49.977434 22948 agents.py:69] completed compute_avg_return(...) = -0.452\n",
      "I0624 23:35:49.978432 22948 agents.py:69] training 121 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:50.277641 22948 agents.py:69] training 121 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:50.282599 22948 agents.py:69] training 121 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:52.242630 22948 agents.py:69] training 121 of 2000: completed tf_agent.train(...) = 4.680 [loss]\n",
      "I0624 23:35:52.243628 22948 agents.py:69] training 121 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:52.245279 22948 agents.py:69] training 122 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:52.527328 22948 agents.py:69] training 122 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:52.532337 22948 agents.py:69] training 122 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:52.719496 22948 agents.py:69] training 122 of 2000: completed tf_agent.train(...) = 132.847 [loss]\n",
      "I0624 23:35:52.722497 22948 agents.py:69] training 122 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:52.724837 22948 agents.py:69] training 123 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:53.061779 22948 agents.py:69] training 123 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:53.065467 22948 agents.py:69] training 123 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:55.118723 22948 agents.py:69] training 123 of 2000: completed tf_agent.train(...) = 19.506 [loss]\n",
      "I0624 23:35:55.119719 22948 agents.py:69] training 123 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:55.122722 22948 agents.py:69] training 124 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:55.301981 22948 agents.py:69] training 124 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:55.305131 22948 agents.py:69] training 124 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:57.246501 22948 agents.py:69] training 124 of 2000: completed tf_agent.train(...) = 9.834 [loss]\n",
      "I0624 23:35:57.248500 22948 agents.py:69] training 124 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:57.250693 22948 agents.py:69] training 125 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:57.523282 22948 agents.py:69] training 125 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:57.528402 22948 agents.py:69] training 125 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:57.686659 22948 agents.py:69] training 125 of 2000: completed tf_agent.train(...) = 9.864 [loss]\n",
      "I0624 23:35:57.687544 22948 agents.py:69] training 125 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:57.689544 22948 agents.py:69] training 126 of 2000: executing collect_driver.run()\n",
      "I0624 23:35:57.895681 22948 agents.py:69] training 126 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:35:57.898692 22948 agents.py:69] training 126 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:35:59.852106 22948 agents.py:69] training 126 of 2000: completed tf_agent.train(...) = 12.260 [loss]\n",
      "I0624 23:35:59.852811 22948 agents.py:69] training 126 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:35:59.854814 22948 agents.py:69] training 127 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:00.148592 22948 agents.py:69] training 127 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:00.151541 22948 agents.py:69] training 127 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:00.294980 22948 agents.py:69] training 127 of 2000: completed tf_agent.train(...) = 13.660 [loss]\n",
      "I0624 23:36:00.295513 22948 agents.py:69] training 127 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:00.297501 22948 agents.py:69] training 128 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:00.531453 22948 agents.py:69] training 128 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:00.536556 22948 agents.py:69] training 128 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:02.582489 22948 agents.py:69] training 128 of 2000: completed tf_agent.train(...) = 20.804 [loss]\n",
      "I0624 23:36:02.584483 22948 agents.py:69] training 128 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:02.587483 22948 agents.py:69] training 129 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:02.817245 22948 agents.py:69] training 129 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:02.821245 22948 agents.py:69] training 129 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:04.840052 22948 agents.py:69] training 129 of 2000: completed tf_agent.train(...) = 8.414 [loss]\n",
      "I0624 23:36:04.843050 22948 agents.py:69] training 129 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:04.846667 22948 agents.py:69] training 130 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:05.113928 22948 agents.py:69] training 130 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:05.120098 22948 agents.py:69] training 130 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:05.275687 22948 agents.py:69] training 130 of 2000: completed tf_agent.train(...) = 10.609 [loss]\n",
      "I0624 23:36:05.277696 22948 agents.py:69] training 130 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:05.279714 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:36:18.896646 22948 agents.py:69] completed compute_avg_return(...) = -2.187\n",
      "I0624 23:36:18.896680 22948 agents.py:69] training 131 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:19.151513 22948 agents.py:69] training 131 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:19.156312 22948 agents.py:69] training 131 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:21.232565 22948 agents.py:69] training 131 of 2000: completed tf_agent.train(...) = 27.591 [loss]\n",
      "I0624 23:36:21.235702 22948 agents.py:69] training 131 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:21.237099 22948 agents.py:69] training 132 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:21.571708 22948 agents.py:69] training 132 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:21.576575 22948 agents.py:69] training 132 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:21.770368 22948 agents.py:69] training 132 of 2000: completed tf_agent.train(...) = 129.179 [loss]\n",
      "I0624 23:36:21.772369 22948 agents.py:69] training 132 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:21.773459 22948 agents.py:69] training 133 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:21.983524 22948 agents.py:69] training 133 of 2000: executing replay_buffer.gather_all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:36:21.987508 22948 agents.py:69] training 133 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:22.136469 22948 agents.py:69] training 133 of 2000: completed tf_agent.train(...) = 7.264 [loss]\n",
      "I0624 23:36:22.137468 22948 agents.py:69] training 133 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:22.139571 22948 agents.py:69] training 134 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:22.352569 22948 agents.py:69] training 134 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:22.357518 22948 agents.py:69] training 134 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:24.418032 22948 agents.py:69] training 134 of 2000: completed tf_agent.train(...) = 8.234 [loss]\n",
      "I0624 23:36:24.420037 22948 agents.py:69] training 134 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:24.422495 22948 agents.py:69] training 135 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:24.625412 22948 agents.py:69] training 135 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:24.627378 22948 agents.py:69] training 135 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:24.793671 22948 agents.py:69] training 135 of 2000: completed tf_agent.train(...) = 8.205 [loss]\n",
      "I0624 23:36:24.795471 22948 agents.py:69] training 135 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:24.796483 22948 agents.py:69] training 136 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:25.092647 22948 agents.py:69] training 136 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:25.097206 22948 agents.py:69] training 136 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:27.053476 22948 agents.py:69] training 136 of 2000: completed tf_agent.train(...) = 14.048 [loss]\n",
      "I0624 23:36:27.055428 22948 agents.py:69] training 136 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:27.056430 22948 agents.py:69] training 137 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:27.342000 22948 agents.py:69] training 137 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:27.345857 22948 agents.py:69] training 137 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:29.473042 22948 agents.py:69] training 137 of 2000: completed tf_agent.train(...) = 28.822 [loss]\n",
      "I0624 23:36:29.476122 22948 agents.py:69] training 137 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:29.477458 22948 agents.py:69] training 138 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:29.750394 22948 agents.py:69] training 138 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:29.755560 22948 agents.py:69] training 138 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:31.790966 22948 agents.py:69] training 138 of 2000: completed tf_agent.train(...) = 6.795 [loss]\n",
      "I0624 23:36:31.792035 22948 agents.py:69] training 138 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:31.794122 22948 agents.py:69] training 139 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:31.989605 22948 agents.py:69] training 139 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:31.993605 22948 agents.py:69] training 139 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:34.013638 22948 agents.py:69] training 139 of 2000: completed tf_agent.train(...) = 9.185 [loss]\n",
      "I0624 23:36:34.015360 22948 agents.py:69] training 139 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:34.016344 22948 agents.py:69] training 140 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:34.238222 22948 agents.py:69] training 140 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:34.242513 22948 agents.py:69] training 140 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:36.319466 22948 agents.py:69] training 140 of 2000: completed tf_agent.train(...) = 12.712 [loss]\n",
      "I0624 23:36:36.321546 22948 agents.py:69] training 140 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:36.323458 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:36:42.417439 22948 agents.py:69] completed compute_avg_return(...) = -0.687\n",
      "I0624 23:36:42.418439 22948 agents.py:69] training 141 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:42.583146 22948 agents.py:69] training 141 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:42.586187 22948 agents.py:69] training 141 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:44.120740 22948 agents.py:69] training 141 of 2000: completed tf_agent.train(...) = 8.349 [loss]\n",
      "I0624 23:36:44.122740 22948 agents.py:69] training 141 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:44.123770 22948 agents.py:69] training 142 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:44.315198 22948 agents.py:69] training 142 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:44.318590 22948 agents.py:69] training 142 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:44.464580 22948 agents.py:69] training 142 of 2000: completed tf_agent.train(...) = 12.153 [loss]\n",
      "I0624 23:36:44.465624 22948 agents.py:69] training 142 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:44.467635 22948 agents.py:69] training 143 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:44.709660 22948 agents.py:69] training 143 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:44.712627 22948 agents.py:69] training 143 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:46.632218 22948 agents.py:69] training 143 of 2000: completed tf_agent.train(...) = 30.562 [loss]\n",
      "I0624 23:36:46.633219 22948 agents.py:69] training 143 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:46.635168 22948 agents.py:69] training 144 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:46.845635 22948 agents.py:69] training 144 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:46.848638 22948 agents.py:69] training 144 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:48.900274 22948 agents.py:69] training 144 of 2000: completed tf_agent.train(...) = 5.608 [loss]\n",
      "I0624 23:36:48.902271 22948 agents.py:69] training 144 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:48.904397 22948 agents.py:69] training 145 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:49.116148 22948 agents.py:69] training 145 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:49.120263 22948 agents.py:69] training 145 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:49.257455 22948 agents.py:69] training 145 of 2000: completed tf_agent.train(...) = 4.904 [loss]\n",
      "I0624 23:36:49.259540 22948 agents.py:69] training 145 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:49.260487 22948 agents.py:69] training 146 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:49.489255 22948 agents.py:69] training 146 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:49.494221 22948 agents.py:69] training 146 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:51.518867 22948 agents.py:69] training 146 of 2000: completed tf_agent.train(...) = 3.696 [loss]\n",
      "I0624 23:36:51.520867 22948 agents.py:69] training 146 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:51.523861 22948 agents.py:69] training 147 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:51.725175 22948 agents.py:69] training 147 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:51.728159 22948 agents.py:69] training 147 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:53.640472 22948 agents.py:69] training 147 of 2000: completed tf_agent.train(...) = 10.402 [loss]\n",
      "I0624 23:36:53.641586 22948 agents.py:69] training 147 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:53.642671 22948 agents.py:69] training 148 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:53.878313 22948 agents.py:69] training 148 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:53.883312 22948 agents.py:69] training 148 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:54.053851 22948 agents.py:69] training 148 of 2000: completed tf_agent.train(...) = 16.934 [loss]\n",
      "I0624 23:36:54.055844 22948 agents.py:69] training 148 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:54.057841 22948 agents.py:69] training 149 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:54.260694 22948 agents.py:69] training 149 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:54.263600 22948 agents.py:69] training 149 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:56.267404 22948 agents.py:69] training 149 of 2000: completed tf_agent.train(...) = 4.243 [loss]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:36:56.269406 22948 agents.py:69] training 149 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:56.271404 22948 agents.py:69] training 150 of 2000: executing collect_driver.run()\n",
      "I0624 23:36:56.443517 22948 agents.py:69] training 150 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:36:56.448183 22948 agents.py:69] training 150 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:36:58.473295 22948 agents.py:69] training 150 of 2000: completed tf_agent.train(...) = 5.017 [loss]\n",
      "I0624 23:36:58.475293 22948 agents.py:69] training 150 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:36:58.477306 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:37:06.566657 22948 agents.py:69] completed compute_avg_return(...) = -1.092\n",
      "I0624 23:37:06.567499 22948 agents.py:69] training 151 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:06.767717 22948 agents.py:69] training 151 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:06.771718 22948 agents.py:69] training 151 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:08.938895 22948 agents.py:69] training 151 of 2000: completed tf_agent.train(...) = 2.652 [loss]\n",
      "I0624 23:37:08.940896 22948 agents.py:69] training 151 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:08.944231 22948 agents.py:69] training 152 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:09.126183 22948 agents.py:69] training 152 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:09.129894 22948 agents.py:69] training 152 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:11.197376 22948 agents.py:69] training 152 of 2000: completed tf_agent.train(...) = 3.161 [loss]\n",
      "I0624 23:37:11.199382 22948 agents.py:69] training 152 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:11.201833 22948 agents.py:69] training 153 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:11.394601 22948 agents.py:69] training 153 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:11.398603 22948 agents.py:69] training 153 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:13.367486 22948 agents.py:69] training 153 of 2000: completed tf_agent.train(...) = 2.309 [loss]\n",
      "I0624 23:37:13.369476 22948 agents.py:69] training 153 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:13.371478 22948 agents.py:69] training 154 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:13.628797 22948 agents.py:69] training 154 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:13.631680 22948 agents.py:69] training 154 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:15.684266 22948 agents.py:69] training 154 of 2000: completed tf_agent.train(...) = 5.948 [loss]\n",
      "I0624 23:37:15.687344 22948 agents.py:69] training 154 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:15.688343 22948 agents.py:69] training 155 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:15.907078 22948 agents.py:69] training 155 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:15.911055 22948 agents.py:69] training 155 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:16.072337 22948 agents.py:69] training 155 of 2000: completed tf_agent.train(...) = 2.729 [loss]\n",
      "I0624 23:37:16.072829 22948 agents.py:69] training 155 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:16.074338 22948 agents.py:69] training 156 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:16.343299 22948 agents.py:69] training 156 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:16.346516 22948 agents.py:69] training 156 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:16.534394 22948 agents.py:69] training 156 of 2000: completed tf_agent.train(...) = 9.491 [loss]\n",
      "I0624 23:37:16.535643 22948 agents.py:69] training 156 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:16.537653 22948 agents.py:69] training 157 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:16.755378 22948 agents.py:69] training 157 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:16.757375 22948 agents.py:69] training 157 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:18.737292 22948 agents.py:69] training 157 of 2000: completed tf_agent.train(...) = 4.190 [loss]\n",
      "I0624 23:37:18.738823 22948 agents.py:69] training 157 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:18.739510 22948 agents.py:69] training 158 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:18.963350 22948 agents.py:69] training 158 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:18.968392 22948 agents.py:69] training 158 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:19.133896 22948 agents.py:69] training 158 of 2000: completed tf_agent.train(...) = 2.773 [loss]\n",
      "I0624 23:37:19.136025 22948 agents.py:69] training 158 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:19.138367 22948 agents.py:69] training 159 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:19.395007 22948 agents.py:69] training 159 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:19.399001 22948 agents.py:69] training 159 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:21.536793 22948 agents.py:69] training 159 of 2000: completed tf_agent.train(...) = 12.897 [loss]\n",
      "I0624 23:37:21.538791 22948 agents.py:69] training 159 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:21.541080 22948 agents.py:69] training 160 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:21.797472 22948 agents.py:69] training 160 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:21.802476 22948 agents.py:69] training 160 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:21.987550 22948 agents.py:69] training 160 of 2000: completed tf_agent.train(...) = 6.592 [loss]\n",
      "I0624 23:37:21.988549 22948 agents.py:69] training 160 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:21.990777 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:37:28.096277 22948 agents.py:69] completed compute_avg_return(...) = -0.359\n",
      "I0624 23:37:28.097312 22948 agents.py:69] training 161 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:28.276333 22948 agents.py:69] training 161 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:28.279294 22948 agents.py:69] training 161 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:30.247231 22948 agents.py:69] training 161 of 2000: completed tf_agent.train(...) = 4.149 [loss]\n",
      "I0624 23:37:30.248292 22948 agents.py:69] training 161 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:30.250362 22948 agents.py:69] training 162 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:30.464926 22948 agents.py:69] training 162 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:30.468610 22948 agents.py:69] training 162 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:32.408577 22948 agents.py:69] training 162 of 2000: completed tf_agent.train(...) = 4.565 [loss]\n",
      "I0624 23:37:32.410475 22948 agents.py:69] training 162 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:32.412412 22948 agents.py:69] training 163 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:32.633418 22948 agents.py:69] training 163 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:32.638372 22948 agents.py:69] training 163 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:34.594283 22948 agents.py:69] training 163 of 2000: completed tf_agent.train(...) = 3.621 [loss]\n",
      "I0624 23:37:34.595350 22948 agents.py:69] training 163 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:34.597362 22948 agents.py:69] training 164 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:34.843481 22948 agents.py:69] training 164 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:34.847429 22948 agents.py:69] training 164 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:36.832459 22948 agents.py:69] training 164 of 2000: completed tf_agent.train(...) = 9.821 [loss]\n",
      "I0624 23:37:36.834460 22948 agents.py:69] training 164 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:36.836407 22948 agents.py:69] training 165 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:37.081085 22948 agents.py:69] training 165 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:37.086073 22948 agents.py:69] training 165 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:39.233215 22948 agents.py:69] training 165 of 2000: completed tf_agent.train(...) = 14.726 [loss]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:37:39.235215 22948 agents.py:69] training 165 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:39.237210 22948 agents.py:69] training 166 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:39.482854 22948 agents.py:69] training 166 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:39.487071 22948 agents.py:69] training 166 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:41.643348 22948 agents.py:69] training 166 of 2000: completed tf_agent.train(...) = 4.806 [loss]\n",
      "I0624 23:37:41.645349 22948 agents.py:69] training 166 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:41.647343 22948 agents.py:69] training 167 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:41.846350 22948 agents.py:69] training 167 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:41.851339 22948 agents.py:69] training 167 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:43.924180 22948 agents.py:69] training 167 of 2000: completed tf_agent.train(...) = 3.594 [loss]\n",
      "I0624 23:37:43.927007 22948 agents.py:69] training 167 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:43.928364 22948 agents.py:69] training 168 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:44.154589 22948 agents.py:69] training 168 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:44.157116 22948 agents.py:69] training 168 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:45.875584 22948 agents.py:69] training 168 of 2000: completed tf_agent.train(...) = 7.980 [loss]\n",
      "I0624 23:37:45.877569 22948 agents.py:69] training 168 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:45.878614 22948 agents.py:69] training 169 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:46.044677 22948 agents.py:69] training 169 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:46.048882 22948 agents.py:69] training 169 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:46.185498 22948 agents.py:69] training 169 of 2000: completed tf_agent.train(...) = 4.744 [loss]\n",
      "I0624 23:37:46.187458 22948 agents.py:69] training 169 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:46.188700 22948 agents.py:69] training 170 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:46.342524 22948 agents.py:69] training 170 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:46.345505 22948 agents.py:69] training 170 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:47.972825 22948 agents.py:69] training 170 of 2000: completed tf_agent.train(...) = 3.721 [loss]\n",
      "I0624 23:37:47.974837 22948 agents.py:69] training 170 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:47.976717 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:37:52.929142 22948 agents.py:69] completed compute_avg_return(...) = -0.437\n",
      "I0624 23:37:52.930148 22948 agents.py:69] training 171 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:53.165458 22948 agents.py:69] training 171 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:53.168465 22948 agents.py:69] training 171 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:53.349466 22948 agents.py:69] training 171 of 2000: completed tf_agent.train(...) = 3.693 [loss]\n",
      "I0624 23:37:53.351467 22948 agents.py:69] training 171 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:53.354466 22948 agents.py:69] training 172 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:53.570764 22948 agents.py:69] training 172 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:53.574761 22948 agents.py:69] training 172 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:53.741458 22948 agents.py:69] training 172 of 2000: completed tf_agent.train(...) = 21.736 [loss]\n",
      "I0624 23:37:53.742455 22948 agents.py:69] training 172 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:53.744458 22948 agents.py:69] training 173 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:53.998450 22948 agents.py:69] training 173 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:54.003453 22948 agents.py:69] training 173 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:55.979484 22948 agents.py:69] training 173 of 2000: completed tf_agent.train(...) = 12.833 [loss]\n",
      "I0624 23:37:55.980697 22948 agents.py:69] training 173 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:55.982484 22948 agents.py:69] training 174 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:56.253444 22948 agents.py:69] training 174 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:56.256448 22948 agents.py:69] training 174 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:37:58.245211 22948 agents.py:69] training 174 of 2000: completed tf_agent.train(...) = 16.605 [loss]\n",
      "I0624 23:37:58.248169 22948 agents.py:69] training 174 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:37:58.249247 22948 agents.py:69] training 175 of 2000: executing collect_driver.run()\n",
      "I0624 23:37:58.488248 22948 agents.py:69] training 175 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:37:58.493211 22948 agents.py:69] training 175 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:00.558073 22948 agents.py:69] training 175 of 2000: completed tf_agent.train(...) = 7.179 [loss]\n",
      "I0624 23:38:00.559072 22948 agents.py:69] training 175 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:00.561075 22948 agents.py:69] training 176 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:00.733875 22948 agents.py:69] training 176 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:00.738829 22948 agents.py:69] training 176 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:02.670651 22948 agents.py:69] training 176 of 2000: completed tf_agent.train(...) = 4.587 [loss]\n",
      "I0624 23:38:02.672655 22948 agents.py:69] training 176 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:02.674654 22948 agents.py:69] training 177 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:02.863732 22948 agents.py:69] training 177 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:02.866749 22948 agents.py:69] training 177 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:03.024618 22948 agents.py:69] training 177 of 2000: completed tf_agent.train(...) = 3.676 [loss]\n",
      "I0624 23:38:03.025202 22948 agents.py:69] training 177 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:03.027179 22948 agents.py:69] training 178 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:03.222658 22948 agents.py:69] training 178 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:03.225426 22948 agents.py:69] training 178 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:05.221305 22948 agents.py:69] training 178 of 2000: completed tf_agent.train(...) = 4.939 [loss]\n",
      "I0624 23:38:05.222304 22948 agents.py:69] training 178 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:05.225381 22948 agents.py:69] training 179 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:05.667366 22948 agents.py:69] training 179 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:05.673457 22948 agents.py:69] training 179 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:07.779144 22948 agents.py:69] training 179 of 2000: completed tf_agent.train(...) = 235.657 [loss]\n",
      "I0624 23:38:07.782142 22948 agents.py:69] training 179 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:07.784137 22948 agents.py:69] training 180 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:07.947566 22948 agents.py:69] training 180 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:07.952779 22948 agents.py:69] training 180 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:08.132052 22948 agents.py:69] training 180 of 2000: completed tf_agent.train(...) = 8.411 [loss]\n",
      "I0624 23:38:08.134050 22948 agents.py:69] training 180 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:08.136052 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:38:18.043236 22948 agents.py:69] completed compute_avg_return(...) = -2.328\n",
      "I0624 23:38:18.044369 22948 agents.py:69] training 181 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:18.276369 22948 agents.py:69] training 181 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:18.282298 22948 agents.py:69] training 181 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:18.467151 22948 agents.py:69] training 181 of 2000: completed tf_agent.train(...) = 65.888 [loss]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:38:18.469149 22948 agents.py:69] training 181 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:18.471147 22948 agents.py:69] training 182 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:18.813162 22948 agents.py:69] training 182 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:18.816201 22948 agents.py:69] training 182 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:20.985145 22948 agents.py:69] training 182 of 2000: completed tf_agent.train(...) = 66.678 [loss]\n",
      "I0624 23:38:20.987141 22948 agents.py:69] training 182 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:20.989144 22948 agents.py:69] training 183 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:21.467165 22948 agents.py:69] training 183 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:21.471193 22948 agents.py:69] training 183 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:21.754357 22948 agents.py:69] training 183 of 2000: completed tf_agent.train(...) = 652.001 [loss]\n",
      "I0624 23:38:21.755579 22948 agents.py:69] training 183 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:21.757590 22948 agents.py:69] training 184 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:22.143087 22948 agents.py:69] training 184 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:22.148076 22948 agents.py:69] training 184 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:24.321401 22948 agents.py:69] training 184 of 2000: completed tf_agent.train(...) = 149.027 [loss]\n",
      "I0624 23:38:24.323397 22948 agents.py:69] training 184 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:24.325531 22948 agents.py:69] training 185 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:24.697194 22948 agents.py:69] training 185 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:24.700243 22948 agents.py:69] training 185 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:26.901667 22948 agents.py:69] training 185 of 2000: completed tf_agent.train(...) = 284.948 [loss]\n",
      "I0624 23:38:26.904670 22948 agents.py:69] training 185 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:26.905737 22948 agents.py:69] training 186 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:27.119773 22948 agents.py:69] training 186 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:27.122719 22948 agents.py:69] training 186 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:27.320658 22948 agents.py:69] training 186 of 2000: completed tf_agent.train(...) = 39.950 [loss]\n",
      "I0624 23:38:27.322654 22948 agents.py:69] training 186 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:27.325326 22948 agents.py:69] training 187 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:27.590303 22948 agents.py:69] training 187 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:27.594457 22948 agents.py:69] training 187 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:27.798450 22948 agents.py:69] training 187 of 2000: completed tf_agent.train(...) = 46.298 [loss]\n",
      "I0624 23:38:27.799695 22948 agents.py:69] training 187 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:27.802029 22948 agents.py:69] training 188 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:28.074419 22948 agents.py:69] training 188 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:28.078382 22948 agents.py:69] training 188 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:28.287131 22948 agents.py:69] training 188 of 2000: completed tf_agent.train(...) = 24.730 [loss]\n",
      "I0624 23:38:28.288842 22948 agents.py:69] training 188 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:28.290130 22948 agents.py:69] training 189 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:28.467440 22948 agents.py:69] training 189 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:28.471442 22948 agents.py:69] training 189 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:30.615065 22948 agents.py:69] training 189 of 2000: completed tf_agent.train(...) = 23.958 [loss]\n",
      "I0624 23:38:30.615992 22948 agents.py:69] training 189 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:30.618909 22948 agents.py:69] training 190 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:30.787392 22948 agents.py:69] training 190 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:30.791781 22948 agents.py:69] training 190 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:30.957282 22948 agents.py:69] training 190 of 2000: completed tf_agent.train(...) = 18.317 [loss]\n",
      "I0624 23:38:30.958199 22948 agents.py:69] training 190 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:30.960203 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:38:40.303204 22948 agents.py:69] completed compute_avg_return(...) = -1.078\n",
      "I0624 23:38:40.304204 22948 agents.py:69] training 191 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:40.478237 22948 agents.py:69] training 191 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:40.482199 22948 agents.py:69] training 191 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:42.535513 22948 agents.py:69] training 191 of 2000: completed tf_agent.train(...) = 13.584 [loss]\n",
      "I0624 23:38:42.537613 22948 agents.py:69] training 191 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:42.539498 22948 agents.py:69] training 192 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:42.724865 22948 agents.py:69] training 192 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:42.729079 22948 agents.py:69] training 192 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:44.923806 22948 agents.py:69] training 192 of 2000: completed tf_agent.train(...) = 13.961 [loss]\n",
      "I0624 23:38:44.925811 22948 agents.py:69] training 192 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:44.927698 22948 agents.py:69] training 193 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:45.131308 22948 agents.py:69] training 193 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:45.136348 22948 agents.py:69] training 193 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:45.327625 22948 agents.py:69] training 193 of 2000: completed tf_agent.train(...) = 9.429 [loss]\n",
      "I0624 23:38:45.329621 22948 agents.py:69] training 193 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:45.330620 22948 agents.py:69] training 194 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:45.524146 22948 agents.py:69] training 194 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:45.528169 22948 agents.py:69] training 194 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:45.699618 22948 agents.py:69] training 194 of 2000: completed tf_agent.train(...) = 8.289 [loss]\n",
      "I0624 23:38:45.700747 22948 agents.py:69] training 194 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:45.701618 22948 agents.py:69] training 195 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:45.903450 22948 agents.py:69] training 195 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:45.907641 22948 agents.py:69] training 195 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:46.092466 22948 agents.py:69] training 195 of 2000: completed tf_agent.train(...) = 6.164 [loss]\n",
      "I0624 23:38:46.094469 22948 agents.py:69] training 195 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:46.096471 22948 agents.py:69] training 196 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:46.318099 22948 agents.py:69] training 196 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:46.323115 22948 agents.py:69] training 196 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:48.372487 22948 agents.py:69] training 196 of 2000: completed tf_agent.train(...) = 17.294 [loss]\n",
      "I0624 23:38:48.373342 22948 agents.py:69] training 196 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:48.375630 22948 agents.py:69] training 197 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:48.588282 22948 agents.py:69] training 197 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:48.592282 22948 agents.py:69] training 197 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:50.643112 22948 agents.py:69] training 197 of 2000: completed tf_agent.train(...) = 8.297 [loss]\n",
      "I0624 23:38:50.644859 22948 agents.py:69] training 197 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:50.646381 22948 agents.py:69] training 198 of 2000: executing collect_driver.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:38:50.842372 22948 agents.py:69] training 198 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:50.847443 22948 agents.py:69] training 198 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:52.926012 22948 agents.py:69] training 198 of 2000: completed tf_agent.train(...) = 11.197 [loss]\n",
      "I0624 23:38:52.928022 22948 agents.py:69] training 198 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:52.929022 22948 agents.py:69] training 199 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:53.188045 22948 agents.py:69] training 199 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:53.191040 22948 agents.py:69] training 199 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:55.180074 22948 agents.py:69] training 199 of 2000: completed tf_agent.train(...) = 14.742 [loss]\n",
      "I0624 23:38:55.181073 22948 agents.py:69] training 199 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:55.184077 22948 agents.py:69] training 200 of 2000: executing collect_driver.run()\n",
      "I0624 23:38:55.346168 22948 agents.py:69] training 200 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:38:55.349169 22948 agents.py:69] training 200 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:38:57.509272 22948 agents.py:69] training 200 of 2000: completed tf_agent.train(...) = 10.285 [loss]\n",
      "I0624 23:38:57.512339 22948 agents.py:69] training 200 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:38:57.513269 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:39:04.279711 22948 agents.py:69] completed compute_avg_return(...) = -0.520\n",
      "I0624 23:39:04.280467 22948 agents.py:69] training 201 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:04.473397 22948 agents.py:69] training 201 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:04.478426 22948 agents.py:69] training 201 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:04.657610 22948 agents.py:69] training 201 of 2000: completed tf_agent.train(...) = 4.346 [loss]\n",
      "I0624 23:39:04.658768 22948 agents.py:69] training 201 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:04.661963 22948 agents.py:69] training 202 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:04.853375 22948 agents.py:69] training 202 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:04.858326 22948 agents.py:69] training 202 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:05.035584 22948 agents.py:69] training 202 of 2000: completed tf_agent.train(...) = 3.979 [loss]\n",
      "I0624 23:39:05.036714 22948 agents.py:69] training 202 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:05.038584 22948 agents.py:69] training 203 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:05.207256 22948 agents.py:69] training 203 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:05.212308 22948 agents.py:69] training 203 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:05.388896 22948 agents.py:69] training 203 of 2000: completed tf_agent.train(...) = 6.065 [loss]\n",
      "I0624 23:39:05.390894 22948 agents.py:69] training 203 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:05.392898 22948 agents.py:69] training 204 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:05.610272 22948 agents.py:69] training 204 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:05.614233 22948 agents.py:69] training 204 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:05.822100 22948 agents.py:69] training 204 of 2000: completed tf_agent.train(...) = 18.510 [loss]\n",
      "I0624 23:39:05.823360 22948 agents.py:69] training 204 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:05.825224 22948 agents.py:69] training 205 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:06.055385 22948 agents.py:69] training 205 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:06.059391 22948 agents.py:69] training 205 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:06.270364 22948 agents.py:69] training 205 of 2000: completed tf_agent.train(...) = 10.077 [loss]\n",
      "I0624 23:39:06.271485 22948 agents.py:69] training 205 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:06.273609 22948 agents.py:69] training 206 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:06.448482 22948 agents.py:69] training 206 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:06.453483 22948 agents.py:69] training 206 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:06.655725 22948 agents.py:69] training 206 of 2000: completed tf_agent.train(...) = 6.546 [loss]\n",
      "I0624 23:39:06.657787 22948 agents.py:69] training 206 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:06.659723 22948 agents.py:69] training 207 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:06.875463 22948 agents.py:69] training 207 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:06.878509 22948 agents.py:69] training 207 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:07.047481 22948 agents.py:69] training 207 of 2000: completed tf_agent.train(...) = 6.057 [loss]\n",
      "I0624 23:39:07.049481 22948 agents.py:69] training 207 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:07.050481 22948 agents.py:69] training 208 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:07.205741 22948 agents.py:69] training 208 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:07.208812 22948 agents.py:69] training 208 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:09.259423 22948 agents.py:69] training 208 of 2000: completed tf_agent.train(...) = 4.390 [loss]\n",
      "I0624 23:39:09.262489 22948 agents.py:69] training 208 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:09.263641 22948 agents.py:69] training 209 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:09.443731 22948 agents.py:69] training 209 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:09.447260 22948 agents.py:69] training 209 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:09.637687 22948 agents.py:69] training 209 of 2000: completed tf_agent.train(...) = 5.819 [loss]\n",
      "I0624 23:39:09.639688 22948 agents.py:69] training 209 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:09.642701 22948 agents.py:69] training 210 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:09.840570 22948 agents.py:69] training 210 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:09.844571 22948 agents.py:69] training 210 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:10.036657 22948 agents.py:69] training 210 of 2000: completed tf_agent.train(...) = 4.484 [loss]\n",
      "I0624 23:39:10.037537 22948 agents.py:69] training 210 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:10.040537 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:39:21.200522 22948 agents.py:69] completed compute_avg_return(...) = -1.365\n",
      "I0624 23:39:21.202527 22948 agents.py:69] training 211 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:21.404664 22948 agents.py:69] training 211 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:21.408481 22948 agents.py:69] training 211 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:21.587221 22948 agents.py:69] training 211 of 2000: completed tf_agent.train(...) = 4.558 [loss]\n",
      "I0624 23:39:21.589218 22948 agents.py:69] training 211 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:21.590257 22948 agents.py:69] training 212 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:21.802661 22948 agents.py:69] training 212 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:21.804696 22948 agents.py:69] training 212 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:22.025585 22948 agents.py:69] training 212 of 2000: completed tf_agent.train(...) = 6.741 [loss]\n",
      "I0624 23:39:22.026804 22948 agents.py:69] training 212 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:22.029002 22948 agents.py:69] training 213 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:22.246548 22948 agents.py:69] training 213 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:22.251550 22948 agents.py:69] training 213 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:24.310730 22948 agents.py:69] training 213 of 2000: completed tf_agent.train(...) = 8.493 [loss]\n",
      "I0624 23:39:24.312621 22948 agents.py:69] training 213 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:24.314626 22948 agents.py:69] training 214 of 2000: executing collect_driver.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:39:24.490262 22948 agents.py:69] training 214 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:24.495155 22948 agents.py:69] training 214 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:26.519559 22948 agents.py:69] training 214 of 2000: completed tf_agent.train(...) = 4.836 [loss]\n",
      "I0624 23:39:26.521787 22948 agents.py:69] training 214 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:26.523558 22948 agents.py:69] training 215 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:26.708050 22948 agents.py:69] training 215 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:26.711050 22948 agents.py:69] training 215 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:28.763599 22948 agents.py:69] training 215 of 2000: completed tf_agent.train(...) = 5.975 [loss]\n",
      "I0624 23:39:28.765599 22948 agents.py:69] training 215 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:28.767920 22948 agents.py:69] training 216 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:28.977447 22948 agents.py:69] training 216 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:28.981508 22948 agents.py:69] training 216 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:29.173399 22948 agents.py:69] training 216 of 2000: completed tf_agent.train(...) = 8.541 [loss]\n",
      "I0624 23:39:29.174315 22948 agents.py:69] training 216 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:29.176586 22948 agents.py:69] training 217 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:29.388660 22948 agents.py:69] training 217 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:29.391989 22948 agents.py:69] training 217 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:29.585719 22948 agents.py:69] training 217 of 2000: completed tf_agent.train(...) = 12.688 [loss]\n",
      "I0624 23:39:29.587564 22948 agents.py:69] training 217 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:29.588643 22948 agents.py:69] training 218 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:29.771902 22948 agents.py:69] training 218 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:29.774772 22948 agents.py:69] training 218 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:29.957798 22948 agents.py:69] training 218 of 2000: completed tf_agent.train(...) = 6.512 [loss]\n",
      "I0624 23:39:29.959802 22948 agents.py:69] training 218 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:29.960800 22948 agents.py:69] training 219 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:30.165917 22948 agents.py:69] training 219 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:30.168771 22948 agents.py:69] training 219 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:30.362541 22948 agents.py:69] training 219 of 2000: completed tf_agent.train(...) = 4.802 [loss]\n",
      "I0624 23:39:30.364540 22948 agents.py:69] training 219 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:30.365540 22948 agents.py:69] training 220 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:30.563579 22948 agents.py:69] training 220 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:30.566543 22948 agents.py:69] training 220 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:30.764791 22948 agents.py:69] training 220 of 2000: completed tf_agent.train(...) = 3.866 [loss]\n",
      "I0624 23:39:30.765790 22948 agents.py:69] training 220 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:30.767845 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:39:40.526719 22948 agents.py:69] completed compute_avg_return(...) = -1.453\n",
      "I0624 23:39:40.527722 22948 agents.py:69] training 221 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:40.721628 22948 agents.py:69] training 221 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:40.724852 22948 agents.py:69] training 221 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:42.722053 22948 agents.py:69] training 221 of 2000: completed tf_agent.train(...) = 5.654 [loss]\n",
      "I0624 23:39:42.724183 22948 agents.py:69] training 221 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:42.725708 22948 agents.py:69] training 222 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:42.953692 22948 agents.py:69] training 222 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:42.957661 22948 agents.py:69] training 222 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:45.021805 22948 agents.py:69] training 222 of 2000: completed tf_agent.train(...) = 10.889 [loss]\n",
      "I0624 23:39:45.023804 22948 agents.py:69] training 222 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:45.027387 22948 agents.py:69] training 223 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:45.215498 22948 agents.py:69] training 223 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:45.218517 22948 agents.py:69] training 223 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:45.398005 22948 agents.py:69] training 223 of 2000: completed tf_agent.train(...) = 4.213 [loss]\n",
      "I0624 23:39:45.400006 22948 agents.py:69] training 223 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:45.402008 22948 agents.py:69] training 224 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:45.552380 22948 agents.py:69] training 224 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:45.558445 22948 agents.py:69] training 224 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:47.596135 22948 agents.py:69] training 224 of 2000: completed tf_agent.train(...) = 4.056 [loss]\n",
      "I0624 23:39:47.597044 22948 agents.py:69] training 224 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:47.599044 22948 agents.py:69] training 225 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:47.828541 22948 agents.py:69] training 225 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:47.833503 22948 agents.py:69] training 225 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:48.053488 22948 agents.py:69] training 225 of 2000: completed tf_agent.train(...) = 4.243 [loss]\n",
      "I0624 23:39:48.054481 22948 agents.py:69] training 225 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:48.056583 22948 agents.py:69] training 226 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:48.295399 22948 agents.py:69] training 226 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:48.298358 22948 agents.py:69] training 226 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:48.512735 22948 agents.py:69] training 226 of 2000: completed tf_agent.train(...) = 9.072 [loss]\n",
      "I0624 23:39:48.515732 22948 agents.py:69] training 226 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:48.517726 22948 agents.py:69] training 227 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:48.709584 22948 agents.py:69] training 227 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:48.713591 22948 agents.py:69] training 227 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:48.903343 22948 agents.py:69] training 227 of 2000: completed tf_agent.train(...) = 2.925 [loss]\n",
      "I0624 23:39:48.905359 22948 agents.py:69] training 227 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:48.907378 22948 agents.py:69] training 228 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:49.120415 22948 agents.py:69] training 228 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:49.123175 22948 agents.py:69] training 228 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:51.328975 22948 agents.py:69] training 228 of 2000: completed tf_agent.train(...) = 5.705 [loss]\n",
      "I0624 23:39:51.329976 22948 agents.py:69] training 228 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:51.330975 22948 agents.py:69] training 229 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:51.510627 22948 agents.py:69] training 229 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:51.514305 22948 agents.py:69] training 229 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:39:53.581128 22948 agents.py:69] training 229 of 2000: completed tf_agent.train(...) = 3.866 [loss]\n",
      "I0624 23:39:53.583121 22948 agents.py:69] training 229 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:53.585445 22948 agents.py:69] training 230 of 2000: executing collect_driver.run()\n",
      "I0624 23:39:53.762171 22948 agents.py:69] training 230 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:39:53.764178 22948 agents.py:69] training 230 of 2000: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:39:53.939437 22948 agents.py:69] training 230 of 2000: completed tf_agent.train(...) = 3.112 [loss]\n",
      "I0624 23:39:53.940436 22948 agents.py:69] training 230 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:39:53.942440 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:40:01.735562 22948 agents.py:69] completed compute_avg_return(...) = -0.780\n",
      "I0624 23:40:01.737562 22948 agents.py:69] training 231 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:01.921282 22948 agents.py:69] training 231 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:01.927279 22948 agents.py:69] training 231 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:02.136833 22948 agents.py:69] training 231 of 2000: completed tf_agent.train(...) = 4.776 [loss]\n",
      "I0624 23:40:02.138838 22948 agents.py:69] training 231 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:02.140837 22948 agents.py:69] training 232 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:02.338562 22948 agents.py:69] training 232 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:02.343513 22948 agents.py:69] training 232 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:02.556888 22948 agents.py:69] training 232 of 2000: completed tf_agent.train(...) = 8.812 [loss]\n",
      "I0624 23:40:02.558892 22948 agents.py:69] training 232 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:02.560893 22948 agents.py:69] training 233 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:02.793746 22948 agents.py:69] training 233 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:02.798737 22948 agents.py:69] training 233 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:03.018496 22948 agents.py:69] training 233 of 2000: completed tf_agent.train(...) = 4.078 [loss]\n",
      "I0624 23:40:03.021502 22948 agents.py:69] training 233 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:03.023494 22948 agents.py:69] training 234 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:03.195828 22948 agents.py:69] training 234 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:03.198846 22948 agents.py:69] training 234 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:05.363623 22948 agents.py:69] training 234 of 2000: completed tf_agent.train(...) = 2.804 [loss]\n",
      "I0624 23:40:05.365625 22948 agents.py:69] training 234 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:05.367619 22948 agents.py:69] training 235 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:05.634484 22948 agents.py:69] training 235 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:05.638610 22948 agents.py:69] training 235 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:05.872438 22948 agents.py:69] training 235 of 2000: completed tf_agent.train(...) = 20.562 [loss]\n",
      "I0624 23:40:05.873438 22948 agents.py:69] training 235 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:05.874475 22948 agents.py:69] training 236 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:05.985377 22948 agents.py:69] training 236 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:05.989388 22948 agents.py:69] training 236 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:08.054639 22948 agents.py:69] training 236 of 2000: completed tf_agent.train(...) = 2.965 [loss]\n",
      "I0624 23:40:08.055639 22948 agents.py:69] training 236 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:08.057645 22948 agents.py:69] training 237 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:08.267565 22948 agents.py:69] training 237 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:08.270569 22948 agents.py:69] training 237 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:10.410193 22948 agents.py:69] training 237 of 2000: completed tf_agent.train(...) = 4.028 [loss]\n",
      "I0624 23:40:10.411191 22948 agents.py:69] training 237 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:10.413198 22948 agents.py:69] training 238 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:10.617325 22948 agents.py:69] training 238 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:10.620303 22948 agents.py:69] training 238 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:12.689262 22948 agents.py:69] training 238 of 2000: completed tf_agent.train(...) = 5.658 [loss]\n",
      "I0624 23:40:12.690259 22948 agents.py:69] training 238 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:12.692423 22948 agents.py:69] training 239 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:12.852504 22948 agents.py:69] training 239 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:12.855502 22948 agents.py:69] training 239 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:14.910380 22948 agents.py:69] training 239 of 2000: completed tf_agent.train(...) = 3.435 [loss]\n",
      "I0624 23:40:14.912376 22948 agents.py:69] training 239 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:14.914377 22948 agents.py:69] training 240 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:15.081508 22948 agents.py:69] training 240 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:15.086924 22948 agents.py:69] training 240 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:15.286886 22948 agents.py:69] training 240 of 2000: completed tf_agent.train(...) = 6.642 [loss]\n",
      "I0624 23:40:15.287746 22948 agents.py:69] training 240 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:15.290748 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:40:21.704643 22948 agents.py:69] completed compute_avg_return(...) = -0.527\n",
      "I0624 23:40:21.705411 22948 agents.py:69] training 241 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:21.902971 22948 agents.py:69] training 241 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:21.906647 22948 agents.py:69] training 241 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:22.120434 22948 agents.py:69] training 241 of 2000: completed tf_agent.train(...) = 2.973 [loss]\n",
      "I0624 23:40:22.123435 22948 agents.py:69] training 241 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:22.125530 22948 agents.py:69] training 242 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:22.309517 22948 agents.py:69] training 242 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:22.313553 22948 agents.py:69] training 242 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:24.520709 22948 agents.py:69] training 242 of 2000: completed tf_agent.train(...) = 13.721 [loss]\n",
      "I0624 23:40:24.522706 22948 agents.py:69] training 242 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:24.525643 22948 agents.py:69] training 243 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:24.734054 22948 agents.py:69] training 243 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:24.737352 22948 agents.py:69] training 243 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:24.950321 22948 agents.py:69] training 243 of 2000: completed tf_agent.train(...) = 3.656 [loss]\n",
      "I0624 23:40:24.952321 22948 agents.py:69] training 243 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:24.955327 22948 agents.py:69] training 244 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:25.118651 22948 agents.py:69] training 244 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:25.121651 22948 agents.py:69] training 244 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:25.326625 22948 agents.py:69] training 244 of 2000: completed tf_agent.train(...) = 2.329 [loss]\n",
      "I0624 23:40:25.327622 22948 agents.py:69] training 244 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:25.330634 22948 agents.py:69] training 245 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:25.496632 22948 agents.py:69] training 245 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:25.499481 22948 agents.py:69] training 245 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:27.583338 22948 agents.py:69] training 245 of 2000: completed tf_agent.train(...) = 2.905 [loss]\n",
      "I0624 23:40:27.584666 22948 agents.py:69] training 245 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:27.586187 22948 agents.py:69] training 246 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:27.799504 22948 agents.py:69] training 246 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:27.804453 22948 agents.py:69] training 246 of 2000: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:40:28.043822 22948 agents.py:69] training 246 of 2000: completed tf_agent.train(...) = 11.429 [loss]\n",
      "I0624 23:40:28.044748 22948 agents.py:69] training 246 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:28.046863 22948 agents.py:69] training 247 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:28.316354 22948 agents.py:69] training 247 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:28.319365 22948 agents.py:69] training 247 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:30.638318 22948 agents.py:69] training 247 of 2000: completed tf_agent.train(...) = 10.736 [loss]\n",
      "I0624 23:40:30.640317 22948 agents.py:69] training 247 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:30.641320 22948 agents.py:69] training 248 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:30.890276 22948 agents.py:69] training 248 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:30.894276 22948 agents.py:69] training 248 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:31.134350 22948 agents.py:69] training 248 of 2000: completed tf_agent.train(...) = 4.673 [loss]\n",
      "I0624 23:40:31.136349 22948 agents.py:69] training 248 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:31.138209 22948 agents.py:69] training 249 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:31.368648 22948 agents.py:69] training 249 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:31.373811 22948 agents.py:69] training 249 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:31.602223 22948 agents.py:69] training 249 of 2000: completed tf_agent.train(...) = 9.713 [loss]\n",
      "I0624 23:40:31.604222 22948 agents.py:69] training 249 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:31.606583 22948 agents.py:69] training 250 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:31.764897 22948 agents.py:69] training 250 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:31.768943 22948 agents.py:69] training 250 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:34.046598 22948 agents.py:69] training 250 of 2000: completed tf_agent.train(...) = 2.562 [loss]\n",
      "I0624 23:40:34.047671 22948 agents.py:69] training 250 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:34.050091 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:40:39.294906 22948 agents.py:69] completed compute_avg_return(...) = -0.244\n",
      "I0624 23:40:39.296902 22948 agents.py:69] training 251 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:39.484542 22948 agents.py:69] training 251 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:39.487913 22948 agents.py:69] training 251 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:39.728712 22948 agents.py:69] training 251 of 2000: completed tf_agent.train(...) = 3.149 [loss]\n",
      "I0624 23:40:39.731711 22948 agents.py:69] training 251 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:39.733713 22948 agents.py:69] training 252 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:39.899381 22948 agents.py:69] training 252 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:39.903477 22948 agents.py:69] training 252 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:42.060433 22948 agents.py:69] training 252 of 2000: completed tf_agent.train(...) = 2.340 [loss]\n",
      "I0624 23:40:42.061234 22948 agents.py:69] training 252 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:42.063233 22948 agents.py:69] training 253 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:42.277779 22948 agents.py:69] training 253 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:42.281779 22948 agents.py:69] training 253 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:42.485707 22948 agents.py:69] training 253 of 2000: completed tf_agent.train(...) = 13.434 [loss]\n",
      "I0624 23:40:42.486698 22948 agents.py:69] training 253 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:42.487697 22948 agents.py:69] training 254 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:42.666586 22948 agents.py:69] training 254 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:42.671590 22948 agents.py:69] training 254 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:44.870844 22948 agents.py:69] training 254 of 2000: completed tf_agent.train(...) = 2.596 [loss]\n",
      "I0624 23:40:44.872773 22948 agents.py:69] training 254 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:44.875433 22948 agents.py:69] training 255 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:45.078510 22948 agents.py:69] training 255 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:45.083704 22948 agents.py:69] training 255 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:45.286933 22948 agents.py:69] training 255 of 2000: completed tf_agent.train(...) = 8.518 [loss]\n",
      "I0624 23:40:45.287789 22948 agents.py:69] training 255 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:45.290156 22948 agents.py:69] training 256 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:45.494622 22948 agents.py:69] training 256 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:45.499620 22948 agents.py:69] training 256 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:47.782219 22948 agents.py:69] training 256 of 2000: completed tf_agent.train(...) = 6.263 [loss]\n",
      "I0624 23:40:47.784213 22948 agents.py:69] training 256 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:47.787215 22948 agents.py:69] training 257 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:47.961249 22948 agents.py:69] training 257 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:47.965294 22948 agents.py:69] training 257 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:48.201506 22948 agents.py:69] training 257 of 2000: completed tf_agent.train(...) = 2.462 [loss]\n",
      "I0624 23:40:48.204506 22948 agents.py:69] training 257 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:48.206660 22948 agents.py:69] training 258 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:48.374428 22948 agents.py:69] training 258 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:48.377390 22948 agents.py:69] training 258 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:48.608699 22948 agents.py:69] training 258 of 2000: completed tf_agent.train(...) = 3.767 [loss]\n",
      "I0624 23:40:48.610697 22948 agents.py:69] training 258 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:48.613698 22948 agents.py:69] training 259 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:48.810321 22948 agents.py:69] training 259 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:48.813355 22948 agents.py:69] training 259 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:49.047077 22948 agents.py:69] training 259 of 2000: completed tf_agent.train(...) = 8.937 [loss]\n",
      "I0624 23:40:49.050078 22948 agents.py:69] training 259 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:49.052073 22948 agents.py:69] training 260 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:49.244671 22948 agents.py:69] training 260 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:49.247948 22948 agents.py:69] training 260 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:49.482181 22948 agents.py:69] training 260 of 2000: completed tf_agent.train(...) = 4.390 [loss]\n",
      "I0624 23:40:49.484182 22948 agents.py:69] training 260 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:49.486446 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:40:56.134352 22948 agents.py:69] completed compute_avg_return(...) = -0.712\n",
      "I0624 23:40:56.135299 22948 agents.py:69] training 261 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:56.331556 22948 agents.py:69] training 261 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:56.335400 22948 agents.py:69] training 261 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:56.563402 22948 agents.py:69] training 261 of 2000: completed tf_agent.train(...) = 2.094 [loss]\n",
      "I0624 23:40:56.566580 22948 agents.py:69] training 261 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:56.568532 22948 agents.py:69] training 262 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:56.784525 22948 agents.py:69] training 262 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:56.788515 22948 agents.py:69] training 262 of 2000: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:40:57.028361 22948 agents.py:69] training 262 of 2000: completed tf_agent.train(...) = 3.070 [loss]\n",
      "I0624 23:40:57.030427 22948 agents.py:69] training 262 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:57.032364 22948 agents.py:69] training 263 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:57.215652 22948 agents.py:69] training 263 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:57.218951 22948 agents.py:69] training 263 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:57.454804 22948 agents.py:69] training 263 of 2000: completed tf_agent.train(...) = 6.191 [loss]\n",
      "I0624 23:40:57.456345 22948 agents.py:69] training 263 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:57.458401 22948 agents.py:69] training 264 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:57.673714 22948 agents.py:69] training 264 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:57.677309 22948 agents.py:69] training 264 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:40:57.905487 22948 agents.py:69] training 264 of 2000: completed tf_agent.train(...) = 3.199 [loss]\n",
      "I0624 23:40:57.908121 22948 agents.py:69] training 264 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:40:57.909489 22948 agents.py:69] training 265 of 2000: executing collect_driver.run()\n",
      "I0624 23:40:58.062376 22948 agents.py:69] training 265 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:40:58.065376 22948 agents.py:69] training 265 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:00.369311 22948 agents.py:69] training 265 of 2000: completed tf_agent.train(...) = 2.264 [loss]\n",
      "I0624 23:41:00.371311 22948 agents.py:69] training 265 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:00.374311 22948 agents.py:69] training 266 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:00.523345 22948 agents.py:69] training 266 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:00.527306 22948 agents.py:69] training 266 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:02.860646 22948 agents.py:69] training 266 of 2000: completed tf_agent.train(...) = 1.479 [loss]\n",
      "I0624 23:41:02.862647 22948 agents.py:69] training 266 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:02.863645 22948 agents.py:69] training 267 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:03.082699 22948 agents.py:69] training 267 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:03.087478 22948 agents.py:69] training 267 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:05.369792 22948 agents.py:69] training 267 of 2000: completed tf_agent.train(...) = 4.210 [loss]\n",
      "I0624 23:41:05.371681 22948 agents.py:69] training 267 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:05.373779 22948 agents.py:69] training 268 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:05.558349 22948 agents.py:69] training 268 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:05.562654 22948 agents.py:69] training 268 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:07.759542 22948 agents.py:69] training 268 of 2000: completed tf_agent.train(...) = 1.872 [loss]\n",
      "I0624 23:41:07.761540 22948 agents.py:69] training 268 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:07.763537 22948 agents.py:69] training 269 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:07.961663 22948 agents.py:69] training 269 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:07.965666 22948 agents.py:69] training 269 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:08.206139 22948 agents.py:69] training 269 of 2000: completed tf_agent.train(...) = 4.024 [loss]\n",
      "I0624 23:41:08.208169 22948 agents.py:69] training 269 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:08.211160 22948 agents.py:69] training 270 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:08.398890 22948 agents.py:69] training 270 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:08.402884 22948 agents.py:69] training 270 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:08.648837 22948 agents.py:69] training 270 of 2000: completed tf_agent.train(...) = 3.527 [loss]\n",
      "I0624 23:41:08.651841 22948 agents.py:69] training 270 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:08.652833 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:41:13.818850 22948 agents.py:69] completed compute_avg_return(...) = -0.406\n",
      "I0624 23:41:13.820846 22948 agents.py:69] training 271 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:14.000455 22948 agents.py:69] training 271 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:14.003287 22948 agents.py:69] training 271 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:16.339672 22948 agents.py:69] training 271 of 2000: completed tf_agent.train(...) = 1.990 [loss]\n",
      "I0624 23:41:16.341672 22948 agents.py:69] training 271 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:16.343674 22948 agents.py:69] training 272 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:16.559428 22948 agents.py:69] training 272 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:16.563466 22948 agents.py:69] training 272 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:16.809448 22948 agents.py:69] training 272 of 2000: completed tf_agent.train(...) = 2.248 [loss]\n",
      "I0624 23:41:16.811450 22948 agents.py:69] training 272 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:16.813450 22948 agents.py:69] training 273 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:16.978438 22948 agents.py:69] training 273 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:16.983448 22948 agents.py:69] training 273 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:19.302189 22948 agents.py:69] training 273 of 2000: completed tf_agent.train(...) = 1.586 [loss]\n",
      "I0624 23:41:19.305196 22948 agents.py:69] training 273 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:19.307178 22948 agents.py:69] training 274 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:19.517408 22948 agents.py:69] training 274 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:19.522454 22948 agents.py:69] training 274 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:19.752097 22948 agents.py:69] training 274 of 2000: completed tf_agent.train(...) = 6.732 [loss]\n",
      "I0624 23:41:19.753099 22948 agents.py:69] training 274 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:19.755530 22948 agents.py:69] training 275 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:19.940193 22948 agents.py:69] training 275 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:19.943183 22948 agents.py:69] training 275 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:20.149801 22948 agents.py:69] training 275 of 2000: completed tf_agent.train(...) = 1.700 [loss]\n",
      "I0624 23:41:20.151797 22948 agents.py:69] training 275 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:20.153118 22948 agents.py:69] training 276 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:20.344452 22948 agents.py:69] training 276 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:20.348423 22948 agents.py:69] training 276 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:20.570467 22948 agents.py:69] training 276 of 2000: completed tf_agent.train(...) = 1.603 [loss]\n",
      "I0624 23:41:20.572220 22948 agents.py:69] training 276 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:20.573701 22948 agents.py:69] training 277 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:20.763542 22948 agents.py:69] training 277 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:20.767618 22948 agents.py:69] training 277 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:20.980631 22948 agents.py:69] training 277 of 2000: completed tf_agent.train(...) = 2.252 [loss]\n",
      "I0624 23:41:20.982238 22948 agents.py:69] training 277 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:20.984308 22948 agents.py:69] training 278 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:21.150978 22948 agents.py:69] training 278 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:21.154799 22948 agents.py:69] training 278 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:21.358380 22948 agents.py:69] training 278 of 2000: completed tf_agent.train(...) = 2.595 [loss]\n",
      "I0624 23:41:21.361391 22948 agents.py:69] training 278 of 2000: executing replay_buffer.clear()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:41:21.363386 22948 agents.py:69] training 279 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:21.622896 22948 agents.py:69] training 279 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:21.626883 22948 agents.py:69] training 279 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:21.886829 22948 agents.py:69] training 279 of 2000: completed tf_agent.train(...) = 22.349 [loss]\n",
      "I0624 23:41:21.888827 22948 agents.py:69] training 279 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:21.891828 22948 agents.py:69] training 280 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:22.087825 22948 agents.py:69] training 280 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:22.092823 22948 agents.py:69] training 280 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:22.334595 22948 agents.py:69] training 280 of 2000: completed tf_agent.train(...) = 2.828 [loss]\n",
      "I0624 23:41:22.335721 22948 agents.py:69] training 280 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:22.337477 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:41:28.677478 22948 agents.py:69] completed compute_avg_return(...) = -1.249\n",
      "I0624 23:41:28.678477 22948 agents.py:69] training 281 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:28.793135 22948 agents.py:69] training 281 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:28.795100 22948 agents.py:69] training 281 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:28.938378 22948 agents.py:69] training 281 of 2000: completed tf_agent.train(...) = 3.301 [loss]\n",
      "I0624 23:41:28.939375 22948 agents.py:69] training 281 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:28.940377 22948 agents.py:69] training 282 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:29.076235 22948 agents.py:69] training 282 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:29.078226 22948 agents.py:69] training 282 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:29.278468 22948 agents.py:69] training 282 of 2000: completed tf_agent.train(...) = 6.553 [loss]\n",
      "I0624 23:41:29.279468 22948 agents.py:69] training 282 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:29.282470 22948 agents.py:69] training 283 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:29.558461 22948 agents.py:69] training 283 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:29.560460 22948 agents.py:69] training 283 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:31.560914 22948 agents.py:69] training 283 of 2000: completed tf_agent.train(...) = 4.189 [loss]\n",
      "I0624 23:41:31.561993 22948 agents.py:69] training 283 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:31.563909 22948 agents.py:69] training 284 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:31.726703 22948 agents.py:69] training 284 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:31.729771 22948 agents.py:69] training 284 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:33.658619 22948 agents.py:69] training 284 of 2000: completed tf_agent.train(...) = 2.716 [loss]\n",
      "I0624 23:41:33.660620 22948 agents.py:69] training 284 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:33.663617 22948 agents.py:69] training 285 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:33.858378 22948 agents.py:69] training 285 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:33.861379 22948 agents.py:69] training 285 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:34.096935 22948 agents.py:69] training 285 of 2000: completed tf_agent.train(...) = 3.720 [loss]\n",
      "I0624 23:41:34.097934 22948 agents.py:69] training 285 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:34.101379 22948 agents.py:69] training 286 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:34.306239 22948 agents.py:69] training 286 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:34.311234 22948 agents.py:69] training 286 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:34.565052 22948 agents.py:69] training 286 of 2000: completed tf_agent.train(...) = 1.935 [loss]\n",
      "I0624 23:41:34.567050 22948 agents.py:69] training 286 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:34.569053 22948 agents.py:69] training 287 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:34.745821 22948 agents.py:69] training 287 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:34.750821 22948 agents.py:69] training 287 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:34.999405 22948 agents.py:69] training 287 of 2000: completed tf_agent.train(...) = 2.846 [loss]\n",
      "I0624 23:41:35.002401 22948 agents.py:69] training 287 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:35.004401 22948 agents.py:69] training 288 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:35.189765 22948 agents.py:69] training 288 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:35.192152 22948 agents.py:69] training 288 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:35.437526 22948 agents.py:69] training 288 of 2000: completed tf_agent.train(...) = 1.756 [loss]\n",
      "I0624 23:41:35.438631 22948 agents.py:69] training 288 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:35.441119 22948 agents.py:69] training 289 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:35.616593 22948 agents.py:69] training 289 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:35.621592 22948 agents.py:69] training 289 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:35.861005 22948 agents.py:69] training 289 of 2000: completed tf_agent.train(...) = 1.459 [loss]\n",
      "I0624 23:41:35.864004 22948 agents.py:69] training 289 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:35.865257 22948 agents.py:69] training 290 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:36.086085 22948 agents.py:69] training 290 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:36.089452 22948 agents.py:69] training 290 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:36.344733 22948 agents.py:69] training 290 of 2000: completed tf_agent.train(...) = 8.036 [loss]\n",
      "I0624 23:41:36.346731 22948 agents.py:69] training 290 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:36.348728 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:41:45.679474 22948 agents.py:69] completed compute_avg_return(...) = -1.034\n",
      "I0624 23:41:45.680453 22948 agents.py:69] training 291 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:45.875356 22948 agents.py:69] training 291 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:45.878099 22948 agents.py:69] training 291 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:46.135741 22948 agents.py:69] training 291 of 2000: completed tf_agent.train(...) = 3.853 [loss]\n",
      "I0624 23:41:46.137741 22948 agents.py:69] training 291 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:46.140743 22948 agents.py:69] training 292 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:46.354560 22948 agents.py:69] training 292 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:46.357634 22948 agents.py:69] training 292 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:46.600352 22948 agents.py:69] training 292 of 2000: completed tf_agent.train(...) = 2.474 [loss]\n",
      "I0624 23:41:46.603344 22948 agents.py:69] training 292 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:46.606354 22948 agents.py:69] training 293 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:46.784454 22948 agents.py:69] training 293 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:46.787255 22948 agents.py:69] training 293 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:47.041884 22948 agents.py:69] training 293 of 2000: completed tf_agent.train(...) = 1.614 [loss]\n",
      "I0624 23:41:47.043896 22948 agents.py:69] training 293 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:47.045981 22948 agents.py:69] training 294 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:47.211685 22948 agents.py:69] training 294 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:47.214687 22948 agents.py:69] training 294 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:47.461424 22948 agents.py:69] training 294 of 2000: completed tf_agent.train(...) = 2.452 [loss]\n",
      "I0624 23:41:47.463427 22948 agents.py:69] training 294 of 2000: executing replay_buffer.clear()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:41:47.465474 22948 agents.py:69] training 295 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:47.639211 22948 agents.py:69] training 295 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:47.644227 22948 agents.py:69] training 295 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:49.986758 22948 agents.py:69] training 295 of 2000: completed tf_agent.train(...) = 1.429 [loss]\n",
      "I0624 23:41:49.988759 22948 agents.py:69] training 295 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:49.989762 22948 agents.py:69] training 296 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:50.170161 22948 agents.py:69] training 296 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:50.175119 22948 agents.py:69] training 296 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:50.447449 22948 agents.py:69] training 296 of 2000: completed tf_agent.train(...) = 3.098 [loss]\n",
      "I0624 23:41:50.448451 22948 agents.py:69] training 296 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:50.450453 22948 agents.py:69] training 297 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:50.646417 22948 agents.py:69] training 297 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:50.651606 22948 agents.py:69] training 297 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:52.866351 22948 agents.py:69] training 297 of 2000: completed tf_agent.train(...) = 1.176 [loss]\n",
      "I0624 23:41:52.867349 22948 agents.py:69] training 297 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:52.869846 22948 agents.py:69] training 298 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:53.073661 22948 agents.py:69] training 298 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:53.077307 22948 agents.py:69] training 298 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:53.320528 22948 agents.py:69] training 298 of 2000: completed tf_agent.train(...) = 10.944 [loss]\n",
      "I0624 23:41:53.322526 22948 agents.py:69] training 298 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:53.324528 22948 agents.py:69] training 299 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:53.480166 22948 agents.py:69] training 299 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:53.485169 22948 agents.py:69] training 299 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:53.739812 22948 agents.py:69] training 299 of 2000: completed tf_agent.train(...) = 1.915 [loss]\n",
      "I0624 23:41:53.742802 22948 agents.py:69] training 299 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:53.745816 22948 agents.py:69] training 300 of 2000: executing collect_driver.run()\n",
      "I0624 23:41:53.954759 22948 agents.py:69] training 300 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:41:53.958843 22948 agents.py:69] training 300 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:41:54.212968 22948 agents.py:69] training 300 of 2000: completed tf_agent.train(...) = 4.018 [loss]\n",
      "I0624 23:41:54.214967 22948 agents.py:69] training 300 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:41:54.217016 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:42:00.726505 22948 agents.py:69] completed compute_avg_return(...) = -0.543\n",
      "I0624 23:42:00.728510 22948 agents.py:69] training 301 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:00.911454 22948 agents.py:69] training 301 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:00.916468 22948 agents.py:69] training 301 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:01.179158 22948 agents.py:69] training 301 of 2000: completed tf_agent.train(...) = 1.665 [loss]\n",
      "I0624 23:42:01.181158 22948 agents.py:69] training 301 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:01.184176 22948 agents.py:69] training 302 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:01.369205 22948 agents.py:69] training 302 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:01.374167 22948 agents.py:69] training 302 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:01.641376 22948 agents.py:69] training 302 of 2000: completed tf_agent.train(...) = 5.248 [loss]\n",
      "I0624 23:42:01.643543 22948 agents.py:69] training 302 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:01.645222 22948 agents.py:69] training 303 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:01.834356 22948 agents.py:69] training 303 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:01.838572 22948 agents.py:69] training 303 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:02.097556 22948 agents.py:69] training 303 of 2000: completed tf_agent.train(...) = 1.759 [loss]\n",
      "I0624 23:42:02.098388 22948 agents.py:69] training 303 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:02.101396 22948 agents.py:69] training 304 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:02.303605 22948 agents.py:69] training 304 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:02.307602 22948 agents.py:69] training 304 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:02.565274 22948 agents.py:69] training 304 of 2000: completed tf_agent.train(...) = 1.600 [loss]\n",
      "I0624 23:42:02.567271 22948 agents.py:69] training 304 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:02.570272 22948 agents.py:69] training 305 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:02.775947 22948 agents.py:69] training 305 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:02.780906 22948 agents.py:69] training 305 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:03.002489 22948 agents.py:69] training 305 of 2000: completed tf_agent.train(...) = 4.053 [loss]\n",
      "I0624 23:42:03.004489 22948 agents.py:69] training 305 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:03.007487 22948 agents.py:69] training 306 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:03.200007 22948 agents.py:69] training 306 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:03.205051 22948 agents.py:69] training 306 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:05.512713 22948 agents.py:69] training 306 of 2000: completed tf_agent.train(...) = 3.240 [loss]\n",
      "I0624 23:42:05.514715 22948 agents.py:69] training 306 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:05.517833 22948 agents.py:69] training 307 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:05.691685 22948 agents.py:69] training 307 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:05.695640 22948 agents.py:69] training 307 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:05.946572 22948 agents.py:69] training 307 of 2000: completed tf_agent.train(...) = 2.079 [loss]\n",
      "I0624 23:42:05.947572 22948 agents.py:69] training 307 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:05.950580 22948 agents.py:69] training 308 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:06.189781 22948 agents.py:69] training 308 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:06.192759 22948 agents.py:69] training 308 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:06.455543 22948 agents.py:69] training 308 of 2000: completed tf_agent.train(...) = 38.055 [loss]\n",
      "I0624 23:42:06.458542 22948 agents.py:69] training 308 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:06.460720 22948 agents.py:69] training 309 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:06.663632 22948 agents.py:69] training 309 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:06.669664 22948 agents.py:69] training 309 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:06.926666 22948 agents.py:69] training 309 of 2000: completed tf_agent.train(...) = 1.782 [loss]\n",
      "I0624 23:42:06.928668 22948 agents.py:69] training 309 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:06.929663 22948 agents.py:69] training 310 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:07.133333 22948 agents.py:69] training 310 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:07.138434 22948 agents.py:69] training 310 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:09.404428 22948 agents.py:69] training 310 of 2000: completed tf_agent.train(...) = 2.290 [loss]\n",
      "I0624 23:42:09.405356 22948 agents.py:69] training 310 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:09.408356 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:42:13.084187 22948 agents.py:69] completed compute_avg_return(...) = 0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:42:13.085183 22948 agents.py:69] training 311 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:13.334156 22948 agents.py:69] training 311 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:13.338159 22948 agents.py:69] training 311 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:13.586273 22948 agents.py:69] training 311 of 2000: completed tf_agent.train(...) = 2.412 [loss]\n",
      "I0624 23:42:13.589079 22948 agents.py:69] training 311 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:13.590283 22948 agents.py:69] training 312 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:13.816943 22948 agents.py:69] training 312 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:13.820732 22948 agents.py:69] training 312 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:14.086432 22948 agents.py:69] training 312 of 2000: completed tf_agent.train(...) = 2.271 [loss]\n",
      "I0624 23:42:14.088624 22948 agents.py:69] training 312 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:14.090435 22948 agents.py:69] training 313 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:14.272464 22948 agents.py:69] training 313 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:14.277484 22948 agents.py:69] training 313 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:14.555365 22948 agents.py:69] training 313 of 2000: completed tf_agent.train(...) = 3.235 [loss]\n",
      "I0624 23:42:14.557358 22948 agents.py:69] training 313 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:14.559474 22948 agents.py:69] training 314 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:14.751374 22948 agents.py:69] training 314 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:14.755709 22948 agents.py:69] training 314 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:15.006465 22948 agents.py:69] training 314 of 2000: completed tf_agent.train(...) = 2.556 [loss]\n",
      "I0624 23:42:15.007714 22948 agents.py:69] training 314 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:15.009592 22948 agents.py:69] training 315 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:15.329322 22948 agents.py:69] training 315 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:15.334367 22948 agents.py:69] training 315 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:15.624996 22948 agents.py:69] training 315 of 2000: completed tf_agent.train(...) = 80.783 [loss]\n",
      "I0624 23:42:15.626620 22948 agents.py:69] training 315 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:15.628688 22948 agents.py:69] training 316 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:15.868813 22948 agents.py:69] training 316 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:15.871813 22948 agents.py:69] training 316 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:16.137338 22948 agents.py:69] training 316 of 2000: completed tf_agent.train(...) = 15.649 [loss]\n",
      "I0624 23:42:16.138267 22948 agents.py:69] training 316 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:16.141268 22948 agents.py:69] training 317 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:16.343342 22948 agents.py:69] training 317 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:16.347332 22948 agents.py:69] training 317 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:16.581854 22948 agents.py:69] training 317 of 2000: completed tf_agent.train(...) = 6.523 [loss]\n",
      "I0624 23:42:16.583852 22948 agents.py:69] training 317 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:16.586261 22948 agents.py:69] training 318 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:16.796540 22948 agents.py:69] training 318 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:16.801506 22948 agents.py:69] training 318 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:17.050059 22948 agents.py:69] training 318 of 2000: completed tf_agent.train(...) = 4.822 [loss]\n",
      "I0624 23:42:17.052059 22948 agents.py:69] training 318 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:17.055053 22948 agents.py:69] training 319 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:17.220283 22948 agents.py:69] training 319 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:17.225282 22948 agents.py:69] training 319 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:17.457550 22948 agents.py:69] training 319 of 2000: completed tf_agent.train(...) = 4.579 [loss]\n",
      "I0624 23:42:17.459550 22948 agents.py:69] training 319 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:17.460624 22948 agents.py:69] training 320 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:17.718512 22948 agents.py:69] training 320 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:17.723479 22948 agents.py:69] training 320 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:20.040450 22948 agents.py:69] training 320 of 2000: completed tf_agent.train(...) = 18.108 [loss]\n",
      "I0624 23:42:20.042610 22948 agents.py:69] training 320 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:20.044451 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:42:29.385366 22948 agents.py:69] completed compute_avg_return(...) = -1.112\n",
      "I0624 23:42:29.386469 22948 agents.py:69] training 321 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:29.591890 22948 agents.py:69] training 321 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:29.597181 22948 agents.py:69] training 321 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:29.861899 22948 agents.py:69] training 321 of 2000: completed tf_agent.train(...) = 2.831 [loss]\n",
      "I0624 23:42:29.862898 22948 agents.py:69] training 321 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:29.865349 22948 agents.py:69] training 322 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:30.075573 22948 agents.py:69] training 322 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:30.079570 22948 agents.py:69] training 322 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:30.330180 22948 agents.py:69] training 322 of 2000: completed tf_agent.train(...) = 7.291 [loss]\n",
      "I0624 23:42:30.332371 22948 agents.py:69] training 322 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:30.333516 22948 agents.py:69] training 323 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:30.488561 22948 agents.py:69] training 323 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:30.494570 22948 agents.py:69] training 323 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:30.755639 22948 agents.py:69] training 323 of 2000: completed tf_agent.train(...) = 3.049 [loss]\n",
      "I0624 23:42:30.757757 22948 agents.py:69] training 323 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:30.759762 22948 agents.py:69] training 324 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:30.954493 22948 agents.py:69] training 324 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:30.958362 22948 agents.py:69] training 324 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:31.210725 22948 agents.py:69] training 324 of 2000: completed tf_agent.train(...) = 1.678 [loss]\n",
      "I0624 23:42:31.212232 22948 agents.py:69] training 324 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:31.213292 22948 agents.py:69] training 325 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:31.409606 22948 agents.py:69] training 325 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:31.414556 22948 agents.py:69] training 325 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:31.683804 22948 agents.py:69] training 325 of 2000: completed tf_agent.train(...) = 2.651 [loss]\n",
      "I0624 23:42:31.685800 22948 agents.py:69] training 325 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:31.686799 22948 agents.py:69] training 326 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:31.949524 22948 agents.py:69] training 326 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:31.954429 22948 agents.py:69] training 326 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:32.203187 22948 agents.py:69] training 326 of 2000: completed tf_agent.train(...) = 4.485 [loss]\n",
      "I0624 23:42:32.205311 22948 agents.py:69] training 326 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:32.207300 22948 agents.py:69] training 327 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:32.416407 22948 agents.py:69] training 327 of 2000: executing replay_buffer.gather_all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:42:32.419265 22948 agents.py:69] training 327 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:32.672405 22948 agents.py:69] training 327 of 2000: completed tf_agent.train(...) = 16.660 [loss]\n",
      "I0624 23:42:32.674153 22948 agents.py:69] training 327 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:32.676086 22948 agents.py:69] training 328 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:32.852756 22948 agents.py:69] training 328 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:32.856860 22948 agents.py:69] training 328 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:33.106337 22948 agents.py:69] training 328 of 2000: completed tf_agent.train(...) = 2.059 [loss]\n",
      "I0624 23:42:33.107167 22948 agents.py:69] training 328 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:33.110170 22948 agents.py:69] training 329 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:33.277541 22948 agents.py:69] training 329 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:33.282576 22948 agents.py:69] training 329 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:33.526598 22948 agents.py:69] training 329 of 2000: completed tf_agent.train(...) = 2.671 [loss]\n",
      "I0624 23:42:33.527808 22948 agents.py:69] training 329 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:33.529603 22948 agents.py:69] training 330 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:33.698362 22948 agents.py:69] training 330 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:33.703450 22948 agents.py:69] training 330 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:33.954453 22948 agents.py:69] training 330 of 2000: completed tf_agent.train(...) = 1.917 [loss]\n",
      "I0624 23:42:33.956454 22948 agents.py:69] training 330 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:33.959449 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:42:42.045153 22948 agents.py:69] completed compute_avg_return(...) = -0.869\n",
      "I0624 23:42:42.045796 22948 agents.py:69] training 331 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:42.276751 22948 agents.py:69] training 331 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:42.279753 22948 agents.py:69] training 331 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:42.486515 22948 agents.py:69] training 331 of 2000: completed tf_agent.train(...) = 6.818 [loss]\n",
      "I0624 23:42:42.488547 22948 agents.py:69] training 331 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:42.491545 22948 agents.py:69] training 332 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:42.690557 22948 agents.py:69] training 332 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:42.695514 22948 agents.py:69] training 332 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:42.966927 22948 agents.py:69] training 332 of 2000: completed tf_agent.train(...) = 2.254 [loss]\n",
      "I0624 23:42:42.968922 22948 agents.py:69] training 332 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:42.971313 22948 agents.py:69] training 333 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:43.256777 22948 agents.py:69] training 333 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:43.261779 22948 agents.py:69] training 333 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:43.562212 22948 agents.py:69] training 333 of 2000: completed tf_agent.train(...) = 14.773 [loss]\n",
      "I0624 23:42:43.564024 22948 agents.py:69] training 333 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:43.566808 22948 agents.py:69] training 334 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:43.786511 22948 agents.py:69] training 334 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:43.790354 22948 agents.py:69] training 334 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:44.048886 22948 agents.py:69] training 334 of 2000: completed tf_agent.train(...) = 3.243 [loss]\n",
      "I0624 23:42:44.050883 22948 agents.py:69] training 334 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:44.052176 22948 agents.py:69] training 335 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:44.298965 22948 agents.py:69] training 335 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:44.303928 22948 agents.py:69] training 335 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:44.573942 22948 agents.py:69] training 335 of 2000: completed tf_agent.train(...) = 34.235 [loss]\n",
      "I0624 23:42:44.576068 22948 agents.py:69] training 335 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:44.578081 22948 agents.py:69] training 336 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:44.773038 22948 agents.py:69] training 336 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:44.775690 22948 agents.py:69] training 336 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:45.048257 22948 agents.py:69] training 336 of 2000: completed tf_agent.train(...) = 4.231 [loss]\n",
      "I0624 23:42:45.049241 22948 agents.py:69] training 336 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:45.052248 22948 agents.py:69] training 337 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:45.276292 22948 agents.py:69] training 337 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:45.279221 22948 agents.py:69] training 337 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:45.541518 22948 agents.py:69] training 337 of 2000: completed tf_agent.train(...) = 3.955 [loss]\n",
      "I0624 23:42:45.543518 22948 agents.py:69] training 337 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:45.546023 22948 agents.py:69] training 338 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:45.736192 22948 agents.py:69] training 338 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:45.740065 22948 agents.py:69] training 338 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:46.011180 22948 agents.py:69] training 338 of 2000: completed tf_agent.train(...) = 2.823 [loss]\n",
      "I0624 23:42:46.013180 22948 agents.py:69] training 338 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:46.015177 22948 agents.py:69] training 339 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:46.279025 22948 agents.py:69] training 339 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:46.282574 22948 agents.py:69] training 339 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:46.574012 22948 agents.py:69] training 339 of 2000: completed tf_agent.train(...) = 12.091 [loss]\n",
      "I0624 23:42:46.576016 22948 agents.py:69] training 339 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:46.578011 22948 agents.py:69] training 340 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:46.815029 22948 agents.py:69] training 340 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:46.818057 22948 agents.py:69] training 340 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:47.079614 22948 agents.py:69] training 340 of 2000: completed tf_agent.train(...) = 8.904 [loss]\n",
      "I0624 23:42:47.081616 22948 agents.py:69] training 340 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:47.083791 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:42:55.424787 22948 agents.py:69] completed compute_avg_return(...) = -0.723\n",
      "I0624 23:42:55.425787 22948 agents.py:69] training 341 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:55.692827 22948 agents.py:69] training 341 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:55.698781 22948 agents.py:69] training 341 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:55.973256 22948 agents.py:69] training 341 of 2000: completed tf_agent.train(...) = 7.562 [loss]\n",
      "I0624 23:42:55.976257 22948 agents.py:69] training 341 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:55.978254 22948 agents.py:69] training 342 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:56.202306 22948 agents.py:69] training 342 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:56.205441 22948 agents.py:69] training 342 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:56.477546 22948 agents.py:69] training 342 of 2000: completed tf_agent.train(...) = 3.219 [loss]\n",
      "I0624 23:42:56.478545 22948 agents.py:69] training 342 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:56.479623 22948 agents.py:69] training 343 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:56.708108 22948 agents.py:69] training 343 of 2000: executing replay_buffer.gather_all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:42:56.714163 22948 agents.py:69] training 343 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:56.987075 22948 agents.py:69] training 343 of 2000: completed tf_agent.train(...) = 5.352 [loss]\n",
      "I0624 23:42:56.988074 22948 agents.py:69] training 343 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:56.990075 22948 agents.py:69] training 344 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:57.189771 22948 agents.py:69] training 344 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:57.194735 22948 agents.py:69] training 344 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:57.473607 22948 agents.py:69] training 344 of 2000: completed tf_agent.train(...) = 3.148 [loss]\n",
      "I0624 23:42:57.476707 22948 agents.py:69] training 344 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:57.478417 22948 agents.py:69] training 345 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:57.672827 22948 agents.py:69] training 345 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:57.676970 22948 agents.py:69] training 345 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:57.953685 22948 agents.py:69] training 345 of 2000: completed tf_agent.train(...) = 2.435 [loss]\n",
      "I0624 23:42:57.956683 22948 agents.py:69] training 345 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:57.958685 22948 agents.py:69] training 346 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:58.151394 22948 agents.py:69] training 346 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:58.155274 22948 agents.py:69] training 346 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:58.423631 22948 agents.py:69] training 346 of 2000: completed tf_agent.train(...) = 3.188 [loss]\n",
      "I0624 23:42:58.425630 22948 agents.py:69] training 346 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:58.428637 22948 agents.py:69] training 347 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:58.647837 22948 agents.py:69] training 347 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:58.650962 22948 agents.py:69] training 347 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:42:58.893338 22948 agents.py:69] training 347 of 2000: completed tf_agent.train(...) = 4.048 [loss]\n",
      "I0624 23:42:58.895341 22948 agents.py:69] training 347 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:42:58.897693 22948 agents.py:69] training 348 of 2000: executing collect_driver.run()\n",
      "I0624 23:42:59.104290 22948 agents.py:69] training 348 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:42:59.108845 22948 agents.py:69] training 348 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:01.516522 22948 agents.py:69] training 348 of 2000: completed tf_agent.train(...) = 5.553 [loss]\n",
      "I0624 23:43:01.518521 22948 agents.py:69] training 348 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:01.521688 22948 agents.py:69] training 349 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:01.783126 22948 agents.py:69] training 349 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:01.787611 22948 agents.py:69] training 349 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:04.206500 22948 agents.py:69] training 349 of 2000: completed tf_agent.train(...) = 5.303 [loss]\n",
      "I0624 23:43:04.209522 22948 agents.py:69] training 349 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:04.211510 22948 agents.py:69] training 350 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:04.421518 22948 agents.py:69] training 350 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:04.426473 22948 agents.py:69] training 350 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:04.694432 22948 agents.py:69] training 350 of 2000: completed tf_agent.train(...) = 2.533 [loss]\n",
      "I0624 23:43:04.696970 22948 agents.py:69] training 350 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:04.698734 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:43:08.727751 22948 agents.py:69] completed compute_avg_return(...) = 0.107\n",
      "I0624 23:43:08.729801 22948 agents.py:69] training 351 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:08.905734 22948 agents.py:69] training 351 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:08.910750 22948 agents.py:69] training 351 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:09.177242 22948 agents.py:69] training 351 of 2000: completed tf_agent.train(...) = 2.862 [loss]\n",
      "I0624 23:43:09.178176 22948 agents.py:69] training 351 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:09.181308 22948 agents.py:69] training 352 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:09.432056 22948 agents.py:69] training 352 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:09.436053 22948 agents.py:69] training 352 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:09.705205 22948 agents.py:69] training 352 of 2000: completed tf_agent.train(...) = 4.483 [loss]\n",
      "I0624 23:43:09.707201 22948 agents.py:69] training 352 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:09.709999 22948 agents.py:69] training 353 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:09.902274 22948 agents.py:69] training 353 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:09.905306 22948 agents.py:69] training 353 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:10.163840 22948 agents.py:69] training 353 of 2000: completed tf_agent.train(...) = 8.215 [loss]\n",
      "I0624 23:43:10.167085 22948 agents.py:69] training 353 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:10.167828 22948 agents.py:69] training 354 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:10.435817 22948 agents.py:69] training 354 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:10.439557 22948 agents.py:69] training 354 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:10.694290 22948 agents.py:69] training 354 of 2000: completed tf_agent.train(...) = 11.460 [loss]\n",
      "I0624 23:43:10.696889 22948 agents.py:69] training 354 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:10.698490 22948 agents.py:69] training 355 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:10.881458 22948 agents.py:69] training 355 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:10.886624 22948 agents.py:69] training 355 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:11.123611 22948 agents.py:69] training 355 of 2000: completed tf_agent.train(...) = 1.743 [loss]\n",
      "I0624 23:43:11.125611 22948 agents.py:69] training 355 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:11.127610 22948 agents.py:69] training 356 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:11.364387 22948 agents.py:69] training 356 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:11.369351 22948 agents.py:69] training 356 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:13.706798 22948 agents.py:69] training 356 of 2000: completed tf_agent.train(...) = 7.213 [loss]\n",
      "I0624 23:43:13.707799 22948 agents.py:69] training 356 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:13.709799 22948 agents.py:69] training 357 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:13.918813 22948 agents.py:69] training 357 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:13.923818 22948 agents.py:69] training 357 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:14.201427 22948 agents.py:69] training 357 of 2000: completed tf_agent.train(...) = 6.253 [loss]\n",
      "I0624 23:43:14.203432 22948 agents.py:69] training 357 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:14.206430 22948 agents.py:69] training 358 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:14.414478 22948 agents.py:69] training 358 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:14.420486 22948 agents.py:69] training 358 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:14.683414 22948 agents.py:69] training 358 of 2000: completed tf_agent.train(...) = 1.631 [loss]\n",
      "I0624 23:43:14.684412 22948 agents.py:69] training 358 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:14.687409 22948 agents.py:69] training 359 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:14.853412 22948 agents.py:69] training 359 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:14.856415 22948 agents.py:69] training 359 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:15.127121 22948 agents.py:69] training 359 of 2000: completed tf_agent.train(...) = 2.437 [loss]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:43:15.129128 22948 agents.py:69] training 359 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:15.130118 22948 agents.py:69] training 360 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:15.415124 22948 agents.py:69] training 360 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:15.419124 22948 agents.py:69] training 360 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:15.671732 22948 agents.py:69] training 360 of 2000: completed tf_agent.train(...) = 33.846 [loss]\n",
      "I0624 23:43:15.672733 22948 agents.py:69] training 360 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:15.674735 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:43:22.882349 22948 agents.py:69] completed compute_avg_return(...) = -0.519\n",
      "I0624 23:43:22.883336 22948 agents.py:69] training 361 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:23.098340 22948 agents.py:69] training 361 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:23.101390 22948 agents.py:69] training 361 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:23.350745 22948 agents.py:69] training 361 of 2000: completed tf_agent.train(...) = 5.696 [loss]\n",
      "I0624 23:43:23.352615 22948 agents.py:69] training 361 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:23.353614 22948 agents.py:69] training 362 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:23.586622 22948 agents.py:69] training 362 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:23.590622 22948 agents.py:69] training 362 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:23.871547 22948 agents.py:69] training 362 of 2000: completed tf_agent.train(...) = 3.619 [loss]\n",
      "I0624 23:43:23.874554 22948 agents.py:69] training 362 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:23.876863 22948 agents.py:69] training 363 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:24.214550 22948 agents.py:69] training 363 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:24.218549 22948 agents.py:69] training 363 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:24.496541 22948 agents.py:69] training 363 of 2000: completed tf_agent.train(...) = 3.349 [loss]\n",
      "I0624 23:43:24.499543 22948 agents.py:69] training 363 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:24.502545 22948 agents.py:69] training 364 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:24.699547 22948 agents.py:69] training 364 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:24.704554 22948 agents.py:69] training 364 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:24.961544 22948 agents.py:69] training 364 of 2000: completed tf_agent.train(...) = 2.363 [loss]\n",
      "I0624 23:43:24.964541 22948 agents.py:69] training 364 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:24.966541 22948 agents.py:69] training 365 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:25.163796 22948 agents.py:69] training 365 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:25.167794 22948 agents.py:69] training 365 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:25.417797 22948 agents.py:69] training 365 of 2000: completed tf_agent.train(...) = 1.630 [loss]\n",
      "I0624 23:43:25.419802 22948 agents.py:69] training 365 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:25.420790 22948 agents.py:69] training 366 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:25.675801 22948 agents.py:69] training 366 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:25.679796 22948 agents.py:69] training 366 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:25.974982 22948 agents.py:69] training 366 of 2000: completed tf_agent.train(...) = 4.187 [loss]\n",
      "I0624 23:43:25.976980 22948 agents.py:69] training 366 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:25.979361 22948 agents.py:69] training 367 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:26.218992 22948 agents.py:69] training 367 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:26.226048 22948 agents.py:69] training 367 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:26.571426 22948 agents.py:69] training 367 of 2000: completed tf_agent.train(...) = 2.209 [loss]\n",
      "I0624 23:43:26.572426 22948 agents.py:69] training 367 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:26.575534 22948 agents.py:69] training 368 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:26.766093 22948 agents.py:69] training 368 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:26.771088 22948 agents.py:69] training 368 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:27.034044 22948 agents.py:69] training 368 of 2000: completed tf_agent.train(...) = 1.633 [loss]\n",
      "I0624 23:43:27.036041 22948 agents.py:69] training 368 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:27.038172 22948 agents.py:69] training 369 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:27.397775 22948 agents.py:69] training 369 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:27.404792 22948 agents.py:69] training 369 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:27.725872 22948 agents.py:69] training 369 of 2000: completed tf_agent.train(...) = 19.961 [loss]\n",
      "I0624 23:43:27.727871 22948 agents.py:69] training 369 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:27.730877 22948 agents.py:69] training 370 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:27.932836 22948 agents.py:69] training 370 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:27.936636 22948 agents.py:69] training 370 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:28.199640 22948 agents.py:69] training 370 of 2000: completed tf_agent.train(...) = 2.521 [loss]\n",
      "I0624 23:43:28.201708 22948 agents.py:69] training 370 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:28.203642 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:43:32.265876 22948 agents.py:69] completed compute_avg_return(...) = 0.110\n",
      "I0624 23:43:32.267878 22948 agents.py:69] training 371 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:32.509902 22948 agents.py:69] training 371 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:32.514906 22948 agents.py:69] training 371 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:32.782731 22948 agents.py:69] training 371 of 2000: completed tf_agent.train(...) = 2.989 [loss]\n",
      "I0624 23:43:32.784732 22948 agents.py:69] training 371 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:32.786732 22948 agents.py:69] training 372 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:32.979505 22948 agents.py:69] training 372 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:32.984383 22948 agents.py:69] training 372 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:33.238388 22948 agents.py:69] training 372 of 2000: completed tf_agent.train(...) = 2.044 [loss]\n",
      "I0624 23:43:33.239380 22948 agents.py:69] training 372 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:33.241383 22948 agents.py:69] training 373 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:33.451232 22948 agents.py:69] training 373 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:33.456276 22948 agents.py:69] training 373 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:33.718472 22948 agents.py:69] training 373 of 2000: completed tf_agent.train(...) = 2.595 [loss]\n",
      "I0624 23:43:33.720476 22948 agents.py:69] training 373 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:33.722983 22948 agents.py:69] training 374 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:33.956063 22948 agents.py:69] training 374 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:33.961067 22948 agents.py:69] training 374 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:34.328933 22948 agents.py:69] training 374 of 2000: completed tf_agent.train(...) = 2.337 [loss]\n",
      "I0624 23:43:34.330936 22948 agents.py:69] training 374 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:34.333936 22948 agents.py:69] training 375 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:34.536941 22948 agents.py:69] training 375 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:34.541949 22948 agents.py:69] training 375 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:36.977004 22948 agents.py:69] training 375 of 2000: completed tf_agent.train(...) = 5.276 [loss]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:43:36.980003 22948 agents.py:69] training 375 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:36.983016 22948 agents.py:69] training 376 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:37.190006 22948 agents.py:69] training 376 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:37.194038 22948 agents.py:69] training 376 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:37.472204 22948 agents.py:69] training 376 of 2000: completed tf_agent.train(...) = 4.980 [loss]\n",
      "I0624 23:43:37.474212 22948 agents.py:69] training 376 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:37.476384 22948 agents.py:69] training 377 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:37.701400 22948 agents.py:69] training 377 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:37.709366 22948 agents.py:69] training 377 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:38.022799 22948 agents.py:69] training 377 of 2000: completed tf_agent.train(...) = 1.585 [loss]\n",
      "I0624 23:43:38.025929 22948 agents.py:69] training 377 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:38.027976 22948 agents.py:69] training 378 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:38.216820 22948 agents.py:69] training 378 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:38.221824 22948 agents.py:69] training 378 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:40.668532 22948 agents.py:69] training 378 of 2000: completed tf_agent.train(...) = 1.224 [loss]\n",
      "I0624 23:43:40.670443 22948 agents.py:69] training 378 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:40.672446 22948 agents.py:69] training 379 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:40.994945 22948 agents.py:69] training 379 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:41.000898 22948 agents.py:69] training 379 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:41.391905 22948 agents.py:69] training 379 of 2000: completed tf_agent.train(...) = 4.340 [loss]\n",
      "I0624 23:43:41.394905 22948 agents.py:69] training 379 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:41.398906 22948 agents.py:69] training 380 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:41.657185 22948 agents.py:69] training 380 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:41.661173 22948 agents.py:69] training 380 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:41.903176 22948 agents.py:69] training 380 of 2000: completed tf_agent.train(...) = 3.469 [loss]\n",
      "I0624 23:43:41.906382 22948 agents.py:69] training 380 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:41.908423 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:43:46.438733 22948 agents.py:69] completed compute_avg_return(...) = 0.232\n",
      "I0624 23:43:46.440733 22948 agents.py:69] training 381 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:46.645864 22948 agents.py:69] training 381 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:46.648728 22948 agents.py:69] training 381 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:46.909840 22948 agents.py:69] training 381 of 2000: completed tf_agent.train(...) = 8.297 [loss]\n",
      "I0624 23:43:46.910923 22948 agents.py:69] training 381 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:46.912843 22948 agents.py:69] training 382 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:47.079250 22948 agents.py:69] training 382 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:47.081939 22948 agents.py:69] training 382 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:47.343574 22948 agents.py:69] training 382 of 2000: completed tf_agent.train(...) = 2.254 [loss]\n",
      "I0624 23:43:47.345573 22948 agents.py:69] training 382 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:47.348151 22948 agents.py:69] training 383 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:47.536622 22948 agents.py:69] training 383 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:47.542572 22948 agents.py:69] training 383 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:47.803811 22948 agents.py:69] training 383 of 2000: completed tf_agent.train(...) = 2.691 [loss]\n",
      "I0624 23:43:47.805812 22948 agents.py:69] training 383 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:47.807813 22948 agents.py:69] training 384 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:48.003813 22948 agents.py:69] training 384 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:48.008817 22948 agents.py:69] training 384 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:48.281714 22948 agents.py:69] training 384 of 2000: completed tf_agent.train(...) = 1.675 [loss]\n",
      "I0624 23:43:48.283237 22948 agents.py:69] training 384 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:48.284238 22948 agents.py:69] training 385 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:48.481243 22948 agents.py:69] training 385 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:48.486239 22948 agents.py:69] training 385 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:48.746399 22948 agents.py:69] training 385 of 2000: completed tf_agent.train(...) = 4.512 [loss]\n",
      "I0624 23:43:48.748398 22948 agents.py:69] training 385 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:48.749403 22948 agents.py:69] training 386 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:48.943815 22948 agents.py:69] training 386 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:48.947818 22948 agents.py:69] training 386 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:49.187823 22948 agents.py:69] training 386 of 2000: completed tf_agent.train(...) = 1.256 [loss]\n",
      "I0624 23:43:49.189818 22948 agents.py:69] training 386 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:49.192097 22948 agents.py:69] training 387 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:49.394300 22948 agents.py:69] training 387 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:49.398318 22948 agents.py:69] training 387 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:49.636982 22948 agents.py:69] training 387 of 2000: completed tf_agent.train(...) = 2.203 [loss]\n",
      "I0624 23:43:49.638984 22948 agents.py:69] training 387 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:49.640983 22948 agents.py:69] training 388 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:49.889162 22948 agents.py:69] training 388 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:49.893162 22948 agents.py:69] training 388 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:50.186164 22948 agents.py:69] training 388 of 2000: completed tf_agent.train(...) = 6.490 [loss]\n",
      "I0624 23:43:50.188164 22948 agents.py:69] training 388 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:50.190167 22948 agents.py:69] training 389 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:50.477203 22948 agents.py:69] training 389 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:50.480203 22948 agents.py:69] training 389 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:50.740152 22948 agents.py:69] training 389 of 2000: completed tf_agent.train(...) = 23.299 [loss]\n",
      "I0624 23:43:50.741991 22948 agents.py:69] training 389 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:50.743923 22948 agents.py:69] training 390 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:50.967266 22948 agents.py:69] training 390 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:50.971274 22948 agents.py:69] training 390 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:51.258531 22948 agents.py:69] training 390 of 2000: completed tf_agent.train(...) = 1.739 [loss]\n",
      "I0624 23:43:51.259228 22948 agents.py:69] training 390 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:51.261298 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:43:53.934724 22948 agents.py:69] completed compute_avg_return(...) = 0.228\n",
      "I0624 23:43:53.936730 22948 agents.py:69] training 391 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:54.121143 22948 agents.py:69] training 391 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:54.126529 22948 agents.py:69] training 391 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:56.508615 22948 agents.py:69] training 391 of 2000: completed tf_agent.train(...) = 3.218 [loss]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:43:56.510617 22948 agents.py:69] training 391 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:56.511617 22948 agents.py:69] training 392 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:56.714825 22948 agents.py:69] training 392 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:56.718400 22948 agents.py:69] training 392 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:56.975015 22948 agents.py:69] training 392 of 2000: completed tf_agent.train(...) = 2.533 [loss]\n",
      "I0624 23:43:56.977175 22948 agents.py:69] training 392 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:56.978738 22948 agents.py:69] training 393 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:57.130886 22948 agents.py:69] training 393 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:57.135883 22948 agents.py:69] training 393 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:59.516464 22948 agents.py:69] training 393 of 2000: completed tf_agent.train(...) = 1.864 [loss]\n",
      "I0624 23:43:59.518465 22948 agents.py:69] training 393 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:59.521765 22948 agents.py:69] training 394 of 2000: executing collect_driver.run()\n",
      "I0624 23:43:59.716586 22948 agents.py:69] training 394 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:43:59.720595 22948 agents.py:69] training 394 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:43:59.985785 22948 agents.py:69] training 394 of 2000: completed tf_agent.train(...) = 1.067 [loss]\n",
      "I0624 23:43:59.987782 22948 agents.py:69] training 394 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:43:59.989784 22948 agents.py:69] training 395 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:00.325684 22948 agents.py:69] training 395 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:00.330692 22948 agents.py:69] training 395 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:00.629768 22948 agents.py:69] training 395 of 2000: completed tf_agent.train(...) = 4.727 [loss]\n",
      "I0624 23:44:00.631774 22948 agents.py:69] training 395 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:00.634769 22948 agents.py:69] training 396 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:00.820590 22948 agents.py:69] training 396 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:00.829549 22948 agents.py:69] training 396 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:01.140846 22948 agents.py:69] training 396 of 2000: completed tf_agent.train(...) = 2.703 [loss]\n",
      "I0624 23:44:01.142708 22948 agents.py:69] training 396 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:01.144709 22948 agents.py:69] training 397 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:01.363785 22948 agents.py:69] training 397 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:01.367778 22948 agents.py:69] training 397 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:01.644901 22948 agents.py:69] training 397 of 2000: completed tf_agent.train(...) = 3.359 [loss]\n",
      "I0624 23:44:01.645900 22948 agents.py:69] training 397 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:01.648900 22948 agents.py:69] training 398 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:01.803464 22948 agents.py:69] training 398 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:01.808520 22948 agents.py:69] training 398 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:04.318143 22948 agents.py:69] training 398 of 2000: completed tf_agent.train(...) = 1.575 [loss]\n",
      "I0624 23:44:04.320603 22948 agents.py:69] training 398 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:04.323614 22948 agents.py:69] training 399 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:04.512700 22948 agents.py:69] training 399 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:04.517698 22948 agents.py:69] training 399 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:04.767559 22948 agents.py:69] training 399 of 2000: completed tf_agent.train(...) = 1.256 [loss]\n",
      "I0624 23:44:04.768561 22948 agents.py:69] training 399 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:04.770565 22948 agents.py:69] training 400 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:04.997629 22948 agents.py:69] training 400 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:05.001624 22948 agents.py:69] training 400 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:05.270881 22948 agents.py:69] training 400 of 2000: completed tf_agent.train(...) = 5.323 [loss]\n",
      "I0624 23:44:05.272804 22948 agents.py:69] training 400 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:05.273777 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:44:06.508030 22948 agents.py:69] completed compute_avg_return(...) = 0.606\n",
      "I0624 23:44:06.510025 22948 agents.py:69] training 401 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:06.713026 22948 agents.py:69] training 401 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:06.718050 22948 agents.py:69] training 401 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:07.019519 22948 agents.py:69] training 401 of 2000: completed tf_agent.train(...) = 3.001 [loss]\n",
      "I0624 23:44:07.020516 22948 agents.py:69] training 401 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:07.023632 22948 agents.py:69] training 402 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:07.232659 22948 agents.py:69] training 402 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:07.237611 22948 agents.py:69] training 402 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:07.533832 22948 agents.py:69] training 402 of 2000: completed tf_agent.train(...) = 1.349 [loss]\n",
      "I0624 23:44:07.534832 22948 agents.py:69] training 402 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:07.538068 22948 agents.py:69] training 403 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:07.727845 22948 agents.py:69] training 403 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:07.731838 22948 agents.py:69] training 403 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:08.008336 22948 agents.py:69] training 403 of 2000: completed tf_agent.train(...) = 1.213 [loss]\n",
      "I0624 23:44:08.010333 22948 agents.py:69] training 403 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:08.013337 22948 agents.py:69] training 404 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:08.282701 22948 agents.py:69] training 404 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:08.285528 22948 agents.py:69] training 404 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:08.598169 22948 agents.py:69] training 404 of 2000: completed tf_agent.train(...) = 17.649 [loss]\n",
      "I0624 23:44:08.600174 22948 agents.py:69] training 404 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:08.603174 22948 agents.py:69] training 405 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:08.810170 22948 agents.py:69] training 405 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:08.813179 22948 agents.py:69] training 405 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:09.087602 22948 agents.py:69] training 405 of 2000: completed tf_agent.train(...) = 2.583 [loss]\n",
      "I0624 23:44:09.089600 22948 agents.py:69] training 405 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:09.092602 22948 agents.py:69] training 406 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:09.263870 22948 agents.py:69] training 406 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:09.266601 22948 agents.py:69] training 406 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:09.536215 22948 agents.py:69] training 406 of 2000: completed tf_agent.train(...) = 5.670 [loss]\n",
      "I0624 23:44:09.538211 22948 agents.py:69] training 406 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:09.541215 22948 agents.py:69] training 407 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:09.783256 22948 agents.py:69] training 407 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:09.788226 22948 agents.py:69] training 407 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:10.120829 22948 agents.py:69] training 407 of 2000: completed tf_agent.train(...) = 2.145 [loss]\n",
      "I0624 23:44:10.122825 22948 agents.py:69] training 407 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:10.125823 22948 agents.py:69] training 408 of 2000: executing collect_driver.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:44:10.326574 22948 agents.py:69] training 408 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:10.331614 22948 agents.py:69] training 408 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:12.685308 22948 agents.py:69] training 408 of 2000: completed tf_agent.train(...) = 2.560 [loss]\n",
      "I0624 23:44:12.687304 22948 agents.py:69] training 408 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:12.689728 22948 agents.py:69] training 409 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:12.867892 22948 agents.py:69] training 409 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:12.871886 22948 agents.py:69] training 409 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:13.130556 22948 agents.py:69] training 409 of 2000: completed tf_agent.train(...) = 1.150 [loss]\n",
      "I0624 23:44:13.132556 22948 agents.py:69] training 409 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:13.135560 22948 agents.py:69] training 410 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:13.317770 22948 agents.py:69] training 410 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:13.321855 22948 agents.py:69] training 410 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:15.710313 22948 agents.py:69] training 410 of 2000: completed tf_agent.train(...) = 0.953 [loss]\n",
      "I0624 23:44:15.712311 22948 agents.py:69] training 410 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:15.715310 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:44:18.361896 22948 agents.py:69] completed compute_avg_return(...) = 0.278\n",
      "I0624 23:44:18.362885 22948 agents.py:69] training 411 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:18.546898 22948 agents.py:69] training 411 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:18.551891 22948 agents.py:69] training 411 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:20.993322 22948 agents.py:69] training 411 of 2000: completed tf_agent.train(...) = 1.354 [loss]\n",
      "I0624 23:44:20.996321 22948 agents.py:69] training 411 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:20.998374 22948 agents.py:69] training 412 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:21.251323 22948 agents.py:69] training 412 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:21.256331 22948 agents.py:69] training 412 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:21.565893 22948 agents.py:69] training 412 of 2000: completed tf_agent.train(...) = 2.999 [loss]\n",
      "I0624 23:44:21.566899 22948 agents.py:69] training 412 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:21.569890 22948 agents.py:69] training 413 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:21.816946 22948 agents.py:69] training 413 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:21.822891 22948 agents.py:69] training 413 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:22.134773 22948 agents.py:69] training 413 of 2000: completed tf_agent.train(...) = 2.122 [loss]\n",
      "I0624 23:44:22.136770 22948 agents.py:69] training 413 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:22.139774 22948 agents.py:69] training 414 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:22.533410 22948 agents.py:69] training 414 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:22.539121 22948 agents.py:69] training 414 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:22.836969 22948 agents.py:69] training 414 of 2000: completed tf_agent.train(...) = 1.698 [loss]\n",
      "I0624 23:44:22.839073 22948 agents.py:69] training 414 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:22.840967 22948 agents.py:69] training 415 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:23.109260 22948 agents.py:69] training 415 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:23.116244 22948 agents.py:69] training 415 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:23.617840 22948 agents.py:69] training 415 of 2000: completed tf_agent.train(...) = 4.306 [loss]\n",
      "I0624 23:44:23.621837 22948 agents.py:69] training 415 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:23.625838 22948 agents.py:69] training 416 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:23.877337 22948 agents.py:69] training 416 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:23.882342 22948 agents.py:69] training 416 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:24.200264 22948 agents.py:69] training 416 of 2000: completed tf_agent.train(...) = 2.772 [loss]\n",
      "I0624 23:44:24.203263 22948 agents.py:69] training 416 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:24.206266 22948 agents.py:69] training 417 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:24.411914 22948 agents.py:69] training 417 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:24.417866 22948 agents.py:69] training 417 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:24.699869 22948 agents.py:69] training 417 of 2000: completed tf_agent.train(...) = 2.155 [loss]\n",
      "I0624 23:44:24.701868 22948 agents.py:69] training 417 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:24.704861 22948 agents.py:69] training 418 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:24.914445 22948 agents.py:69] training 418 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:24.918446 22948 agents.py:69] training 418 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:25.220446 22948 agents.py:69] training 418 of 2000: completed tf_agent.train(...) = 2.714 [loss]\n",
      "I0624 23:44:25.222443 22948 agents.py:69] training 418 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:25.225438 22948 agents.py:69] training 419 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:25.418019 22948 agents.py:69] training 419 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:25.423018 22948 agents.py:69] training 419 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:25.742628 22948 agents.py:69] training 419 of 2000: completed tf_agent.train(...) = 1.818 [loss]\n",
      "I0624 23:44:25.744279 22948 agents.py:69] training 419 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:25.747299 22948 agents.py:69] training 420 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:25.959840 22948 agents.py:69] training 420 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:25.963029 22948 agents.py:69] training 420 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:26.213840 22948 agents.py:69] training 420 of 2000: completed tf_agent.train(...) = 1.112 [loss]\n",
      "I0624 23:44:26.214839 22948 agents.py:69] training 420 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:26.215842 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:44:27.345871 22948 agents.py:69] completed compute_avg_return(...) = 0.584\n",
      "I0624 23:44:27.347821 22948 agents.py:69] training 421 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:27.590942 22948 agents.py:69] training 421 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:27.594981 22948 agents.py:69] training 421 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:27.900366 22948 agents.py:69] training 421 of 2000: completed tf_agent.train(...) = 1.550 [loss]\n",
      "I0624 23:44:27.902362 22948 agents.py:69] training 421 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:27.905577 22948 agents.py:69] training 422 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:28.107776 22948 agents.py:69] training 422 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:28.110791 22948 agents.py:69] training 422 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:28.391489 22948 agents.py:69] training 422 of 2000: completed tf_agent.train(...) = 1.662 [loss]\n",
      "I0624 23:44:28.393485 22948 agents.py:69] training 422 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:28.396490 22948 agents.py:69] training 423 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:28.667536 22948 agents.py:69] training 423 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:28.672530 22948 agents.py:69] training 423 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:28.984018 22948 agents.py:69] training 423 of 2000: completed tf_agent.train(...) = 2.277 [loss]\n",
      "I0624 23:44:28.986013 22948 agents.py:69] training 423 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:28.989019 22948 agents.py:69] training 424 of 2000: executing collect_driver.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:44:29.224010 22948 agents.py:69] training 424 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:29.227012 22948 agents.py:69] training 424 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:31.625929 22948 agents.py:69] training 424 of 2000: completed tf_agent.train(...) = 2.589 [loss]\n",
      "I0624 23:44:31.628927 22948 agents.py:69] training 424 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:31.630930 22948 agents.py:69] training 425 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:31.877498 22948 agents.py:69] training 425 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:31.884729 22948 agents.py:69] training 425 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:32.188371 22948 agents.py:69] training 425 of 2000: completed tf_agent.train(...) = 4.853 [loss]\n",
      "I0624 23:44:32.190765 22948 agents.py:69] training 425 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:32.192768 22948 agents.py:69] training 426 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:32.372092 22948 agents.py:69] training 426 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:32.377015 22948 agents.py:69] training 426 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:32.663010 22948 agents.py:69] training 426 of 2000: completed tf_agent.train(...) = 1.522 [loss]\n",
      "I0624 23:44:32.664005 22948 agents.py:69] training 426 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:32.668015 22948 agents.py:69] training 427 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:32.944473 22948 agents.py:69] training 427 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:32.948509 22948 agents.py:69] training 427 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:33.251457 22948 agents.py:69] training 427 of 2000: completed tf_agent.train(...) = 5.175 [loss]\n",
      "I0624 23:44:33.254451 22948 agents.py:69] training 427 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:33.256452 22948 agents.py:69] training 428 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:33.519398 22948 agents.py:69] training 428 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:33.523002 22948 agents.py:69] training 428 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:33.787155 22948 agents.py:69] training 428 of 2000: completed tf_agent.train(...) = 3.219 [loss]\n",
      "I0624 23:44:33.787995 22948 agents.py:69] training 428 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:33.789994 22948 agents.py:69] training 429 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:34.016509 22948 agents.py:69] training 429 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:34.019504 22948 agents.py:69] training 429 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:34.293478 22948 agents.py:69] training 429 of 2000: completed tf_agent.train(...) = 2.949 [loss]\n",
      "I0624 23:44:34.295615 22948 agents.py:69] training 429 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:34.297468 22948 agents.py:69] training 430 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:34.478815 22948 agents.py:69] training 430 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:34.484818 22948 agents.py:69] training 430 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:34.782823 22948 agents.py:69] training 430 of 2000: completed tf_agent.train(...) = 1.421 [loss]\n",
      "I0624 23:44:34.784768 22948 agents.py:69] training 430 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:34.785767 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:44:37.716563 22948 agents.py:69] completed compute_avg_return(...) = 0.239\n",
      "I0624 23:44:37.718155 22948 agents.py:69] training 431 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:37.939778 22948 agents.py:69] training 431 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:37.943781 22948 agents.py:69] training 431 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:38.222014 22948 agents.py:69] training 431 of 2000: completed tf_agent.train(...) = 1.891 [loss]\n",
      "I0624 23:44:38.224016 22948 agents.py:69] training 431 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:38.227019 22948 agents.py:69] training 432 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:38.438450 22948 agents.py:69] training 432 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:38.440455 22948 agents.py:69] training 432 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:38.701485 22948 agents.py:69] training 432 of 2000: completed tf_agent.train(...) = 2.233 [loss]\n",
      "I0624 23:44:38.703489 22948 agents.py:69] training 432 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:38.705487 22948 agents.py:69] training 433 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:38.893529 22948 agents.py:69] training 433 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:38.896761 22948 agents.py:69] training 433 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:41.615165 22948 agents.py:69] training 433 of 2000: completed tf_agent.train(...) = 1.245 [loss]\n",
      "I0624 23:44:41.616590 22948 agents.py:69] training 433 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:41.618596 22948 agents.py:69] training 434 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:41.824604 22948 agents.py:69] training 434 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:41.829595 22948 agents.py:69] training 434 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:42.139172 22948 agents.py:69] training 434 of 2000: completed tf_agent.train(...) = 1.231 [loss]\n",
      "I0624 23:44:42.142162 22948 agents.py:69] training 434 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:42.144166 22948 agents.py:69] training 435 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:42.340166 22948 agents.py:69] training 435 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:42.343170 22948 agents.py:69] training 435 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:42.625395 22948 agents.py:69] training 435 of 2000: completed tf_agent.train(...) = 1.935 [loss]\n",
      "I0624 23:44:42.628393 22948 agents.py:69] training 435 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:42.630389 22948 agents.py:69] training 436 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:42.798395 22948 agents.py:69] training 436 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:42.801498 22948 agents.py:69] training 436 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:45.138851 22948 agents.py:69] training 436 of 2000: completed tf_agent.train(...) = 0.809 [loss]\n",
      "I0624 23:44:45.139770 22948 agents.py:69] training 436 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:45.141773 22948 agents.py:69] training 437 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:45.346905 22948 agents.py:69] training 437 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:45.350771 22948 agents.py:69] training 437 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:45.641849 22948 agents.py:69] training 437 of 2000: completed tf_agent.train(...) = 0.962 [loss]\n",
      "I0624 23:44:45.643659 22948 agents.py:69] training 437 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:45.645688 22948 agents.py:69] training 438 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:45.842060 22948 agents.py:69] training 438 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:45.848062 22948 agents.py:69] training 438 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:46.145445 22948 agents.py:69] training 438 of 2000: completed tf_agent.train(...) = 1.072 [loss]\n",
      "I0624 23:44:46.147444 22948 agents.py:69] training 438 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:46.150440 22948 agents.py:69] training 439 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:46.374495 22948 agents.py:69] training 439 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:46.379486 22948 agents.py:69] training 439 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:46.712448 22948 agents.py:69] training 439 of 2000: completed tf_agent.train(...) = 0.971 [loss]\n",
      "I0624 23:44:46.715442 22948 agents.py:69] training 439 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:46.717447 22948 agents.py:69] training 440 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:46.962893 22948 agents.py:69] training 440 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:46.967884 22948 agents.py:69] training 440 of 2000: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:44:47.271882 22948 agents.py:69] training 440 of 2000: completed tf_agent.train(...) = 1.216 [loss]\n",
      "I0624 23:44:47.273884 22948 agents.py:69] training 440 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:47.276881 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:44:51.727025 22948 agents.py:69] completed compute_avg_return(...) = 0.149\n",
      "I0624 23:44:51.728025 22948 agents.py:69] training 441 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:52.001717 22948 agents.py:69] training 441 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:52.006676 22948 agents.py:69] training 441 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:52.335680 22948 agents.py:69] training 441 of 2000: completed tf_agent.train(...) = 2.852 [loss]\n",
      "I0624 23:44:52.337671 22948 agents.py:69] training 441 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:52.340758 22948 agents.py:69] training 442 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:52.552672 22948 agents.py:69] training 442 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:52.556686 22948 agents.py:69] training 442 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:52.876370 22948 agents.py:69] training 442 of 2000: completed tf_agent.train(...) = 1.708 [loss]\n",
      "I0624 23:44:52.876971 22948 agents.py:69] training 442 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:52.879984 22948 agents.py:69] training 443 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:53.109028 22948 agents.py:69] training 443 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:53.114109 22948 agents.py:69] training 443 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:53.442871 22948 agents.py:69] training 443 of 2000: completed tf_agent.train(...) = 1.191 [loss]\n",
      "I0624 23:44:53.444868 22948 agents.py:69] training 443 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:53.446870 22948 agents.py:69] training 444 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:53.682875 22948 agents.py:69] training 444 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:53.685869 22948 agents.py:69] training 444 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:53.980870 22948 agents.py:69] training 444 of 2000: completed tf_agent.train(...) = 1.865 [loss]\n",
      "I0624 23:44:53.982868 22948 agents.py:69] training 444 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:53.984870 22948 agents.py:69] training 445 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:54.244882 22948 agents.py:69] training 445 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:54.248869 22948 agents.py:69] training 445 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:54.565481 22948 agents.py:69] training 445 of 2000: completed tf_agent.train(...) = 2.362 [loss]\n",
      "I0624 23:44:54.567485 22948 agents.py:69] training 445 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:54.570487 22948 agents.py:69] training 446 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:54.751486 22948 agents.py:69] training 446 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:54.754484 22948 agents.py:69] training 446 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:57.276361 22948 agents.py:69] training 446 of 2000: completed tf_agent.train(...) = 1.202 [loss]\n",
      "I0624 23:44:57.278362 22948 agents.py:69] training 446 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:57.281364 22948 agents.py:69] training 447 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:57.507950 22948 agents.py:69] training 447 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:57.513951 22948 agents.py:69] training 447 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:57.823519 22948 agents.py:69] training 447 of 2000: completed tf_agent.train(...) = 1.123 [loss]\n",
      "I0624 23:44:57.825522 22948 agents.py:69] training 447 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:57.828522 22948 agents.py:69] training 448 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:58.035104 22948 agents.py:69] training 448 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:58.040095 22948 agents.py:69] training 448 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:58.319097 22948 agents.py:69] training 448 of 2000: completed tf_agent.train(...) = 3.231 [loss]\n",
      "I0624 23:44:58.322097 22948 agents.py:69] training 448 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:58.325104 22948 agents.py:69] training 449 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:58.503679 22948 agents.py:69] training 449 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:58.509640 22948 agents.py:69] training 449 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:58.813638 22948 agents.py:69] training 449 of 2000: completed tf_agent.train(...) = 1.261 [loss]\n",
      "I0624 23:44:58.815638 22948 agents.py:69] training 449 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:58.817780 22948 agents.py:69] training 450 of 2000: executing collect_driver.run()\n",
      "I0624 23:44:59.069034 22948 agents.py:69] training 450 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:44:59.072765 22948 agents.py:69] training 450 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:44:59.379028 22948 agents.py:69] training 450 of 2000: completed tf_agent.train(...) = 1.098 [loss]\n",
      "I0624 23:44:59.382030 22948 agents.py:69] training 450 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:44:59.384032 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:45:02.220732 22948 agents.py:69] completed compute_avg_return(...) = 0.475\n",
      "I0624 23:45:02.222739 22948 agents.py:69] training 451 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:02.417369 22948 agents.py:69] training 451 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:02.423369 22948 agents.py:69] training 451 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:02.749442 22948 agents.py:69] training 451 of 2000: completed tf_agent.train(...) = 1.735 [loss]\n",
      "I0624 23:45:02.751445 22948 agents.py:69] training 451 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:02.754442 22948 agents.py:69] training 452 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:02.983971 22948 agents.py:69] training 452 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:02.988963 22948 agents.py:69] training 452 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:03.291099 22948 agents.py:69] training 452 of 2000: completed tf_agent.train(...) = 1.601 [loss]\n",
      "I0624 23:45:03.294105 22948 agents.py:69] training 452 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:03.296099 22948 agents.py:69] training 453 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:03.510137 22948 agents.py:69] training 453 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:03.516102 22948 agents.py:69] training 453 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:06.182687 22948 agents.py:69] training 453 of 2000: completed tf_agent.train(...) = 0.903 [loss]\n",
      "I0624 23:45:06.184690 22948 agents.py:69] training 453 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:06.186694 22948 agents.py:69] training 454 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:06.390690 22948 agents.py:69] training 454 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:06.397692 22948 agents.py:69] training 454 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:06.790746 22948 agents.py:69] training 454 of 2000: completed tf_agent.train(...) = 0.794 [loss]\n",
      "I0624 23:45:06.791748 22948 agents.py:69] training 454 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:06.794749 22948 agents.py:69] training 455 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:07.022008 22948 agents.py:69] training 455 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:07.029092 22948 agents.py:69] training 455 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:07.347007 22948 agents.py:69] training 455 of 2000: completed tf_agent.train(...) = 1.078 [loss]\n",
      "I0624 23:45:07.349008 22948 agents.py:69] training 455 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:07.352008 22948 agents.py:69] training 456 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:07.819006 22948 agents.py:69] training 456 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:07.824009 22948 agents.py:69] training 456 of 2000: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:45:08.213008 22948 agents.py:69] training 456 of 2000: completed tf_agent.train(...) = 0.803 [loss]\n",
      "I0624 23:45:08.215010 22948 agents.py:69] training 456 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:08.218005 22948 agents.py:69] training 457 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:08.424006 22948 agents.py:69] training 457 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:08.428007 22948 agents.py:69] training 457 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:10.739247 22948 agents.py:69] training 457 of 2000: completed tf_agent.train(...) = 0.674 [loss]\n",
      "I0624 23:45:10.740246 22948 agents.py:69] training 457 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:10.742249 22948 agents.py:69] training 458 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:10.898286 22948 agents.py:69] training 458 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:10.902285 22948 agents.py:69] training 458 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:11.179288 22948 agents.py:69] training 458 of 2000: completed tf_agent.train(...) = 1.039 [loss]\n",
      "I0624 23:45:11.181295 22948 agents.py:69] training 458 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:11.183751 22948 agents.py:69] training 459 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:11.381290 22948 agents.py:69] training 459 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:11.385292 22948 agents.py:69] training 459 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:11.703904 22948 agents.py:69] training 459 of 2000: completed tf_agent.train(...) = 0.900 [loss]\n",
      "I0624 23:45:11.707137 22948 agents.py:69] training 459 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:11.708896 22948 agents.py:69] training 460 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:11.892888 22948 agents.py:69] training 460 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:11.896888 22948 agents.py:69] training 460 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:12.153893 22948 agents.py:69] training 460 of 2000: completed tf_agent.train(...) = 1.071 [loss]\n",
      "I0624 23:45:12.155894 22948 agents.py:69] training 460 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:12.156907 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:45:19.480573 22948 agents.py:69] completed compute_avg_return(...) = -0.574\n",
      "I0624 23:45:19.483736 22948 agents.py:69] training 461 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:19.842564 22948 agents.py:69] training 461 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:19.849008 22948 agents.py:69] training 461 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:20.156192 22948 agents.py:69] training 461 of 2000: completed tf_agent.train(...) = 1.335 [loss]\n",
      "I0624 23:45:20.157076 22948 agents.py:69] training 461 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:20.159075 22948 agents.py:69] training 462 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:20.525160 22948 agents.py:69] training 462 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:20.530201 22948 agents.py:69] training 462 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:20.873250 22948 agents.py:69] training 462 of 2000: completed tf_agent.train(...) = 2.309 [loss]\n",
      "I0624 23:45:20.875316 22948 agents.py:69] training 462 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:20.879350 22948 agents.py:69] training 463 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:21.043157 22948 agents.py:69] training 463 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:21.046155 22948 agents.py:69] training 463 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:21.306545 22948 agents.py:69] training 463 of 2000: completed tf_agent.train(...) = 1.188 [loss]\n",
      "I0624 23:45:21.308268 22948 agents.py:69] training 463 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:21.310158 22948 agents.py:69] training 464 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:21.483821 22948 agents.py:69] training 464 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:21.486847 22948 agents.py:69] training 464 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:21.791715 22948 agents.py:69] training 464 of 2000: completed tf_agent.train(...) = 0.894 [loss]\n",
      "I0624 23:45:21.792847 22948 agents.py:69] training 464 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:21.794726 22948 agents.py:69] training 465 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:22.086641 22948 agents.py:69] training 465 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:22.096647 22948 agents.py:69] training 465 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:22.558875 22948 agents.py:69] training 465 of 2000: completed tf_agent.train(...) = 1.126 [loss]\n",
      "I0624 23:45:22.562872 22948 agents.py:69] training 465 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:22.565871 22948 agents.py:69] training 466 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:22.777897 22948 agents.py:69] training 466 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:22.781907 22948 agents.py:69] training 466 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:23.055195 22948 agents.py:69] training 466 of 2000: completed tf_agent.train(...) = 0.786 [loss]\n",
      "I0624 23:45:23.056203 22948 agents.py:69] training 466 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:23.058311 22948 agents.py:69] training 467 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:23.243204 22948 agents.py:69] training 467 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:23.247193 22948 agents.py:69] training 467 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:23.523185 22948 agents.py:69] training 467 of 2000: completed tf_agent.train(...) = 1.551 [loss]\n",
      "I0624 23:45:23.524186 22948 agents.py:69] training 467 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:23.526185 22948 agents.py:69] training 468 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:23.725234 22948 agents.py:69] training 468 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:23.730189 22948 agents.py:69] training 468 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:24.047887 22948 agents.py:69] training 468 of 2000: completed tf_agent.train(...) = 0.855 [loss]\n",
      "I0624 23:45:24.049887 22948 agents.py:69] training 468 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:24.052884 22948 agents.py:69] training 469 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:24.237890 22948 agents.py:69] training 469 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:24.240884 22948 agents.py:69] training 469 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:24.519888 22948 agents.py:69] training 469 of 2000: completed tf_agent.train(...) = 0.962 [loss]\n",
      "I0624 23:45:24.521886 22948 agents.py:69] training 469 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:24.523024 22948 agents.py:69] training 470 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:24.742893 22948 agents.py:69] training 470 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:24.747929 22948 agents.py:69] training 470 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:25.080316 22948 agents.py:69] training 470 of 2000: completed tf_agent.train(...) = 1.118 [loss]\n",
      "I0624 23:45:25.082323 22948 agents.py:69] training 470 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:25.085313 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:45:28.106462 22948 agents.py:69] completed compute_avg_return(...) = 0.310\n",
      "I0624 23:45:28.107501 22948 agents.py:69] training 471 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:28.329448 22948 agents.py:69] training 471 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:28.333491 22948 agents.py:69] training 471 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:28.640986 22948 agents.py:69] training 471 of 2000: completed tf_agent.train(...) = 1.300 [loss]\n",
      "I0624 23:45:28.642984 22948 agents.py:69] training 471 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:28.645994 22948 agents.py:69] training 472 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:28.850069 22948 agents.py:69] training 472 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:28.857188 22948 agents.py:69] training 472 of 2000: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:45:29.241983 22948 agents.py:69] training 472 of 2000: completed tf_agent.train(...) = 0.912 [loss]\n",
      "I0624 23:45:29.243979 22948 agents.py:69] training 472 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:29.246988 22948 agents.py:69] training 473 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:29.485617 22948 agents.py:69] training 473 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:29.488649 22948 agents.py:69] training 473 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:29.781621 22948 agents.py:69] training 473 of 2000: completed tf_agent.train(...) = 5.938 [loss]\n",
      "I0624 23:45:29.783621 22948 agents.py:69] training 473 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:29.785822 22948 agents.py:69] training 474 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:30.014400 22948 agents.py:69] training 474 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:30.021428 22948 agents.py:69] training 474 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:30.403336 22948 agents.py:69] training 474 of 2000: completed tf_agent.train(...) = 0.889 [loss]\n",
      "I0624 23:45:30.405331 22948 agents.py:69] training 474 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:30.408341 22948 agents.py:69] training 475 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:30.623540 22948 agents.py:69] training 475 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:30.627479 22948 agents.py:69] training 475 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:30.859580 22948 agents.py:69] training 475 of 2000: completed tf_agent.train(...) = 1.013 [loss]\n",
      "I0624 23:45:30.862591 22948 agents.py:69] training 475 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:30.864585 22948 agents.py:69] training 476 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:31.036784 22948 agents.py:69] training 476 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:31.041717 22948 agents.py:69] training 476 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:31.334640 22948 agents.py:69] training 476 of 2000: completed tf_agent.train(...) = 0.932 [loss]\n",
      "I0624 23:45:31.336637 22948 agents.py:69] training 476 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:31.339642 22948 agents.py:69] training 477 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:31.539411 22948 agents.py:69] training 477 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:31.543365 22948 agents.py:69] training 477 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:31.819753 22948 agents.py:69] training 477 of 2000: completed tf_agent.train(...) = 1.474 [loss]\n",
      "I0624 23:45:31.821749 22948 agents.py:69] training 477 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:31.823749 22948 agents.py:69] training 478 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:31.998850 22948 agents.py:69] training 478 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:32.003814 22948 agents.py:69] training 478 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:32.276801 22948 agents.py:69] training 478 of 2000: completed tf_agent.train(...) = 1.220 [loss]\n",
      "I0624 23:45:32.278687 22948 agents.py:69] training 478 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:32.279803 22948 agents.py:69] training 479 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:32.452473 22948 agents.py:69] training 479 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:32.461488 22948 agents.py:69] training 479 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:32.945481 22948 agents.py:69] training 479 of 2000: completed tf_agent.train(...) = 0.754 [loss]\n",
      "I0624 23:45:32.950486 22948 agents.py:69] training 479 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:32.955481 22948 agents.py:69] training 480 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:33.261655 22948 agents.py:69] training 480 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:33.264654 22948 agents.py:69] training 480 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:33.571063 22948 agents.py:69] training 480 of 2000: completed tf_agent.train(...) = 0.819 [loss]\n",
      "I0624 23:45:33.573067 22948 agents.py:69] training 480 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:33.575225 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:45:36.750840 22948 agents.py:69] completed compute_avg_return(...) = 0.453\n",
      "I0624 23:45:36.752839 22948 agents.py:69] training 481 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:37.008280 22948 agents.py:69] training 481 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:37.011834 22948 agents.py:69] training 481 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:37.285873 22948 agents.py:69] training 481 of 2000: completed tf_agent.train(...) = 2.931 [loss]\n",
      "I0624 23:45:37.287880 22948 agents.py:69] training 481 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:37.290886 22948 agents.py:69] training 482 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:37.572695 22948 agents.py:69] training 482 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:37.577658 22948 agents.py:69] training 482 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:37.888645 22948 agents.py:69] training 482 of 2000: completed tf_agent.train(...) = 8.507 [loss]\n",
      "I0624 23:45:37.890652 22948 agents.py:69] training 482 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:37.891648 22948 agents.py:69] training 483 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:38.106607 22948 agents.py:69] training 483 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:38.111614 22948 agents.py:69] training 483 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:38.384611 22948 agents.py:69] training 483 of 2000: completed tf_agent.train(...) = 1.814 [loss]\n",
      "I0624 23:45:38.386614 22948 agents.py:69] training 483 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:38.389608 22948 agents.py:69] training 484 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:38.610661 22948 agents.py:69] training 484 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:38.615621 22948 agents.py:69] training 484 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:38.953717 22948 agents.py:69] training 484 of 2000: completed tf_agent.train(...) = 1.801 [loss]\n",
      "I0624 23:45:38.955690 22948 agents.py:69] training 484 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:38.957716 22948 agents.py:69] training 485 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:39.186193 22948 agents.py:69] training 485 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:39.191162 22948 agents.py:69] training 485 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:39.455145 22948 agents.py:69] training 485 of 2000: completed tf_agent.train(...) = 2.716 [loss]\n",
      "I0624 23:45:39.456150 22948 agents.py:69] training 485 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:39.458147 22948 agents.py:69] training 486 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:39.645768 22948 agents.py:69] training 486 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:39.651806 22948 agents.py:69] training 486 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:39.980766 22948 agents.py:69] training 486 of 2000: completed tf_agent.train(...) = 1.764 [loss]\n",
      "I0624 23:45:39.982760 22948 agents.py:69] training 486 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:39.985765 22948 agents.py:69] training 487 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:40.220459 22948 agents.py:69] training 487 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:40.224464 22948 agents.py:69] training 487 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:40.528453 22948 agents.py:69] training 487 of 2000: completed tf_agent.train(...) = 4.407 [loss]\n",
      "I0624 23:45:40.530448 22948 agents.py:69] training 487 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:40.532455 22948 agents.py:69] training 488 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:40.724457 22948 agents.py:69] training 488 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:40.729464 22948 agents.py:69] training 488 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:41.003449 22948 agents.py:69] training 488 of 2000: completed tf_agent.train(...) = 1.512 [loss]\n",
      "I0624 23:45:41.006471 22948 agents.py:69] training 488 of 2000: executing replay_buffer.clear()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:45:41.009072 22948 agents.py:69] training 489 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:41.205583 22948 agents.py:69] training 489 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:41.207625 22948 agents.py:69] training 489 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:41.478537 22948 agents.py:69] training 489 of 2000: completed tf_agent.train(...) = 1.291 [loss]\n",
      "I0624 23:45:41.480532 22948 agents.py:69] training 489 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:41.483539 22948 agents.py:69] training 490 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:41.673019 22948 agents.py:69] training 490 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:41.679032 22948 agents.py:69] training 490 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:42.001021 22948 agents.py:69] training 490 of 2000: completed tf_agent.train(...) = 2.785 [loss]\n",
      "I0624 23:45:42.002509 22948 agents.py:69] training 490 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:42.005499 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:45:44.608890 22948 agents.py:69] completed compute_avg_return(...) = 0.434\n",
      "I0624 23:45:44.610855 22948 agents.py:69] training 491 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:44.812858 22948 agents.py:69] training 491 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:44.817860 22948 agents.py:69] training 491 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:45.129379 22948 agents.py:69] training 491 of 2000: completed tf_agent.train(...) = 1.741 [loss]\n",
      "I0624 23:45:45.131372 22948 agents.py:69] training 491 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:45.133371 22948 agents.py:69] training 492 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:45.300371 22948 agents.py:69] training 492 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:45.304371 22948 agents.py:69] training 492 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:47.883251 22948 agents.py:69] training 492 of 2000: completed tf_agent.train(...) = 0.673 [loss]\n",
      "I0624 23:45:47.885249 22948 agents.py:69] training 492 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:47.888257 22948 agents.py:69] training 493 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:48.068700 22948 agents.py:69] training 493 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:48.072652 22948 agents.py:69] training 493 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:50.639009 22948 agents.py:69] training 493 of 2000: completed tf_agent.train(...) = 0.733 [loss]\n",
      "I0624 23:45:50.641018 22948 agents.py:69] training 493 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:50.645015 22948 agents.py:69] training 494 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:50.872177 22948 agents.py:69] training 494 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:50.874568 22948 agents.py:69] training 494 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:51.150248 22948 agents.py:69] training 494 of 2000: completed tf_agent.train(...) = 7.747 [loss]\n",
      "I0624 23:45:51.151422 22948 agents.py:69] training 494 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:51.153251 22948 agents.py:69] training 495 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:51.347293 22948 agents.py:69] training 495 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:51.352528 22948 agents.py:69] training 495 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:51.621754 22948 agents.py:69] training 495 of 2000: completed tf_agent.train(...) = 0.933 [loss]\n",
      "I0624 23:45:51.622614 22948 agents.py:69] training 495 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:51.624682 22948 agents.py:69] training 496 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:51.785618 22948 agents.py:69] training 496 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:51.789798 22948 agents.py:69] training 496 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:54.150591 22948 agents.py:69] training 496 of 2000: completed tf_agent.train(...) = 0.853 [loss]\n",
      "I0624 23:45:54.152582 22948 agents.py:69] training 496 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:54.155580 22948 agents.py:69] training 497 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:54.327594 22948 agents.py:69] training 497 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:54.332625 22948 agents.py:69] training 497 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:54.684732 22948 agents.py:69] training 497 of 2000: completed tf_agent.train(...) = 0.899 [loss]\n",
      "I0624 23:45:54.687759 22948 agents.py:69] training 497 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:54.689739 22948 agents.py:69] training 498 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:54.869762 22948 agents.py:69] training 498 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:54.873744 22948 agents.py:69] training 498 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:55.186899 22948 agents.py:69] training 498 of 2000: completed tf_agent.train(...) = 1.211 [loss]\n",
      "I0624 23:45:55.189903 22948 agents.py:69] training 498 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:55.192034 22948 agents.py:69] training 499 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:55.388962 22948 agents.py:69] training 499 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:55.395923 22948 agents.py:69] training 499 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:55.754503 22948 agents.py:69] training 499 of 2000: completed tf_agent.train(...) = 1.708 [loss]\n",
      "I0624 23:45:55.756502 22948 agents.py:69] training 499 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:55.759500 22948 agents.py:69] training 500 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:55.956552 22948 agents.py:69] training 500 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:55.961877 22948 agents.py:69] training 500 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:56.275160 22948 agents.py:69] training 500 of 2000: completed tf_agent.train(...) = 2.027 [loss]\n",
      "I0624 23:45:56.276157 22948 agents.py:69] training 500 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:56.279279 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:45:58.961764 22948 agents.py:69] completed compute_avg_return(...) = 0.494\n",
      "I0624 23:45:58.963752 22948 agents.py:69] training 501 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:59.145986 22948 agents.py:69] training 501 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:59.154974 22948 agents.py:69] training 501 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:59.495570 22948 agents.py:69] training 501 of 2000: completed tf_agent.train(...) = 0.742 [loss]\n",
      "I0624 23:45:59.497564 22948 agents.py:69] training 501 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:45:59.499851 22948 agents.py:69] training 502 of 2000: executing collect_driver.run()\n",
      "I0624 23:45:59.697980 22948 agents.py:69] training 502 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:45:59.703065 22948 agents.py:69] training 502 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:45:59.999933 22948 agents.py:69] training 502 of 2000: completed tf_agent.train(...) = 0.846 [loss]\n",
      "I0624 23:46:00.001929 22948 agents.py:69] training 502 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:00.005259 22948 agents.py:69] training 503 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:00.221396 22948 agents.py:69] training 503 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:00.225502 22948 agents.py:69] training 503 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:00.520397 22948 agents.py:69] training 503 of 2000: completed tf_agent.train(...) = 8.393 [loss]\n",
      "I0624 23:46:00.522403 22948 agents.py:69] training 503 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:00.525398 22948 agents.py:69] training 504 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:00.764447 22948 agents.py:69] training 504 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:00.769405 22948 agents.py:69] training 504 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:01.102223 22948 agents.py:69] training 504 of 2000: completed tf_agent.train(...) = 1.408 [loss]\n",
      "I0624 23:46:01.105221 22948 agents.py:69] training 504 of 2000: executing replay_buffer.clear()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:46:01.107223 22948 agents.py:69] training 505 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:01.344309 22948 agents.py:69] training 505 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:01.350323 22948 agents.py:69] training 505 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:01.661800 22948 agents.py:69] training 505 of 2000: completed tf_agent.train(...) = 2.793 [loss]\n",
      "I0624 23:46:01.663793 22948 agents.py:69] training 505 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:01.666815 22948 agents.py:69] training 506 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:01.859800 22948 agents.py:69] training 506 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:01.864223 22948 agents.py:69] training 506 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:02.158431 22948 agents.py:69] training 506 of 2000: completed tf_agent.train(...) = 1.458 [loss]\n",
      "I0624 23:46:02.160440 22948 agents.py:69] training 506 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:02.163644 22948 agents.py:69] training 507 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:02.337435 22948 agents.py:69] training 507 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:02.342430 22948 agents.py:69] training 507 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:02.637016 22948 agents.py:69] training 507 of 2000: completed tf_agent.train(...) = 1.212 [loss]\n",
      "I0624 23:46:02.640014 22948 agents.py:69] training 507 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:02.642009 22948 agents.py:69] training 508 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:02.827234 22948 agents.py:69] training 508 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:02.830235 22948 agents.py:69] training 508 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:03.121690 22948 agents.py:69] training 508 of 2000: completed tf_agent.train(...) = 2.327 [loss]\n",
      "I0624 23:46:03.123685 22948 agents.py:69] training 508 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:03.126686 22948 agents.py:69] training 509 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:03.347675 22948 agents.py:69] training 509 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:03.353628 22948 agents.py:69] training 509 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:03.679614 22948 agents.py:69] training 509 of 2000: completed tf_agent.train(...) = 0.856 [loss]\n",
      "I0624 23:46:03.682619 22948 agents.py:69] training 509 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:03.684616 22948 agents.py:69] training 510 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:03.899625 22948 agents.py:69] training 510 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:03.908679 22948 agents.py:69] training 510 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:04.268842 22948 agents.py:69] training 510 of 2000: completed tf_agent.train(...) = 0.787 [loss]\n",
      "I0624 23:46:04.270844 22948 agents.py:69] training 510 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:04.272845 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:46:13.058489 22948 agents.py:69] completed compute_avg_return(...) = -0.764\n",
      "I0624 23:46:13.060496 22948 agents.py:69] training 511 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:13.248442 22948 agents.py:69] training 511 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:13.250436 22948 agents.py:69] training 511 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:13.506438 22948 agents.py:69] training 511 of 2000: completed tf_agent.train(...) = 3.270 [loss]\n",
      "I0624 23:46:13.508530 22948 agents.py:69] training 511 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:13.510444 22948 agents.py:69] training 512 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:13.671563 22948 agents.py:69] training 512 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:13.674604 22948 agents.py:69] training 512 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:13.928695 22948 agents.py:69] training 512 of 2000: completed tf_agent.train(...) = 1.032 [loss]\n",
      "I0624 23:46:13.930696 22948 agents.py:69] training 512 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:13.932751 22948 agents.py:69] training 513 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:14.135332 22948 agents.py:69] training 513 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:14.139331 22948 agents.py:69] training 513 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:14.418717 22948 agents.py:69] training 513 of 2000: completed tf_agent.train(...) = 4.101 [loss]\n",
      "I0624 23:46:14.419570 22948 agents.py:69] training 513 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:14.421646 22948 agents.py:69] training 514 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:14.601150 22948 agents.py:69] training 514 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:14.612141 22948 agents.py:69] training 514 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:14.997170 22948 agents.py:69] training 514 of 2000: completed tf_agent.train(...) = 1.014 [loss]\n",
      "I0624 23:46:14.999170 22948 agents.py:69] training 514 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:15.002171 22948 agents.py:69] training 515 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:15.196659 22948 agents.py:69] training 515 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:15.199664 22948 agents.py:69] training 515 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:15.504522 22948 agents.py:69] training 515 of 2000: completed tf_agent.train(...) = 1.090 [loss]\n",
      "I0624 23:46:15.507517 22948 agents.py:69] training 515 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:15.509550 22948 agents.py:69] training 516 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:15.726984 22948 agents.py:69] training 516 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:15.730018 22948 agents.py:69] training 516 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:16.024980 22948 agents.py:69] training 516 of 2000: completed tf_agent.train(...) = 3.190 [loss]\n",
      "I0624 23:46:16.026981 22948 agents.py:69] training 516 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:16.029981 22948 agents.py:69] training 517 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:16.206026 22948 agents.py:69] training 517 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:16.210200 22948 agents.py:69] training 517 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:16.490452 22948 agents.py:69] training 517 of 2000: completed tf_agent.train(...) = 1.017 [loss]\n",
      "I0624 23:46:16.493452 22948 agents.py:69] training 517 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:16.496450 22948 agents.py:69] training 518 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:16.684513 22948 agents.py:69] training 518 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:16.688476 22948 agents.py:69] training 518 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:16.957468 22948 agents.py:69] training 518 of 2000: completed tf_agent.train(...) = 1.206 [loss]\n",
      "I0624 23:46:16.959474 22948 agents.py:69] training 518 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:16.962468 22948 agents.py:69] training 519 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:17.188007 22948 agents.py:69] training 519 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:17.191047 22948 agents.py:69] training 519 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:17.467198 22948 agents.py:69] training 519 of 2000: completed tf_agent.train(...) = 24.956 [loss]\n",
      "I0624 23:46:17.468198 22948 agents.py:69] training 519 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:17.470299 22948 agents.py:69] training 520 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:17.676741 22948 agents.py:69] training 520 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:17.681740 22948 agents.py:69] training 520 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:17.986951 22948 agents.py:69] training 520 of 2000: completed tf_agent.train(...) = 2.102 [loss]\n",
      "I0624 23:46:17.988270 22948 agents.py:69] training 520 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:17.990020 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:46:23.678045 22948 agents.py:69] completed compute_avg_return(...) = -0.282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:46:23.680043 22948 agents.py:69] training 521 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:23.859052 22948 agents.py:69] training 521 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:23.864042 22948 agents.py:69] training 521 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:24.148359 22948 agents.py:69] training 521 of 2000: completed tf_agent.train(...) = 1.943 [loss]\n",
      "I0624 23:46:24.150359 22948 agents.py:69] training 521 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:24.152690 22948 agents.py:69] training 522 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:24.323413 22948 agents.py:69] training 522 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:24.326410 22948 agents.py:69] training 522 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:24.602094 22948 agents.py:69] training 522 of 2000: completed tf_agent.train(...) = 1.301 [loss]\n",
      "I0624 23:46:24.603094 22948 agents.py:69] training 522 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:24.606109 22948 agents.py:69] training 523 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:24.780487 22948 agents.py:69] training 523 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:24.785471 22948 agents.py:69] training 523 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:25.157486 22948 agents.py:69] training 523 of 2000: completed tf_agent.train(...) = 1.900 [loss]\n",
      "I0624 23:46:25.159519 22948 agents.py:69] training 523 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:25.161489 22948 agents.py:69] training 524 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:25.329486 22948 agents.py:69] training 524 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:25.332485 22948 agents.py:69] training 524 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:25.634407 22948 agents.py:69] training 524 of 2000: completed tf_agent.train(...) = 1.420 [loss]\n",
      "I0624 23:46:25.636410 22948 agents.py:69] training 524 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:25.638405 22948 agents.py:69] training 525 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:25.836414 22948 agents.py:69] training 525 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:25.846462 22948 agents.py:69] training 525 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:26.205134 22948 agents.py:69] training 525 of 2000: completed tf_agent.train(...) = 0.824 [loss]\n",
      "I0624 23:46:26.207138 22948 agents.py:69] training 525 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:26.211138 22948 agents.py:69] training 526 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:26.438797 22948 agents.py:69] training 526 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:26.441820 22948 agents.py:69] training 526 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:26.672873 22948 agents.py:69] training 526 of 2000: completed tf_agent.train(...) = 2.460 [loss]\n",
      "I0624 23:46:26.673875 22948 agents.py:69] training 526 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:26.674877 22948 agents.py:69] training 527 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:26.787231 22948 agents.py:69] training 527 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:26.789228 22948 agents.py:69] training 527 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:26.976176 22948 agents.py:69] training 527 of 2000: completed tf_agent.train(...) = 1.334 [loss]\n",
      "I0624 23:46:26.977264 22948 agents.py:69] training 527 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:26.978177 22948 agents.py:69] training 528 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:27.120020 22948 agents.py:69] training 528 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:27.123025 22948 agents.py:69] training 528 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:27.414140 22948 agents.py:69] training 528 of 2000: completed tf_agent.train(...) = 0.857 [loss]\n",
      "I0624 23:46:27.416138 22948 agents.py:69] training 528 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:27.419146 22948 agents.py:69] training 529 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:27.609755 22948 agents.py:69] training 529 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:27.613758 22948 agents.py:69] training 529 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:27.916337 22948 agents.py:69] training 529 of 2000: completed tf_agent.train(...) = 1.971 [loss]\n",
      "I0624 23:46:27.918336 22948 agents.py:69] training 529 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:27.921339 22948 agents.py:69] training 530 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:28.136830 22948 agents.py:69] training 530 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:28.142860 22948 agents.py:69] training 530 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:28.483824 22948 agents.py:69] training 530 of 2000: completed tf_agent.train(...) = 1.332 [loss]\n",
      "I0624 23:46:28.485824 22948 agents.py:69] training 530 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:28.488820 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:46:33.112298 22948 agents.py:69] completed compute_avg_return(...) = 0.280\n",
      "I0624 23:46:33.113298 22948 agents.py:69] training 531 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:33.330522 22948 agents.py:69] training 531 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:33.333107 22948 agents.py:69] training 531 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:33.553797 22948 agents.py:69] training 531 of 2000: completed tf_agent.train(...) = 1.278 [loss]\n",
      "I0624 23:46:33.555796 22948 agents.py:69] training 531 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:33.558797 22948 agents.py:69] training 532 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:33.741798 22948 agents.py:69] training 532 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:33.745461 22948 agents.py:69] training 532 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:34.040892 22948 agents.py:69] training 532 of 2000: completed tf_agent.train(...) = 0.975 [loss]\n",
      "I0624 23:46:34.043211 22948 agents.py:69] training 532 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:34.044888 22948 agents.py:69] training 533 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:34.232892 22948 agents.py:69] training 533 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:34.237891 22948 agents.py:69] training 533 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:34.531856 22948 agents.py:69] training 533 of 2000: completed tf_agent.train(...) = 1.552 [loss]\n",
      "I0624 23:46:34.533853 22948 agents.py:69] training 533 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:34.535923 22948 agents.py:69] training 534 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:34.728371 22948 agents.py:69] training 534 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:34.732370 22948 agents.py:69] training 534 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:35.030372 22948 agents.py:69] training 534 of 2000: completed tf_agent.train(...) = 1.507 [loss]\n",
      "I0624 23:46:35.032695 22948 agents.py:69] training 534 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:35.034368 22948 agents.py:69] training 535 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:35.262866 22948 agents.py:69] training 535 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:35.267856 22948 agents.py:69] training 535 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:35.606441 22948 agents.py:69] training 535 of 2000: completed tf_agent.train(...) = 0.787 [loss]\n",
      "I0624 23:46:35.608445 22948 agents.py:69] training 535 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:35.611448 22948 agents.py:69] training 536 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:35.787342 22948 agents.py:69] training 536 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:35.792348 22948 agents.py:69] training 536 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:36.071393 22948 agents.py:69] training 536 of 2000: completed tf_agent.train(...) = 0.924 [loss]\n",
      "I0624 23:46:36.074398 22948 agents.py:69] training 536 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:36.076396 22948 agents.py:69] training 537 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:36.364142 22948 agents.py:69] training 537 of 2000: executing replay_buffer.gather_all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:46:36.373093 22948 agents.py:69] training 537 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:36.704377 22948 agents.py:69] training 537 of 2000: completed tf_agent.train(...) = 2.121 [loss]\n",
      "I0624 23:46:36.706377 22948 agents.py:69] training 537 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:36.707377 22948 agents.py:69] training 538 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:36.882389 22948 agents.py:69] training 538 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:36.886383 22948 agents.py:69] training 538 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:37.259377 22948 agents.py:69] training 538 of 2000: completed tf_agent.train(...) = 1.248 [loss]\n",
      "I0624 23:46:37.260378 22948 agents.py:69] training 538 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:37.264377 22948 agents.py:69] training 539 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:37.455380 22948 agents.py:69] training 539 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:37.458913 22948 agents.py:69] training 539 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:37.762996 22948 agents.py:69] training 539 of 2000: completed tf_agent.train(...) = 2.328 [loss]\n",
      "I0624 23:46:37.765949 22948 agents.py:69] training 539 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:37.766990 22948 agents.py:69] training 540 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:37.941992 22948 agents.py:69] training 540 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:37.946176 22948 agents.py:69] training 540 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:38.242694 22948 agents.py:69] training 540 of 2000: completed tf_agent.train(...) = 1.014 [loss]\n",
      "I0624 23:46:38.243616 22948 agents.py:69] training 540 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:38.245777 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:46:42.955002 22948 agents.py:69] completed compute_avg_return(...) = -0.137\n",
      "I0624 23:46:42.955999 22948 agents.py:69] training 541 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:43.105411 22948 agents.py:69] training 541 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:43.108278 22948 agents.py:69] training 541 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:43.306576 22948 agents.py:69] training 541 of 2000: completed tf_agent.train(...) = 0.854 [loss]\n",
      "I0624 23:46:43.307575 22948 agents.py:69] training 541 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:43.309225 22948 agents.py:69] training 542 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:43.444477 22948 agents.py:69] training 542 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:43.447480 22948 agents.py:69] training 542 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:43.641328 22948 agents.py:69] training 542 of 2000: completed tf_agent.train(...) = 0.786 [loss]\n",
      "I0624 23:46:43.642383 22948 agents.py:69] training 542 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:43.644501 22948 agents.py:69] training 543 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:43.775327 22948 agents.py:69] training 543 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:43.778961 22948 agents.py:69] training 543 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:43.980567 22948 agents.py:69] training 543 of 2000: completed tf_agent.train(...) = 0.760 [loss]\n",
      "I0624 23:46:43.981577 22948 agents.py:69] training 543 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:43.982651 22948 agents.py:69] training 544 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:44.143582 22948 agents.py:69] training 544 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:44.147584 22948 agents.py:69] training 544 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:46.328532 22948 agents.py:69] training 544 of 2000: completed tf_agent.train(...) = 0.869 [loss]\n",
      "I0624 23:46:46.330526 22948 agents.py:69] training 544 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:46.332530 22948 agents.py:69] training 545 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:46.494658 22948 agents.py:69] training 545 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:46.499848 22948 agents.py:69] training 545 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:46.852259 22948 agents.py:69] training 545 of 2000: completed tf_agent.train(...) = 1.161 [loss]\n",
      "I0624 23:46:46.854661 22948 agents.py:69] training 545 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:46.857137 22948 agents.py:69] training 546 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:47.026290 22948 agents.py:69] training 546 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:47.031243 22948 agents.py:69] training 546 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:47.299643 22948 agents.py:69] training 546 of 2000: completed tf_agent.train(...) = 0.758 [loss]\n",
      "I0624 23:46:47.301648 22948 agents.py:69] training 546 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:47.304840 22948 agents.py:69] training 547 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:47.491923 22948 agents.py:69] training 547 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:47.495953 22948 agents.py:69] training 547 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:47.754558 22948 agents.py:69] training 547 of 2000: completed tf_agent.train(...) = 5.086 [loss]\n",
      "I0624 23:46:47.756563 22948 agents.py:69] training 547 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:47.759557 22948 agents.py:69] training 548 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:47.936528 22948 agents.py:69] training 548 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:47.939535 22948 agents.py:69] training 548 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:48.218556 22948 agents.py:69] training 548 of 2000: completed tf_agent.train(...) = 2.550 [loss]\n",
      "I0624 23:46:48.219556 22948 agents.py:69] training 548 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:48.221553 22948 agents.py:69] training 549 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:48.390834 22948 agents.py:69] training 549 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:48.397822 22948 agents.py:69] training 549 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:48.718029 22948 agents.py:69] training 549 of 2000: completed tf_agent.train(...) = 1.064 [loss]\n",
      "I0624 23:46:48.720396 22948 agents.py:69] training 549 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:48.722400 22948 agents.py:69] training 550 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:48.956139 22948 agents.py:69] training 550 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:48.964598 22948 agents.py:69] training 550 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:49.345790 22948 agents.py:69] training 550 of 2000: completed tf_agent.train(...) = 2.922 [loss]\n",
      "I0624 23:46:49.347789 22948 agents.py:69] training 550 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:49.350945 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:46:51.762295 22948 agents.py:69] completed compute_avg_return(...) = -0.373\n",
      "I0624 23:46:51.763127 22948 agents.py:69] training 551 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:51.921481 22948 agents.py:69] training 551 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:51.926446 22948 agents.py:69] training 551 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:52.186944 22948 agents.py:69] training 551 of 2000: completed tf_agent.train(...) = 0.868 [loss]\n",
      "I0624 23:46:52.188930 22948 agents.py:69] training 551 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:52.191930 22948 agents.py:69] training 552 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:52.360579 22948 agents.py:69] training 552 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:52.366576 22948 agents.py:69] training 552 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:52.585279 22948 agents.py:69] training 552 of 2000: completed tf_agent.train(...) = 2.395 [loss]\n",
      "I0624 23:46:52.586444 22948 agents.py:69] training 552 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:52.588282 22948 agents.py:69] training 553 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:52.777290 22948 agents.py:69] training 553 of 2000: executing replay_buffer.gather_all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:46:52.781285 22948 agents.py:69] training 553 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:53.020277 22948 agents.py:69] training 553 of 2000: completed tf_agent.train(...) = 0.899 [loss]\n",
      "I0624 23:46:53.022281 22948 agents.py:69] training 553 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:53.024277 22948 agents.py:69] training 554 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:53.175278 22948 agents.py:69] training 554 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:53.178279 22948 agents.py:69] training 554 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:53.440782 22948 agents.py:69] training 554 of 2000: completed tf_agent.train(...) = 0.875 [loss]\n",
      "I0624 23:46:53.441775 22948 agents.py:69] training 554 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:53.443780 22948 agents.py:69] training 555 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:53.605046 22948 agents.py:69] training 555 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:53.608051 22948 agents.py:69] training 555 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:53.878638 22948 agents.py:69] training 555 of 2000: completed tf_agent.train(...) = 1.508 [loss]\n",
      "I0624 23:46:53.880638 22948 agents.py:69] training 555 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:53.882640 22948 agents.py:69] training 556 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:54.030719 22948 agents.py:69] training 556 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:54.035884 22948 agents.py:69] training 556 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:54.407542 22948 agents.py:69] training 556 of 2000: completed tf_agent.train(...) = 0.711 [loss]\n",
      "I0624 23:46:54.409546 22948 agents.py:69] training 556 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:54.412548 22948 agents.py:69] training 557 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:54.653959 22948 agents.py:69] training 557 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:54.658957 22948 agents.py:69] training 557 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:54.932902 22948 agents.py:69] training 557 of 2000: completed tf_agent.train(...) = 2.104 [loss]\n",
      "I0624 23:46:54.934901 22948 agents.py:69] training 557 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:54.938014 22948 agents.py:69] training 558 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:55.090267 22948 agents.py:69] training 558 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:55.094343 22948 agents.py:69] training 558 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:55.384073 22948 agents.py:69] training 558 of 2000: completed tf_agent.train(...) = 1.126 [loss]\n",
      "I0624 23:46:55.386071 22948 agents.py:69] training 558 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:55.388717 22948 agents.py:69] training 559 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:55.590157 22948 agents.py:69] training 559 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:55.594156 22948 agents.py:69] training 559 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:55.859956 22948 agents.py:69] training 559 of 2000: completed tf_agent.train(...) = 1.718 [loss]\n",
      "I0624 23:46:55.861092 22948 agents.py:69] training 559 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:55.862953 22948 agents.py:69] training 560 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:56.054420 22948 agents.py:69] training 560 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:56.057312 22948 agents.py:69] training 560 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:56.298315 22948 agents.py:69] training 560 of 2000: completed tf_agent.train(...) = 1.921 [loss]\n",
      "I0624 23:46:56.300314 22948 agents.py:69] training 560 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:56.303778 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:46:57.635898 22948 agents.py:69] completed compute_avg_return(...) = 0.630\n",
      "I0624 23:46:57.637899 22948 agents.py:69] training 561 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:57.804491 22948 agents.py:69] training 561 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:57.807493 22948 agents.py:69] training 561 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:58.050648 22948 agents.py:69] training 561 of 2000: completed tf_agent.train(...) = 1.115 [loss]\n",
      "I0624 23:46:58.051649 22948 agents.py:69] training 561 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:58.054360 22948 agents.py:69] training 562 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:58.207524 22948 agents.py:69] training 562 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:58.211703 22948 agents.py:69] training 562 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:58.497874 22948 agents.py:69] training 562 of 2000: completed tf_agent.train(...) = 1.132 [loss]\n",
      "I0624 23:46:58.499614 22948 agents.py:69] training 562 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:58.500452 22948 agents.py:69] training 563 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:58.706006 22948 agents.py:69] training 563 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:58.710051 22948 agents.py:69] training 563 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:58.995323 22948 agents.py:69] training 563 of 2000: completed tf_agent.train(...) = 1.187 [loss]\n",
      "I0624 23:46:58.997331 22948 agents.py:69] training 563 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:58.999296 22948 agents.py:69] training 564 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:59.155660 22948 agents.py:69] training 564 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:59.160611 22948 agents.py:69] training 564 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:59.507720 22948 agents.py:69] training 564 of 2000: completed tf_agent.train(...) = 0.865 [loss]\n",
      "I0624 23:46:59.509721 22948 agents.py:69] training 564 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:59.512555 22948 agents.py:69] training 565 of 2000: executing collect_driver.run()\n",
      "I0624 23:46:59.709331 22948 agents.py:69] training 565 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:46:59.714390 22948 agents.py:69] training 565 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:46:59.988334 22948 agents.py:69] training 565 of 2000: completed tf_agent.train(...) = 0.644 [loss]\n",
      "I0624 23:46:59.989887 22948 agents.py:69] training 565 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:46:59.991929 22948 agents.py:69] training 566 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:00.164834 22948 agents.py:69] training 566 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:00.168838 22948 agents.py:69] training 566 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:00.438700 22948 agents.py:69] training 566 of 2000: completed tf_agent.train(...) = 0.760 [loss]\n",
      "I0624 23:47:00.439917 22948 agents.py:69] training 566 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:00.440819 22948 agents.py:69] training 567 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:00.597977 22948 agents.py:69] training 567 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:00.602503 22948 agents.py:69] training 567 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:00.862136 22948 agents.py:69] training 567 of 2000: completed tf_agent.train(...) = 0.538 [loss]\n",
      "I0624 23:47:00.864633 22948 agents.py:69] training 567 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:00.866135 22948 agents.py:69] training 568 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:01.017437 22948 agents.py:69] training 568 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:01.021121 22948 agents.py:69] training 568 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:01.275593 22948 agents.py:69] training 568 of 2000: completed tf_agent.train(...) = 0.570 [loss]\n",
      "I0624 23:47:01.276530 22948 agents.py:69] training 568 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:01.278644 22948 agents.py:69] training 569 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:01.438700 22948 agents.py:69] training 569 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:01.442042 22948 agents.py:69] training 569 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:01.663618 22948 agents.py:69] training 569 of 2000: completed tf_agent.train(...) = 0.780 [loss]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:47:01.664616 22948 agents.py:69] training 569 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:01.666621 22948 agents.py:69] training 570 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:01.792556 22948 agents.py:69] training 570 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:01.796507 22948 agents.py:69] training 570 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:02.057571 22948 agents.py:69] training 570 of 2000: completed tf_agent.train(...) = 0.546 [loss]\n",
      "I0624 23:47:02.059731 22948 agents.py:69] training 570 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:02.060577 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:47:03.584346 22948 agents.py:69] completed compute_avg_return(...) = 0.686\n",
      "I0624 23:47:03.587342 22948 agents.py:69] training 571 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:03.883564 22948 agents.py:69] training 571 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:03.887575 22948 agents.py:69] training 571 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:04.157479 22948 agents.py:69] training 571 of 2000: completed tf_agent.train(...) = 2.021 [loss]\n",
      "I0624 23:47:04.158524 22948 agents.py:69] training 571 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:04.160540 22948 agents.py:69] training 572 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:04.330914 22948 agents.py:69] training 572 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:04.333911 22948 agents.py:69] training 572 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:04.625545 22948 agents.py:69] training 572 of 2000: completed tf_agent.train(...) = 0.733 [loss]\n",
      "I0624 23:47:04.627547 22948 agents.py:69] training 572 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:04.630543 22948 agents.py:69] training 573 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:04.795740 22948 agents.py:69] training 573 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:04.799696 22948 agents.py:69] training 573 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:05.015286 22948 agents.py:69] training 573 of 2000: completed tf_agent.train(...) = 2.716 [loss]\n",
      "I0624 23:47:05.016287 22948 agents.py:69] training 573 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:05.018286 22948 agents.py:69] training 574 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:05.134715 22948 agents.py:69] training 574 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:05.137848 22948 agents.py:69] training 574 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:07.497037 22948 agents.py:69] training 574 of 2000: completed tf_agent.train(...) = 0.786 [loss]\n",
      "I0624 23:47:07.499710 22948 agents.py:69] training 574 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:07.501586 22948 agents.py:69] training 575 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:07.862772 22948 agents.py:69] training 575 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:07.868769 22948 agents.py:69] training 575 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:08.214210 22948 agents.py:69] training 575 of 2000: completed tf_agent.train(...) = 1.206 [loss]\n",
      "I0624 23:47:08.215207 22948 agents.py:69] training 575 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:08.218221 22948 agents.py:69] training 576 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:08.419623 22948 agents.py:69] training 576 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:08.424423 22948 agents.py:69] training 576 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:08.726656 22948 agents.py:69] training 576 of 2000: completed tf_agent.train(...) = 1.609 [loss]\n",
      "I0624 23:47:08.729651 22948 agents.py:69] training 576 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:08.732650 22948 agents.py:69] training 577 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:08.923655 22948 agents.py:69] training 577 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:08.928618 22948 agents.py:69] training 577 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:09.201691 22948 agents.py:69] training 577 of 2000: completed tf_agent.train(...) = 0.948 [loss]\n",
      "I0624 23:47:09.202625 22948 agents.py:69] training 577 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:09.205146 22948 agents.py:69] training 578 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:09.378306 22948 agents.py:69] training 578 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:09.382301 22948 agents.py:69] training 578 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:09.653218 22948 agents.py:69] training 578 of 2000: completed tf_agent.train(...) = 1.018 [loss]\n",
      "I0624 23:47:09.655300 22948 agents.py:69] training 578 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:09.657219 22948 agents.py:69] training 579 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:09.867124 22948 agents.py:69] training 579 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:09.877158 22948 agents.py:69] training 579 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:10.189448 22948 agents.py:69] training 579 of 2000: completed tf_agent.train(...) = 0.701 [loss]\n",
      "I0624 23:47:10.190446 22948 agents.py:69] training 579 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:10.193452 22948 agents.py:69] training 580 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:10.393442 22948 agents.py:69] training 580 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:10.398397 22948 agents.py:69] training 580 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:10.705495 22948 agents.py:69] training 580 of 2000: completed tf_agent.train(...) = 0.775 [loss]\n",
      "I0624 23:47:10.708495 22948 agents.py:69] training 580 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:10.710571 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:47:12.077724 22948 agents.py:69] completed compute_avg_return(...) = 0.628\n",
      "I0624 23:47:12.079759 22948 agents.py:69] training 581 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:12.271862 22948 agents.py:69] training 581 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:12.274861 22948 agents.py:69] training 581 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:12.519679 22948 agents.py:69] training 581 of 2000: completed tf_agent.train(...) = 1.114 [loss]\n",
      "I0624 23:47:12.520679 22948 agents.py:69] training 581 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:12.523685 22948 agents.py:69] training 582 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:12.741021 22948 agents.py:69] training 582 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:12.744025 22948 agents.py:69] training 582 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:13.052490 22948 agents.py:69] training 582 of 2000: completed tf_agent.train(...) = 1.260 [loss]\n",
      "I0624 23:47:13.054724 22948 agents.py:69] training 582 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:13.058487 22948 agents.py:69] training 583 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:13.226106 22948 agents.py:69] training 583 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:13.230105 22948 agents.py:69] training 583 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:13.509505 22948 agents.py:69] training 583 of 2000: completed tf_agent.train(...) = 0.711 [loss]\n",
      "I0624 23:47:13.510362 22948 agents.py:69] training 583 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:13.513361 22948 agents.py:69] training 584 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:13.670180 22948 agents.py:69] training 584 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:13.674177 22948 agents.py:69] training 584 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:13.956754 22948 agents.py:69] training 584 of 2000: completed tf_agent.train(...) = 0.860 [loss]\n",
      "I0624 23:47:13.958758 22948 agents.py:69] training 584 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:13.960452 22948 agents.py:69] training 585 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:14.130534 22948 agents.py:69] training 585 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:14.134857 22948 agents.py:69] training 585 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:14.404611 22948 agents.py:69] training 585 of 2000: completed tf_agent.train(...) = 0.689 [loss]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:47:14.406603 22948 agents.py:69] training 585 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:14.408523 22948 agents.py:69] training 586 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:14.575988 22948 agents.py:69] training 586 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:14.578985 22948 agents.py:69] training 586 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:14.848747 22948 agents.py:69] training 586 of 2000: completed tf_agent.train(...) = 0.472 [loss]\n",
      "I0624 23:47:14.850746 22948 agents.py:69] training 586 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:14.851875 22948 agents.py:69] training 587 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:15.009432 22948 agents.py:69] training 587 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:15.013347 22948 agents.py:69] training 587 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:15.287536 22948 agents.py:69] training 587 of 2000: completed tf_agent.train(...) = 0.573 [loss]\n",
      "I0624 23:47:15.288486 22948 agents.py:69] training 587 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:15.290468 22948 agents.py:69] training 588 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:15.464541 22948 agents.py:69] training 588 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:15.468401 22948 agents.py:69] training 588 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:15.742787 22948 agents.py:69] training 588 of 2000: completed tf_agent.train(...) = 1.787 [loss]\n",
      "I0624 23:47:15.743786 22948 agents.py:69] training 588 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:15.745788 22948 agents.py:69] training 589 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:15.882475 22948 agents.py:69] training 589 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:15.885477 22948 agents.py:69] training 589 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:18.238656 22948 agents.py:69] training 589 of 2000: completed tf_agent.train(...) = 0.572 [loss]\n",
      "I0624 23:47:18.240596 22948 agents.py:69] training 589 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:18.242593 22948 agents.py:69] training 590 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:18.401710 22948 agents.py:69] training 590 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:18.404702 22948 agents.py:69] training 590 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:18.687590 22948 agents.py:69] training 590 of 2000: completed tf_agent.train(...) = 1.298 [loss]\n",
      "I0624 23:47:18.689473 22948 agents.py:69] training 590 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:18.691098 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:47:22.022025 22948 agents.py:69] completed compute_avg_return(...) = 0.395\n",
      "I0624 23:47:22.025022 22948 agents.py:69] training 591 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:22.231096 22948 agents.py:69] training 591 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:22.234098 22948 agents.py:69] training 591 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:22.517045 22948 agents.py:69] training 591 of 2000: completed tf_agent.train(...) = 0.519 [loss]\n",
      "I0624 23:47:22.518964 22948 agents.py:69] training 591 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:22.521046 22948 agents.py:69] training 592 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:22.724947 22948 agents.py:69] training 592 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:22.730948 22948 agents.py:69] training 592 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:25.154539 22948 agents.py:69] training 592 of 2000: completed tf_agent.train(...) = 3.444 [loss]\n",
      "I0624 23:47:25.155537 22948 agents.py:69] training 592 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:25.156542 22948 agents.py:69] training 593 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:25.397850 22948 agents.py:69] training 593 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:25.406845 22948 agents.py:69] training 593 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:25.794614 22948 agents.py:69] training 593 of 2000: completed tf_agent.train(...) = 0.704 [loss]\n",
      "I0624 23:47:25.796612 22948 agents.py:69] training 593 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:25.799188 22948 agents.py:69] training 594 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:25.998284 22948 agents.py:69] training 594 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:26.002288 22948 agents.py:69] training 594 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:26.286469 22948 agents.py:69] training 594 of 2000: completed tf_agent.train(...) = 1.008 [loss]\n",
      "I0624 23:47:26.288469 22948 agents.py:69] training 594 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:26.290724 22948 agents.py:69] training 595 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:26.452593 22948 agents.py:69] training 595 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:26.456595 22948 agents.py:69] training 595 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:26.709767 22948 agents.py:69] training 595 of 2000: completed tf_agent.train(...) = 0.740 [loss]\n",
      "I0624 23:47:26.710654 22948 agents.py:69] training 595 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:26.712655 22948 agents.py:69] training 596 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:26.876916 22948 agents.py:69] training 596 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:26.878838 22948 agents.py:69] training 596 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:27.145461 22948 agents.py:69] training 596 of 2000: completed tf_agent.train(...) = 0.768 [loss]\n",
      "I0624 23:47:27.148757 22948 agents.py:69] training 596 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:27.150454 22948 agents.py:69] training 597 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:27.375745 22948 agents.py:69] training 597 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:27.378800 22948 agents.py:69] training 597 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:27.661712 22948 agents.py:69] training 597 of 2000: completed tf_agent.train(...) = 0.668 [loss]\n",
      "I0624 23:47:27.662620 22948 agents.py:69] training 597 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:27.664720 22948 agents.py:69] training 598 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:27.818951 22948 agents.py:69] training 598 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:27.821901 22948 agents.py:69] training 598 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:28.095449 22948 agents.py:69] training 598 of 2000: completed tf_agent.train(...) = 1.405 [loss]\n",
      "I0624 23:47:28.098446 22948 agents.py:69] training 598 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:28.100572 22948 agents.py:69] training 599 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:28.279767 22948 agents.py:69] training 599 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:28.284769 22948 agents.py:69] training 599 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:28.591828 22948 agents.py:69] training 599 of 2000: completed tf_agent.train(...) = 2.600 [loss]\n",
      "I0624 23:47:28.592824 22948 agents.py:69] training 599 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:28.595829 22948 agents.py:69] training 600 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:28.772777 22948 agents.py:69] training 600 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:28.776597 22948 agents.py:69] training 600 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:29.040503 22948 agents.py:69] training 600 of 2000: completed tf_agent.train(...) = 0.977 [loss]\n",
      "I0624 23:47:29.041502 22948 agents.py:69] training 600 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:29.044514 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:47:35.268017 22948 agents.py:69] completed compute_avg_return(...) = 0.093\n",
      "I0624 23:47:35.269165 22948 agents.py:69] training 601 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:35.448572 22948 agents.py:69] training 601 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:35.453535 22948 agents.py:69] training 601 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:35.723735 22948 agents.py:69] training 601 of 2000: completed tf_agent.train(...) = 1.386 [loss]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:47:35.726181 22948 agents.py:69] training 601 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:35.727736 22948 agents.py:69] training 602 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:35.928500 22948 agents.py:69] training 602 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:35.931451 22948 agents.py:69] training 602 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:36.214546 22948 agents.py:69] training 602 of 2000: completed tf_agent.train(...) = 0.657 [loss]\n",
      "I0624 23:47:36.215550 22948 agents.py:69] training 602 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:36.217677 22948 agents.py:69] training 603 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:36.455031 22948 agents.py:69] training 603 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:36.459798 22948 agents.py:69] training 603 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:36.736712 22948 agents.py:69] training 603 of 2000: completed tf_agent.train(...) = 0.812 [loss]\n",
      "I0624 23:47:36.737576 22948 agents.py:69] training 603 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:36.739676 22948 agents.py:69] training 604 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:36.893901 22948 agents.py:69] training 604 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:36.897251 22948 agents.py:69] training 604 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:37.158553 22948 agents.py:69] training 604 of 2000: completed tf_agent.train(...) = 0.579 [loss]\n",
      "I0624 23:47:37.159673 22948 agents.py:69] training 604 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:37.160569 22948 agents.py:69] training 605 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:37.347124 22948 agents.py:69] training 605 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:37.351112 22948 agents.py:69] training 605 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:37.630645 22948 agents.py:69] training 605 of 2000: completed tf_agent.train(...) = 0.996 [loss]\n",
      "I0624 23:47:37.633649 22948 agents.py:69] training 605 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:37.635838 22948 agents.py:69] training 606 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:37.808587 22948 agents.py:69] training 606 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:37.812606 22948 agents.py:69] training 606 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:38.101771 22948 agents.py:69] training 606 of 2000: completed tf_agent.train(...) = 1.145 [loss]\n",
      "I0624 23:47:38.103771 22948 agents.py:69] training 606 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:38.106771 22948 agents.py:69] training 607 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:38.290241 22948 agents.py:69] training 607 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:38.294793 22948 agents.py:69] training 607 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:38.600876 22948 agents.py:69] training 607 of 2000: completed tf_agent.train(...) = 0.735 [loss]\n",
      "I0624 23:47:38.602881 22948 agents.py:69] training 607 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:38.605889 22948 agents.py:69] training 608 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:38.793636 22948 agents.py:69] training 608 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:38.796730 22948 agents.py:69] training 608 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:39.063535 22948 agents.py:69] training 608 of 2000: completed tf_agent.train(...) = 0.718 [loss]\n",
      "I0624 23:47:39.066538 22948 agents.py:69] training 608 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:39.068536 22948 agents.py:69] training 609 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:39.249446 22948 agents.py:69] training 609 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:39.253450 22948 agents.py:69] training 609 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:39.554428 22948 agents.py:69] training 609 of 2000: completed tf_agent.train(...) = 0.639 [loss]\n",
      "I0624 23:47:39.557431 22948 agents.py:69] training 609 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:39.560500 22948 agents.py:69] training 610 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:39.738756 22948 agents.py:69] training 610 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:39.742712 22948 agents.py:69] training 610 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:40.046412 22948 agents.py:69] training 610 of 2000: completed tf_agent.train(...) = 0.731 [loss]\n",
      "I0624 23:47:40.048406 22948 agents.py:69] training 610 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:40.050408 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:47:42.639102 22948 agents.py:69] completed compute_avg_return(...) = 0.440\n",
      "I0624 23:47:42.640004 22948 agents.py:69] training 611 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:42.820628 22948 agents.py:69] training 611 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:42.823734 22948 agents.py:69] training 611 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:43.133973 22948 agents.py:69] training 611 of 2000: completed tf_agent.train(...) = 0.641 [loss]\n",
      "I0624 23:47:43.135973 22948 agents.py:69] training 611 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:43.137974 22948 agents.py:69] training 612 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:43.317245 22948 agents.py:69] training 612 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:43.320198 22948 agents.py:69] training 612 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:43.614671 22948 agents.py:69] training 612 of 2000: completed tf_agent.train(...) = 0.758 [loss]\n",
      "I0624 23:47:43.617673 22948 agents.py:69] training 612 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:43.619668 22948 agents.py:69] training 613 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:43.789634 22948 agents.py:69] training 613 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:43.792623 22948 agents.py:69] training 613 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:44.103685 22948 agents.py:69] training 613 of 2000: completed tf_agent.train(...) = 0.649 [loss]\n",
      "I0624 23:47:44.106681 22948 agents.py:69] training 613 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:44.109850 22948 agents.py:69] training 614 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:44.260683 22948 agents.py:69] training 614 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:44.263522 22948 agents.py:69] training 614 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:44.546574 22948 agents.py:69] training 614 of 2000: completed tf_agent.train(...) = 0.500 [loss]\n",
      "I0624 23:47:44.548573 22948 agents.py:69] training 614 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:44.550572 22948 agents.py:69] training 615 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:44.737646 22948 agents.py:69] training 615 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:44.740784 22948 agents.py:69] training 615 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:45.036850 22948 agents.py:69] training 615 of 2000: completed tf_agent.train(...) = 0.955 [loss]\n",
      "I0624 23:47:45.040835 22948 agents.py:69] training 615 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:45.044838 22948 agents.py:69] training 616 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:45.276863 22948 agents.py:69] training 616 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:45.284016 22948 agents.py:69] training 616 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:45.684708 22948 agents.py:69] training 616 of 2000: completed tf_agent.train(...) = 0.986 [loss]\n",
      "I0624 23:47:45.686702 22948 agents.py:69] training 616 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:45.688974 22948 agents.py:69] training 617 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:45.887750 22948 agents.py:69] training 617 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:45.890912 22948 agents.py:69] training 617 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:46.155575 22948 agents.py:69] training 617 of 2000: completed tf_agent.train(...) = 1.264 [loss]\n",
      "I0624 23:47:46.158834 22948 agents.py:69] training 617 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:46.159570 22948 agents.py:69] training 618 of 2000: executing collect_driver.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:47:46.433727 22948 agents.py:69] training 618 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:46.436689 22948 agents.py:69] training 618 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:46.713146 22948 agents.py:69] training 618 of 2000: completed tf_agent.train(...) = 13.129 [loss]\n",
      "I0624 23:47:46.714147 22948 agents.py:69] training 618 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:46.716148 22948 agents.py:69] training 619 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:46.873100 22948 agents.py:69] training 619 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:46.877783 22948 agents.py:69] training 619 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:47.140288 22948 agents.py:69] training 619 of 2000: completed tf_agent.train(...) = 0.895 [loss]\n",
      "I0624 23:47:47.143287 22948 agents.py:69] training 619 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:47.145288 22948 agents.py:69] training 620 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:47.343739 22948 agents.py:69] training 620 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:47.346892 22948 agents.py:69] training 620 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:47.635651 22948 agents.py:69] training 620 of 2000: completed tf_agent.train(...) = 7.835 [loss]\n",
      "I0624 23:47:47.638651 22948 agents.py:69] training 620 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:47.641648 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:47:56.249867 22948 agents.py:69] completed compute_avg_return(...) = -1.393\n",
      "I0624 23:47:56.250866 22948 agents.py:69] training 621 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:56.389576 22948 agents.py:69] training 621 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:56.392575 22948 agents.py:69] training 621 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:56.614817 22948 agents.py:69] training 621 of 2000: completed tf_agent.train(...) = 1.612 [loss]\n",
      "I0624 23:47:56.615845 22948 agents.py:69] training 621 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:56.617817 22948 agents.py:69] training 622 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:56.751697 22948 agents.py:69] training 622 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:56.754698 22948 agents.py:69] training 622 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:56.982669 22948 agents.py:69] training 622 of 2000: completed tf_agent.train(...) = 0.917 [loss]\n",
      "I0624 23:47:56.983698 22948 agents.py:69] training 622 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:56.984663 22948 agents.py:69] training 623 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:57.111536 22948 agents.py:69] training 623 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:57.115543 22948 agents.py:69] training 623 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:57.335700 22948 agents.py:69] training 623 of 2000: completed tf_agent.train(...) = 1.197 [loss]\n",
      "I0624 23:47:57.337281 22948 agents.py:69] training 623 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:57.338698 22948 agents.py:69] training 624 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:57.544677 22948 agents.py:69] training 624 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:57.550077 22948 agents.py:69] training 624 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:57.902133 22948 agents.py:69] training 624 of 2000: completed tf_agent.train(...) = 2.438 [loss]\n",
      "I0624 23:47:57.904131 22948 agents.py:69] training 624 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:57.908135 22948 agents.py:69] training 625 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:58.105664 22948 agents.py:69] training 625 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:58.108667 22948 agents.py:69] training 625 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:58.338773 22948 agents.py:69] training 625 of 2000: completed tf_agent.train(...) = 0.895 [loss]\n",
      "I0624 23:47:58.339894 22948 agents.py:69] training 625 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:58.341789 22948 agents.py:69] training 626 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:58.535946 22948 agents.py:69] training 626 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:58.538946 22948 agents.py:69] training 626 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:58.805183 22948 agents.py:69] training 626 of 2000: completed tf_agent.train(...) = 1.588 [loss]\n",
      "I0624 23:47:58.806180 22948 agents.py:69] training 626 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:58.809184 22948 agents.py:69] training 627 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:58.951247 22948 agents.py:69] training 627 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:58.954883 22948 agents.py:69] training 627 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:59.184455 22948 agents.py:69] training 627 of 2000: completed tf_agent.train(...) = 0.847 [loss]\n",
      "I0624 23:47:59.185742 22948 agents.py:69] training 627 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:59.187370 22948 agents.py:69] training 628 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:59.339752 22948 agents.py:69] training 628 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:59.344680 22948 agents.py:69] training 628 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:59.613014 22948 agents.py:69] training 628 of 2000: completed tf_agent.train(...) = 1.154 [loss]\n",
      "I0624 23:47:59.614377 22948 agents.py:69] training 628 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:59.615922 22948 agents.py:69] training 629 of 2000: executing collect_driver.run()\n",
      "I0624 23:47:59.742299 22948 agents.py:69] training 629 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:47:59.745509 22948 agents.py:69] training 629 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:47:59.958615 22948 agents.py:69] training 629 of 2000: completed tf_agent.train(...) = 0.773 [loss]\n",
      "I0624 23:47:59.959617 22948 agents.py:69] training 629 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:47:59.961618 22948 agents.py:69] training 630 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:00.146085 22948 agents.py:69] training 630 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:00.148053 22948 agents.py:69] training 630 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:00.373050 22948 agents.py:69] training 630 of 2000: completed tf_agent.train(...) = 92.271 [loss]\n",
      "I0624 23:48:00.374737 22948 agents.py:69] training 630 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:00.375734 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:48:08.082505 22948 agents.py:69] completed compute_avg_return(...) = -0.822\n",
      "I0624 23:48:08.083505 22948 agents.py:69] training 631 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:08.218780 22948 agents.py:69] training 631 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:08.221780 22948 agents.py:69] training 631 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:10.421862 22948 agents.py:69] training 631 of 2000: completed tf_agent.train(...) = 2.679 [loss]\n",
      "I0624 23:48:10.423652 22948 agents.py:69] training 631 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:10.424820 22948 agents.py:69] training 632 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:10.591824 22948 agents.py:69] training 632 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:10.597806 22948 agents.py:69] training 632 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:10.946772 22948 agents.py:69] training 632 of 2000: completed tf_agent.train(...) = 29.082 [loss]\n",
      "I0624 23:48:10.947771 22948 agents.py:69] training 632 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:10.950775 22948 agents.py:69] training 633 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:11.259495 22948 agents.py:69] training 633 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:11.261469 22948 agents.py:69] training 633 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:13.171684 22948 agents.py:69] training 633 of 2000: completed tf_agent.train(...) = 191.002 [loss]\n",
      "I0624 23:48:13.172838 22948 agents.py:69] training 633 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:13.174866 22948 agents.py:69] training 634 of 2000: executing collect_driver.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:48:13.306820 22948 agents.py:69] training 634 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:13.310861 22948 agents.py:69] training 634 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:13.543643 22948 agents.py:69] training 634 of 2000: completed tf_agent.train(...) = 17.081 [loss]\n",
      "I0624 23:48:13.544551 22948 agents.py:69] training 634 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:13.545549 22948 agents.py:69] training 635 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:13.701255 22948 agents.py:69] training 635 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:13.704219 22948 agents.py:69] training 635 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:13.928554 22948 agents.py:69] training 635 of 2000: completed tf_agent.train(...) = 10.200 [loss]\n",
      "I0624 23:48:13.929536 22948 agents.py:69] training 635 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:13.931535 22948 agents.py:69] training 636 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:14.053687 22948 agents.py:69] training 636 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:14.057780 22948 agents.py:69] training 636 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:14.269469 22948 agents.py:69] training 636 of 2000: completed tf_agent.train(...) = 17.697 [loss]\n",
      "I0624 23:48:14.270435 22948 agents.py:69] training 636 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:14.271765 22948 agents.py:69] training 637 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:14.375688 22948 agents.py:69] training 637 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:14.378659 22948 agents.py:69] training 637 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:14.576487 22948 agents.py:69] training 637 of 2000: completed tf_agent.train(...) = 10.228 [loss]\n",
      "I0624 23:48:14.577522 22948 agents.py:69] training 637 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:14.578484 22948 agents.py:69] training 638 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:14.706876 22948 agents.py:69] training 638 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:14.710874 22948 agents.py:69] training 638 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:14.943813 22948 agents.py:69] training 638 of 2000: completed tf_agent.train(...) = 2.449 [loss]\n",
      "I0624 23:48:14.944632 22948 agents.py:69] training 638 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:14.946636 22948 agents.py:69] training 639 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:15.171640 22948 agents.py:69] training 639 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:15.174610 22948 agents.py:69] training 639 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:15.438266 22948 agents.py:69] training 639 of 2000: completed tf_agent.train(...) = 63.441 [loss]\n",
      "I0624 23:48:15.439566 22948 agents.py:69] training 639 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:15.440788 22948 agents.py:69] training 640 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:15.557589 22948 agents.py:69] training 640 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:15.560594 22948 agents.py:69] training 640 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:15.801555 22948 agents.py:69] training 640 of 2000: completed tf_agent.train(...) = 6.704 [loss]\n",
      "I0624 23:48:15.802793 22948 agents.py:69] training 640 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:15.804560 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:48:21.008160 22948 agents.py:69] completed compute_avg_return(...) = -0.207\n",
      "I0624 23:48:21.012415 22948 agents.py:69] training 641 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:21.255495 22948 agents.py:69] training 641 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:21.259164 22948 agents.py:69] training 641 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:21.634593 22948 agents.py:69] training 641 of 2000: completed tf_agent.train(...) = 3.449 [loss]\n",
      "I0624 23:48:21.637593 22948 agents.py:69] training 641 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:21.640592 22948 agents.py:69] training 642 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:21.781593 22948 agents.py:69] training 642 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:21.784589 22948 agents.py:69] training 642 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:21.981438 22948 agents.py:69] training 642 of 2000: completed tf_agent.train(...) = 2.782 [loss]\n",
      "I0624 23:48:21.982121 22948 agents.py:69] training 642 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:21.984125 22948 agents.py:69] training 643 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:22.087155 22948 agents.py:69] training 643 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:22.089125 22948 agents.py:69] training 643 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:22.285121 22948 agents.py:69] training 643 of 2000: completed tf_agent.train(...) = 3.729 [loss]\n",
      "I0624 23:48:22.286119 22948 agents.py:69] training 643 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:22.287213 22948 agents.py:69] training 644 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:22.412905 22948 agents.py:69] training 644 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:22.416476 22948 agents.py:69] training 644 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:22.695039 22948 agents.py:69] training 644 of 2000: completed tf_agent.train(...) = 1.911 [loss]\n",
      "I0624 23:48:22.696037 22948 agents.py:69] training 644 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:22.698545 22948 agents.py:69] training 645 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:22.854353 22948 agents.py:69] training 645 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:22.857317 22948 agents.py:69] training 645 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:23.118624 22948 agents.py:69] training 645 of 2000: completed tf_agent.train(...) = 1.995 [loss]\n",
      "I0624 23:48:23.119421 22948 agents.py:69] training 645 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:23.121430 22948 agents.py:69] training 646 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:23.264723 22948 agents.py:69] training 646 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:23.267720 22948 agents.py:69] training 646 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:23.516339 22948 agents.py:69] training 646 of 2000: completed tf_agent.train(...) = 2.002 [loss]\n",
      "I0624 23:48:23.518288 22948 agents.py:69] training 646 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:23.519287 22948 agents.py:69] training 647 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:23.733599 22948 agents.py:69] training 647 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:23.735566 22948 agents.py:69] training 647 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:23.985726 22948 agents.py:69] training 647 of 2000: completed tf_agent.train(...) = 88.965 [loss]\n",
      "I0624 23:48:23.986730 22948 agents.py:69] training 647 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:23.988727 22948 agents.py:69] training 648 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:24.098515 22948 agents.py:69] training 648 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:24.102518 22948 agents.py:69] training 648 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:24.295010 22948 agents.py:69] training 648 of 2000: completed tf_agent.train(...) = 5.267 [loss]\n",
      "I0624 23:48:24.296009 22948 agents.py:69] training 648 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:24.297009 22948 agents.py:69] training 649 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:24.431671 22948 agents.py:69] training 649 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:24.434546 22948 agents.py:69] training 649 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:24.639499 22948 agents.py:69] training 649 of 2000: completed tf_agent.train(...) = 3.469 [loss]\n",
      "I0624 23:48:24.641380 22948 agents.py:69] training 649 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:24.643382 22948 agents.py:69] training 650 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:24.780855 22948 agents.py:69] training 650 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:24.783859 22948 agents.py:69] training 650 of 2000: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:48:25.019938 22948 agents.py:69] training 650 of 2000: completed tf_agent.train(...) = 25.781 [loss]\n",
      "I0624 23:48:25.020934 22948 agents.py:69] training 650 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:25.023942 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:48:25.755285 22948 agents.py:69] completed compute_avg_return(...) = 0.652\n",
      "I0624 23:48:25.756272 22948 agents.py:69] training 651 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:25.875773 22948 agents.py:69] training 651 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:25.877772 22948 agents.py:69] training 651 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:26.090619 22948 agents.py:69] training 651 of 2000: completed tf_agent.train(...) = 12.203 [loss]\n",
      "I0624 23:48:26.091619 22948 agents.py:69] training 651 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:26.093620 22948 agents.py:69] training 652 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:26.337610 22948 agents.py:69] training 652 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:26.343617 22948 agents.py:69] training 652 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:28.414440 22948 agents.py:69] training 652 of 2000: completed tf_agent.train(...) = 236.292 [loss]\n",
      "I0624 23:48:28.416440 22948 agents.py:69] training 652 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:28.417551 22948 agents.py:69] training 653 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:28.603599 22948 agents.py:69] training 653 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:28.610610 22948 agents.py:69] training 653 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:29.056100 22948 agents.py:69] training 653 of 2000: completed tf_agent.train(...) = 14.112 [loss]\n",
      "I0624 23:48:29.057549 22948 agents.py:69] training 653 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:29.062821 22948 agents.py:69] training 654 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:29.295941 22948 agents.py:69] training 654 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:29.299070 22948 agents.py:69] training 654 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:29.593467 22948 agents.py:69] training 654 of 2000: completed tf_agent.train(...) = 13.959 [loss]\n",
      "I0624 23:48:29.595019 22948 agents.py:69] training 654 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:29.597024 22948 agents.py:69] training 655 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:29.859790 22948 agents.py:69] training 655 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:29.866583 22948 agents.py:69] training 655 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:30.281714 22948 agents.py:69] training 655 of 2000: completed tf_agent.train(...) = 7.332 [loss]\n",
      "I0624 23:48:30.283766 22948 agents.py:69] training 655 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:30.286713 22948 agents.py:69] training 656 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:30.456807 22948 agents.py:69] training 656 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:30.459618 22948 agents.py:69] training 656 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:30.699526 22948 agents.py:69] training 656 of 2000: completed tf_agent.train(...) = 10.053 [loss]\n",
      "I0624 23:48:30.700484 22948 agents.py:69] training 656 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:30.702486 22948 agents.py:69] training 657 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:30.826123 22948 agents.py:69] training 657 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:30.831572 22948 agents.py:69] training 657 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:31.267057 22948 agents.py:69] training 657 of 2000: completed tf_agent.train(...) = 5.112 [loss]\n",
      "I0624 23:48:31.269009 22948 agents.py:69] training 657 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:31.272008 22948 agents.py:69] training 658 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:31.528932 22948 agents.py:69] training 658 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:31.533935 22948 agents.py:69] training 658 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:31.880882 22948 agents.py:69] training 658 of 2000: completed tf_agent.train(...) = 4.450 [loss]\n",
      "I0624 23:48:31.882884 22948 agents.py:69] training 658 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:31.884882 22948 agents.py:69] training 659 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:31.993614 22948 agents.py:69] training 659 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:31.996613 22948 agents.py:69] training 659 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:32.191722 22948 agents.py:69] training 659 of 2000: completed tf_agent.train(...) = 4.437 [loss]\n",
      "I0624 23:48:32.193040 22948 agents.py:69] training 659 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:32.193721 22948 agents.py:69] training 660 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:32.364100 22948 agents.py:69] training 660 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:32.369506 22948 agents.py:69] training 660 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:32.633510 22948 agents.py:69] training 660 of 2000: completed tf_agent.train(...) = 6.277 [loss]\n",
      "I0624 23:48:32.634510 22948 agents.py:69] training 660 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:32.636511 22948 agents.py:69] executing compute_avg_return(...)\n",
      "I0624 23:48:34.750078 22948 agents.py:69] completed compute_avg_return(...) = 0.493\n",
      "I0624 23:48:34.750852 22948 agents.py:69] training 661 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:34.862867 22948 agents.py:69] training 661 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:34.865974 22948 agents.py:69] training 661 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:35.080660 22948 agents.py:69] training 661 of 2000: completed tf_agent.train(...) = 5.601 [loss]\n",
      "I0624 23:48:35.082660 22948 agents.py:69] training 661 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:35.084667 22948 agents.py:69] training 662 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:35.273772 22948 agents.py:69] training 662 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:35.277732 22948 agents.py:69] training 662 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:35.496463 22948 agents.py:69] training 662 of 2000: completed tf_agent.train(...) = 8.436 [loss]\n",
      "I0624 23:48:35.498462 22948 agents.py:69] training 662 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:35.499462 22948 agents.py:69] training 663 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:35.610583 22948 agents.py:69] training 663 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:35.613685 22948 agents.py:69] training 663 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:35.824918 22948 agents.py:69] training 663 of 2000: completed tf_agent.train(...) = 4.760 [loss]\n",
      "I0624 23:48:35.825920 22948 agents.py:69] training 663 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:35.826915 22948 agents.py:69] training 664 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:36.005047 22948 agents.py:69] training 664 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:36.010007 22948 agents.py:69] training 664 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:36.346783 22948 agents.py:69] training 664 of 2000: completed tf_agent.train(...) = 3.641 [loss]\n",
      "I0624 23:48:36.348592 22948 agents.py:69] training 664 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:36.350905 22948 agents.py:69] training 665 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:36.545188 22948 agents.py:69] training 665 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:36.548450 22948 agents.py:69] training 665 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:36.787174 22948 agents.py:69] training 665 of 2000: completed tf_agent.train(...) = 1.574 [loss]\n",
      "I0624 23:48:36.787996 22948 agents.py:69] training 665 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:36.789996 22948 agents.py:69] training 666 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:36.915201 22948 agents.py:69] training 666 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:36.919204 22948 agents.py:69] training 666 of 2000: executing tf_agent.train(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 23:48:37.144398 22948 agents.py:69] training 666 of 2000: completed tf_agent.train(...) = 4.669 [loss]\n",
      "I0624 23:48:37.146472 22948 agents.py:69] training 666 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:37.148398 22948 agents.py:69] training 667 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:37.289731 22948 agents.py:69] training 667 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:37.293601 22948 agents.py:69] training 667 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:37.534844 22948 agents.py:69] training 667 of 2000: completed tf_agent.train(...) = 3.586 [loss]\n",
      "I0624 23:48:37.536353 22948 agents.py:69] training 667 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:37.537840 22948 agents.py:69] training 668 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:37.721325 22948 agents.py:69] training 668 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:37.724329 22948 agents.py:69] training 668 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:37.939496 22948 agents.py:69] training 668 of 2000: completed tf_agent.train(...) = 2.031 [loss]\n",
      "I0624 23:48:37.940497 22948 agents.py:69] training 668 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:37.942497 22948 agents.py:69] training 669 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:38.058428 22948 agents.py:69] training 669 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:38.060426 22948 agents.py:69] training 669 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:38.263542 22948 agents.py:69] training 669 of 2000: completed tf_agent.train(...) = 1.918 [loss]\n",
      "I0624 23:48:38.264543 22948 agents.py:69] training 669 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:38.265542 22948 agents.py:69] training 670 of 2000: executing collect_driver.run()\n",
      "I0624 23:48:38.388267 22948 agents.py:69] training 670 of 2000: executing replay_buffer.gather_all()\n",
      "I0624 23:48:38.391270 22948 agents.py:69] training 670 of 2000: executing tf_agent.train(...)\n",
      "I0624 23:48:38.587503 22948 agents.py:69] training 670 of 2000: completed tf_agent.train(...) = 3.768 [loss]\n",
      "I0624 23:48:38.588505 22948 agents.py:69] training 670 of 2000: executing replay_buffer.clear()\n",
      "I0624 23:48:38.589504 22948 agents.py:69] executing compute_avg_return(...)\n"
     ]
    }
   ],
   "source": [
    "ppoAgent = PpoAgent(    gym_env_name = 'Berater-v1',\n",
    "                        fc_layers=(500,500,500), \n",
    "                        training_duration=training_duration,\n",
    "                        logging=logging,\n",
    "                        learning_rate=1e-4\n",
    "                   )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zpzHtN3-kQ26",
    "w3OdHyWEEEwy",
    "bzoq0VM85p46"
   ],
   "name": "190316-2_tfagents_berater-v12_ppo.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
