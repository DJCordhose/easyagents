{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU7ylMh1kQ2y"
   },
   "source": [
    "# Berater Environment v13 (Orso's live)\n",
    "\n",
    "## Changes from v12 (work in progress)\n",
    "* migration to easyagents\n",
    "* Max. number of steps management and debug logging migrated to easyagents.TrainingDuration\n",
    "* render yields the last step, supporting modes 'ansi', 'human', 'rgb_array'\n",
    "* the story of Orso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Install gym, tensorflow, tf-agents,..., setup display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install easyagents > /dev/null\n",
    "!pip install networkx==2.3.0 > /dev/null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !apt-get install xvfb >/dev/null\n",
    "    !pip install pyvirtualdisplay >/dev/null    \n",
    "    \n",
    "    from pyvirtualdisplay import Display\n",
    "    Display(visible=0, size=(960, 720)).start() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3OdHyWEEEwy"
   },
   "source": [
    "# Define Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQyb_Aq8Kg9j",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "from gym import spaces\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsJ6zcXvwN53"
   },
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-S4sZG5ZkQ3T",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def state_name_to_int(state):\n",
    "    state_name_map = {\n",
    "        'S': 0,\n",
    "        'A': 1,\n",
    "        'B': 2,\n",
    "        'C': 3,\n",
    "        'D': 4,\n",
    "        'E': 5,\n",
    "        'F': 6,\n",
    "        'G': 7,\n",
    "        'H': 8,\n",
    "        'K': 9,\n",
    "        'L': 10,\n",
    "        'M': 11,\n",
    "        'N': 12,\n",
    "        'O': 13\n",
    "    }\n",
    "    return state_name_map[state]\n",
    "\n",
    "def int_to_state_name(state_as_int):\n",
    "    state_map = {\n",
    "        0: 'S',\n",
    "        1: 'A',\n",
    "        2: 'B',\n",
    "        3: 'C',\n",
    "        4: 'D',\n",
    "        5: 'E',\n",
    "        6: 'F',\n",
    "        7: 'G',\n",
    "        8: 'H',\n",
    "        9: 'K',\n",
    "        10: 'L',\n",
    "        11: 'M',\n",
    "        12: 'N',\n",
    "        13: 'O'\n",
    "    }\n",
    "    return state_map[state_as_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-olom0nwiSX"
   },
   "source": [
    "### Berater Environment (OpenAI Gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3plH2u3Swotj",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class BeraterEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    The Berater Problem\n",
    "\n",
    "    Actions:\n",
    "    There are 4 discrete deterministic actions, each choosing one direction\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['ansi']}\n",
    "    showStep = False\n",
    "\n",
    "    def __init__(self):\n",
    "        #         self.map = {\n",
    "        #             'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
    "        #             'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
    "        #             'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
    "        #             'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
    "        #         }\n",
    "        self.map = {\n",
    "            'S': [('A', 300), ('B', 100), ('C', 200)],\n",
    "            'A': [('S', 300), ('B', 100), ('E', 100), ('D', 100)],\n",
    "            'B': [('S', 100), ('A', 100), ('C', 50), ('K', 200)],\n",
    "            'C': [('S', 200), ('B', 50), ('M', 100), ('L', 200)],\n",
    "            'D': [('A', 100), ('F', 50)],\n",
    "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
    "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
    "            'G': [('F', 200), ('O', 300)],\n",
    "            'H': [('E', 100), ('K', 300)],\n",
    "            'K': [('B', 200), ('H', 300)],\n",
    "            'L': [('C', 200), ('M', 50)],\n",
    "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
    "            'N': [('M', 100), ('O', 100)],\n",
    "            'O': [('N', 100), ('G', 300)]\n",
    "        }\n",
    "        max_paths = 4\n",
    "        self.action_space = spaces.Discrete(max_paths)\n",
    "\n",
    "        positions = len(self.map)\n",
    "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
    "        # non existing path is -1000 and no position change\n",
    "        # look at what #getObservation returns if you are confused\n",
    "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
    "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
    "        self.observation_space = spaces.Box(low=low,\n",
    "                                            high=high,\n",
    "                                            dtype=np.float32)\n",
    "        self.reward_range = (-1, 1)\n",
    "        self.envEpisodeCount = 0\n",
    "        self.envStepCount = 0\n",
    "\n",
    "        self.reset()\n",
    "        self.optimum = self.calculate_customers_reward()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def iterate_path(self, state, action):\n",
    "        paths = self.map[state]\n",
    "        if action < len(paths):\n",
    "            return paths[action]\n",
    "        else:\n",
    "            # sorry, no such action, stay where you are and pay a high penalty\n",
    "            return (state, 1000)\n",
    "\n",
    "    def step(self, action):\n",
    "        destination, cost = self.iterate_path(self.state, action)\n",
    "\n",
    "        self.cost = cost\n",
    "        self.action=action\n",
    "        self.lastStep_state = self.state\n",
    "        self.state = destination\n",
    "        self.customerReward = self.customer_reward[destination]\n",
    "        self.reward = 0\n",
    "        self.reward = (self.customerReward - self.cost) / self.optimum\n",
    "\n",
    "        self.customer_visited(destination)\n",
    "        done = (destination == 'S' and self.all_customers_visited())\n",
    "\n",
    "        stateAsInt = state_name_to_int(self.state)\n",
    "        self.totalReward += self.reward\n",
    "        self.stepCount += 1\n",
    "        self.envStepCount += 1\n",
    "\n",
    "        if done and not self.isDone:\n",
    "            self.envEpisodeCount += 1\n",
    "\n",
    "        self.isDone = done\n",
    "        observation = self.getObservation(stateAsInt)\n",
    "        info = {\"from\": self.state, \"to\": destination}\n",
    "        return observation, self.reward, done, info\n",
    "\n",
    "    def getObservation(self, position):\n",
    "        result = np.array([position,\n",
    "                           self.getPathObservation(position, 0),\n",
    "                           self.getPathObservation(position, 1),\n",
    "                           self.getPathObservation(position, 2),\n",
    "                           self.getPathObservation(position, 3)\n",
    "                           ],\n",
    "                          dtype=np.float32)\n",
    "        all_rest_rewards = list(self.customer_reward.values())\n",
    "        result = np.append(result, all_rest_rewards)\n",
    "        return result\n",
    "\n",
    "    def getPathObservation(self, position, path):\n",
    "        paths = self.map[self.state]\n",
    "        if path < len(paths):\n",
    "            target, cost = paths[path]\n",
    "            reward = self.customer_reward[target]\n",
    "            result = reward - cost\n",
    "        else:\n",
    "            result = -1000\n",
    "\n",
    "        return result\n",
    "\n",
    "    def customer_visited(self, customer):\n",
    "        self.customer_reward[customer] = 0\n",
    "\n",
    "    def all_customers_visited(self):\n",
    "        return self.calculate_customers_reward() == 0\n",
    "\n",
    "    def calculate_customers_reward(self):\n",
    "        sum = 0\n",
    "        for value in self.customer_reward.values():\n",
    "            sum += value\n",
    "        return sum\n",
    "\n",
    "    def modulate_reward(self):\n",
    "        number_of_customers = len(self.map) - 1\n",
    "        number_per_consultant = int(number_of_customers / 2)\n",
    "        self.customer_reward = {\n",
    "            'S': 0\n",
    "        }\n",
    "        for customer_nr in range(1, number_of_customers + 1):\n",
    "            self.customer_reward[int_to_state_name(customer_nr)] = 0\n",
    "\n",
    "        # every consultant only visits a few random customers\n",
    "        samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
    "        key_list = list(self.customer_reward.keys())\n",
    "        for sample in samples:\n",
    "            self.customer_reward[key_list[sample]] = 1000\n",
    "\n",
    "    def reset(self):\n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "        self.state = 'S'\n",
    "        self.cost = 0\n",
    "        self.action=0\n",
    "        self.lastStep_state = ''\n",
    "        self.customerReward = 0\n",
    "        self.reward = 0\n",
    "        self.envEpisodeCount += 1\n",
    "        self.modulate_reward()\n",
    "        return self.getObservation(state_name_to_int(self.state))\n",
    "\n",
    "    def _render_matplotlib(self):\n",
    "        '''Renders the current state as a graph with matplotlib\n",
    "        '''\n",
    "        G = nx.Graph()\n",
    "        node_colors=[]\n",
    "        node_sizes=[]\n",
    "        for node in self.map.keys():\n",
    "            size = 0\n",
    "            color = 'black'\n",
    "            if self.customer_reward[node] > 0:\n",
    "                color ='gold'\n",
    "                size =400\n",
    "            else: \n",
    "                color= 'darkgoldenrod'\n",
    "                size = 200\n",
    "            if self.state == node:\n",
    "                color = 'fuchsia'\n",
    "                size = 1000\n",
    "            node_sizes.append(size)\n",
    "            node_colors.append( color )\n",
    "            G.add_node(node)    \n",
    "        for source, connections in self.map.items():\n",
    "            for target, cost in connections:\n",
    "                if cost >= 300:\n",
    "                    color='dodgerblue'\n",
    "                elif cost >= 200:\n",
    "                    color='darkgoldenrod'\n",
    "                elif cost >= 100:\n",
    "                    color='forestgreen'\n",
    "                else:\n",
    "                    color='greenyellow'\n",
    "                G.add_edge(source,target,color=color,weight=(400-cost)/50 + 1)        \n",
    "        edges = G.edges()\n",
    "        edge_colors = [G[u][v]['color'] for u,v in edges]\n",
    "        edge_weights = [G[u][v]['weight'] for u,v in edges]        \n",
    "        pos = nx.planar_layout(G)\n",
    "        nx.draw(G, pos = pos, node_color=node_colors, node_size=node_sizes, \n",
    "                edges=edges, edge_color=edge_colors, width=edge_weights, with_labels=True)\n",
    "        \n",
    "    def _render_ansi(self):\n",
    "        result=(\"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) +\n",
    "                  \" Step: \" + (\"%4.0f  \" % self.stepCount) +\n",
    "                  self.lastStep_state + ' --' + str(self.action) + '-> ' + self.state +\n",
    "                  ' R=' + (\"% 2.2f\" % self.reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) +\n",
    "                  ' cost=' + (\"%4.0f\" % self.cost) + ' customerR=' + (\"%4.0f\" % self.customerReward) + ' optimum=' + (\n",
    "                              \"%4.0f\" % self.optimum)\n",
    "                )\n",
    "        return result\n",
    "    \n",
    "    def _render_rgb(self):\n",
    "        figure = plt.figure(1)\n",
    "        figure.add_subplot ( 111 )\n",
    "        plt.clf()\n",
    "        self._render_matplotlib()\n",
    "        figure.canvas.draw()\n",
    "        buf = figure.canvas.tostring_rgb()\n",
    "        ncols, nrows = figure.canvas.get_width_height()\n",
    "        result = np.fromstring(buf, dtype=np.uint8).reshape(nrows, ncols, 3)\n",
    "        return result\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'ansi':\n",
    "            return self._render_ansi()\n",
    "        elif mode == 'human':\n",
    "            return self._render_matplotlib()\n",
    "        elif mode == 'rgb_array':\n",
    "            return self._render_rgb()\n",
    "        else:\n",
    "            super().render(mode=mode)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Render Environment\n",
    "\n",
    "A way to view the visualization: \n",
    "\n",
    "Orso - the bear - lives in its cave (position 'S'). Orso his area and where he can \n",
    "typically find some honey (positions 'A' to 'O'). The honey places are scattered all over his home turf and connected\n",
    "by some pathways. Some connections are easy like walking over a grassy field (light green, very low cost) or forests \n",
    "(dark green, low cost), a bit strenuous like walking over a hill (brown, higher costs) and very arduous like swimming \n",
    "through a like (blue, very high costs).\n",
    "\n",
    "There are other bears around competing for the honey pots. So some spots have honey (yellow nodes) and some\n",
    "are already taken (brown nodes).\n",
    "\n",
    "Every morning, Orso awakes and starts looking for some honey. But every day, the honey pots are at a new place\n",
    "( env.reset() ). So Orso starts to search. At each spot he has to decide where to go next (the action).\n",
    "Of course Orsos goal is to get as much honey as possible, at the lowest costs. His daily journey ends when his back in\n",
    "his own cave.\n",
    "\n",
    "Be god and create Orso's world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "b = BeraterEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A new morning starts (rerun the cell below to see how the honey pots change places). But Orso \n",
    "(depicted in 'fuchsia', Marsha's favorite color ?) doesn't know \n",
    "about the sweetness of honey or his surroundings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b.reset()\n",
    "b.render() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Take Orso by his paws and go for a few steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b.step(1)\n",
    "print(b.render(mode='ansi'))\n",
    "b.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b.step(2)\n",
    "print(b.render(mode='ansi'))\n",
    "b.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the honey pot nodes turn brown once Orso has passed and taken the honey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b.step(3)\n",
    "print(b.render(mode='ansi'))\n",
    "b.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b.step(1)\n",
    "print(b.render(mode='ansi'))\n",
    "b.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYaTAvAyYO-U"
   },
   "source": [
    "Now Orso must start to learn by himself to take actions, memorize the geography of its neighbourhood and \n",
    "to look for honey while minimizing the effort:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train policy with tfagents PpoAgent\n",
    "\n",
    "### Register with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if not 'isEnvRegistered' in locals():\n",
    "  env_name=\"Berater-v1\"\n",
    "  gym.envs.registration.register(id=env_name,entry_point=BeraterEnv,max_episode_steps=1000)\n",
    "  isEnvRegistered=True\n",
    "  print(\"Berater registered as '\" + env_name + \"'\")\n",
    "else:\n",
    "  print(\"Already registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dry run (short training, no logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDurationFast\n",
    "from easyagents.config import LoggingSilent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent = PpoAgent(    gym_env_name = 'Berater-v1',                        \n",
    "                        training_duration=TrainingDurationFast(max_steps_per_episode = 50),\n",
    "                        logging=LoggingSilent())\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some training & logging (default, on custom network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent = PpoAgent( gym_env_name = 'Berater-v1', fc_layers=(500,500,500), \n",
    "                     training_duration=TrainingDuration(max_steps_per_episode = 50) )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.render_episodes(num_episodes=1,mode='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML( ppoAgent.render_episodes_to_html(num_episodes=1, fps=10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training (duration, learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "training_duration=TrainingDuration( num_iterations = 5000,\n",
    "                                    num_episodes_per_iteration = 10,\n",
    "                                    max_steps_per_episode = 50,\n",
    "                                    num_epochs_per_iteration = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent = PpoAgent(    gym_env_name = 'Berater-v1',\n",
    "                        fc_layers=(500,500,500), \n",
    "                        training_duration=training_duration,\n",
    "                        learning_rate=1e-4\n",
    "                   )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize (with custom y-limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_rewards(ylim=[-2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent.plot_average_steps(ylim=[0,210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent.plot_losses(ylim=[0,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML( ppoAgent.render_episodes_to_html(num_episodes=1, fps=1) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zpzHtN3-kQ26",
    "w3OdHyWEEEwy",
    "bzoq0VM85p46"
   ],
   "name": "easyagents_berater.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}