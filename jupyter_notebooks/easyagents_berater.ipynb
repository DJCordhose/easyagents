{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU7ylMh1kQ2y"
   },
   "source": [
    "# Berater Environment v13\n",
    "\n",
    "## Changes from v12 (work in progress)\n",
    "* migration to easyagents\n",
    "* Max. number of steps management and debug logging migrated to easyagents.TrainingDuration\n",
    "* render yields the last step, supporting modes 'ansi' and 'human', "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Install gym, tensorflow, tf-agents,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "You are using pip version 19.0.3, however version 19.1.1 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "!pip install easyagents > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3OdHyWEEEwy"
   },
   "source": [
    "# Define Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQyb_Aq8Kg9j",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "from gym import spaces\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsJ6zcXvwN53"
   },
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-S4sZG5ZkQ3T",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def state_name_to_int(state):\n",
    "    state_name_map = {\n",
    "        'S': 0,\n",
    "        'A': 1,\n",
    "        'B': 2,\n",
    "        'C': 3,\n",
    "        'D': 4,\n",
    "        'E': 5,\n",
    "        'F': 6,\n",
    "        'G': 7,\n",
    "        'H': 8,\n",
    "        'K': 9,\n",
    "        'L': 10,\n",
    "        'M': 11,\n",
    "        'N': 12,\n",
    "        'O': 13\n",
    "    }\n",
    "    return state_name_map[state]\n",
    "\n",
    "def int_to_state_name(state_as_int):\n",
    "    state_map = {\n",
    "        0: 'S',\n",
    "        1: 'A',\n",
    "        2: 'B',\n",
    "        3: 'C',\n",
    "        4: 'D',\n",
    "        5: 'E',\n",
    "        6: 'F',\n",
    "        7: 'G',\n",
    "        8: 'H',\n",
    "        9: 'K',\n",
    "        10: 'L',\n",
    "        11: 'M',\n",
    "        12: 'N',\n",
    "        13: 'O'\n",
    "    }\n",
    "    return state_map[state_as_int]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-olom0nwiSX"
   },
   "source": [
    "### Berater Environment (OpenAI Gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3plH2u3Swotj",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class BeraterEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    The Berater Problem\n",
    "\n",
    "    Actions:\n",
    "    There are 4 discrete deterministic actions, each choosing one direction\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['ansi']}\n",
    "    showStep = False\n",
    "\n",
    "    def __init__(self):\n",
    "        #         self.map = {\n",
    "        #             'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
    "        #             'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
    "        #             'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
    "        #             'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
    "        #         }\n",
    "        self.map = {\n",
    "            'S': [('A', 300), ('B', 100), ('C', 200)],\n",
    "            'A': [('S', 300), ('B', 100), ('E', 100), ('D', 100)],\n",
    "            'B': [('S', 100), ('A', 100), ('C', 50), ('K', 200)],\n",
    "            'C': [('S', 200), ('B', 50), ('M', 100), ('L', 200)],\n",
    "            'D': [('A', 100), ('F', 50)],\n",
    "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
    "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
    "            'G': [('F', 200), ('O', 300)],\n",
    "            'H': [('E', 100), ('K', 300)],\n",
    "            'K': [('B', 200), ('H', 300)],\n",
    "            'L': [('C', 200), ('M', 50)],\n",
    "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
    "            'N': [('M', 100), ('O', 100)],\n",
    "            'O': [('N', 100), ('G', 300)]\n",
    "        }\n",
    "        max_paths = 4\n",
    "        self.action_space = spaces.Discrete(max_paths)\n",
    "\n",
    "        positions = len(self.map)\n",
    "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
    "        # non existing path is -1000 and no position change\n",
    "        # look at what #getObservation returns if you are confused\n",
    "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
    "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
    "        self.observation_space = spaces.Box(low=low,\n",
    "                                            high=high,\n",
    "                                            dtype=np.float32)\n",
    "        self.reward_range = (-1, 1)\n",
    "        self.envEpisodeCount = 0\n",
    "        self.envStepCount = 0\n",
    "\n",
    "        self.reset()\n",
    "        self.optimum = self.calculate_customers_reward()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def iterate_path(self, state, action):\n",
    "        paths = self.map[state]\n",
    "        if action < len(paths):\n",
    "            return paths[action]\n",
    "        else:\n",
    "            # sorry, no such action, stay where you are and pay a high penalty\n",
    "            return (state, 1000)\n",
    "\n",
    "    def step(self, action):\n",
    "        destination, cost = self.iterate_path(self.state, action)\n",
    "\n",
    "        self.cost = cost\n",
    "        self.action=action\n",
    "        self.lastStep_state = self.state\n",
    "        self.state = destination\n",
    "        self.customerReward = self.customer_reward[destination]\n",
    "        self.reward = 0\n",
    "        self.reward = (self.customerReward - self.cost) / self.optimum\n",
    "\n",
    "        self.customer_visited(destination)\n",
    "        done = (destination == 'S' and self.all_customers_visited())\n",
    "\n",
    "        stateAsInt = state_name_to_int(self.state)\n",
    "        self.totalReward += self.reward\n",
    "        self.stepCount += 1\n",
    "        self.envStepCount += 1\n",
    "\n",
    "        if done and not self.isDone:\n",
    "            self.envEpisodeCount += 1\n",
    "\n",
    "        self.isDone = done\n",
    "        observation = self.getObservation(stateAsInt)\n",
    "        info = {\"from\": self.state, \"to\": destination}\n",
    "        return observation, self.reward, done, info\n",
    "\n",
    "    def getObservation(self, position):\n",
    "        result = np.array([position,\n",
    "                           self.getPathObservation(position, 0),\n",
    "                           self.getPathObservation(position, 1),\n",
    "                           self.getPathObservation(position, 2),\n",
    "                           self.getPathObservation(position, 3)\n",
    "                           ],\n",
    "                          dtype=np.float32)\n",
    "        all_rest_rewards = list(self.customer_reward.values())\n",
    "        result = np.append(result, all_rest_rewards)\n",
    "        return result\n",
    "\n",
    "    def getPathObservation(self, position, path):\n",
    "        paths = self.map[self.state]\n",
    "        if path < len(paths):\n",
    "            target, cost = paths[path]\n",
    "            reward = self.customer_reward[target]\n",
    "            result = reward - cost\n",
    "        else:\n",
    "            result = -1000\n",
    "\n",
    "        return result\n",
    "\n",
    "    def customer_visited(self, customer):\n",
    "        self.customer_reward[customer] = 0\n",
    "\n",
    "    def all_customers_visited(self):\n",
    "        return self.calculate_customers_reward() == 0\n",
    "\n",
    "    def calculate_customers_reward(self):\n",
    "        sum = 0\n",
    "        for value in self.customer_reward.values():\n",
    "            sum += value\n",
    "        return sum\n",
    "\n",
    "    def modulate_reward(self):\n",
    "        number_of_customers = len(self.map) - 1\n",
    "        number_per_consultant = int(number_of_customers / 2)\n",
    "        self.customer_reward = {\n",
    "            'S': 0\n",
    "        }\n",
    "        for customer_nr in range(1, number_of_customers + 1):\n",
    "            self.customer_reward[int_to_state_name(customer_nr)] = 0\n",
    "\n",
    "        # every consultant only visits a few random customers\n",
    "        samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
    "        key_list = list(self.customer_reward.keys())\n",
    "        for sample in samples:\n",
    "            self.customer_reward[key_list[sample]] = 1000\n",
    "\n",
    "    def reset(self):\n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "        self.state = ''\n",
    "        self.cost = 0\n",
    "        self.action=0\n",
    "        self.lastStep_state = ''\n",
    "        self.customerReward = 0\n",
    "        self.reward = 0\n",
    "\n",
    "        self.envEpisodeCount += 1\n",
    "\n",
    "        self.modulate_reward()\n",
    "        self.state = 'S'\n",
    "        return self.getObservation(state_name_to_int(self.state))\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        msg=(\"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) +\n",
    "              \" Step: \" + (\"%4.0f  \" % self.stepCount) +\n",
    "              self.lastStep_state + ' --' + str(self.action) + '-> ' + self.state +\n",
    "              ' R=' + (\"% 2.2f\" % self.reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) +\n",
    "              ' cost=' + (\"%4.0f\" % self.cost) + ' customerR=' + (\"%4.0f\" % self.customerReward) + ' optimum=' + (\n",
    "                          \"%4.0f\" % self.optimum)\n",
    "             )\n",
    "        if mode == 'ansi':\n",
    "            return msg\n",
    "        elif mode is 'human':\n",
    "            print(msg)\n",
    "        else:\n",
    "            super().render(mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYaTAvAyYO-U"
   },
   "source": [
    "### Register with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Berater registered as 'Berater-v1'\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "if not 'isEnvRegistered' in locals():\n",
    "  env_name=\"Berater-v1\"\n",
    "  gym.envs.registration.register(id=env_name,entry_point=BeraterEnv,max_episode_steps=1000)\n",
    "  isEnvRegistered=True\n",
    "  print(\"Berater registered as '\" + env_name + \"'\")\n",
    "else:\n",
    "  print(\"Already registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sX8eJGcbOJ30"
   },
   "source": [
    "# Train policy with tfagents PpoAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzoq0VM85p46"
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "You are using pip version 19.0.3, however version 19.1.1 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "!pip install easyagents > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !apt-get install xvfb >/dev/null\n",
    "    !pip install pyvirtualdisplay >/dev/null    \n",
    "    \n",
    "    from pyvirtualdisplay import Display\n",
    "    Display(visible=0, size=(960, 720)).start()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dry run (short training, no logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDurationFast\n",
    "from easyagents.config import LoggingSilent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\nW0707 23:22:40.234194 15896 deprecation.py:323] From c:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1221: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0707 23:22:40.371163 15896 deprecation.py:323] From c:\\dev\\github\\easyagents\\env\\lib\\site-packages\\tensorflow_core\\python\\training\\optimizer.py:172: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nApply a constraint manually following the optimizer update step.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "ppoAgent = PpoAgent(    gym_env_name = 'Berater-v1',                        \n",
    "                        training_duration=TrainingDurationFast(),\n",
    "                        logging=LoggingSilent())\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_average_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_average_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default training & logging (on custom network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "I0707 23:22:50.957328 15896 agents.py:92] PpoAgent on Berater-v1 [fc_layers=(500, 500, 500), learning_rate=0.001]\n",
      "I0707 23:22:50.958327 15896 agents.py:92] TrainingDuration 250=25*10 episodes [max 500 steps/episode, 5 epochs/iteration, policy eval every 50=5*10 episodes]\n",
      "I0707 23:23:13.618320 15896 agents.py:92] current policy       : avg_reward=-7.319, avg_steps=500.000\n",
      "I0707 23:23:19.811322 15896 agents.py:92] training    1 of 25  : completed tf_agent.train(...) = 3225.138 [loss]\n",
      "I0707 23:23:24.546323 15896 agents.py:92] training    2 of 25  : completed tf_agent.train(...) = 7136.351 [loss]\n",
      "I0707 23:23:29.705323 15896 agents.py:92] training    3 of 25  : completed tf_agent.train(...) = 5196.691 [loss]\n",
      "I0707 23:23:31.811322 15896 agents.py:92] training    4 of 25  : completed tf_agent.train(...) = 1630.860 [loss]\n",
      "I0707 23:23:34.288321 15896 agents.py:92] training    5 of 25  : completed tf_agent.train(...) = 1484.802 [loss]\n",
      "I0707 23:23:55.666357 15896 agents.py:92] current policy       : avg_reward=-23.140, avg_steps=500.000\n",
      "I0707 23:23:57.739325 15896 agents.py:92] training    6 of 25  : completed tf_agent.train(...) = 1319.945 [loss]\n",
      "I0707 23:24:00.056323 15896 agents.py:92] training    7 of 25  : completed tf_agent.train(...) = 1236.951 [loss]\n",
      "I0707 23:24:02.510199 15896 agents.py:92] training    8 of 25  : completed tf_agent.train(...) =  787.547 [loss]\n",
      "I0707 23:24:05.025200 15896 agents.py:92] training    9 of 25  : completed tf_agent.train(...) =  900.805 [loss]\n",
      "I0707 23:24:07.521198 15896 agents.py:92] training   10 of 25  : completed tf_agent.train(...) =  485.356 [loss]\n",
      "I0707 23:24:28.819200 15896 agents.py:92] current policy       : avg_reward=-8.723, avg_steps=500.000\n",
      "I0707 23:24:30.978199 15896 agents.py:92] training   11 of 25  : completed tf_agent.train(...) =  494.623 [loss]\n",
      "I0707 23:24:33.374204 15896 agents.py:92] training   12 of 25  : completed tf_agent.train(...) =  524.706 [loss]\n",
      "I0707 23:24:35.281200 15896 agents.py:92] training   13 of 25  : completed tf_agent.train(...) =  271.112 [loss]\n",
      "I0707 23:24:37.343201 15896 agents.py:92] training   14 of 25  : completed tf_agent.train(...) =  179.369 [loss]\n",
      "I0707 23:24:39.714354 15896 agents.py:92] training   15 of 25  : completed tf_agent.train(...) =  805.190 [loss]\n",
      "I0707 23:25:01.413640 15896 agents.py:92] current policy       : avg_reward=-10.491, avg_steps=500.000\n",
      "I0707 23:25:03.128641 15896 agents.py:92] training   16 of 25  : completed tf_agent.train(...) =  326.755 [loss]\n",
      "I0707 23:25:05.143640 15896 agents.py:92] training   17 of 25  : completed tf_agent.train(...) =  330.744 [loss]\n",
      "I0707 23:25:06.859640 15896 agents.py:92] training   18 of 25  : completed tf_agent.train(...) =  141.756 [loss]\n",
      "I0707 23:25:08.398639 15896 agents.py:92] training   19 of 25  : completed tf_agent.train(...) =  175.550 [loss]\n",
      "I0707 23:25:10.481641 15896 agents.py:92] training   20 of 25  : completed tf_agent.train(...) =  291.113 [loss]\n",
      "I0707 23:25:32.876639 15896 agents.py:92] current policy       : avg_reward=-7.108, avg_steps=500.000\n",
      "I0707 23:25:35.050640 15896 agents.py:92] training   21 of 25  : completed tf_agent.train(...) =  264.657 [loss]\n",
      "I0707 23:25:37.082639 15896 agents.py:92] training   22 of 25  : completed tf_agent.train(...) =  217.608 [loss]\n",
      "I0707 23:25:38.849640 15896 agents.py:92] training   23 of 25  : completed tf_agent.train(...) =  150.992 [loss]\n",
      "I0707 23:25:40.696639 15896 agents.py:92] training   24 of 25  : completed tf_agent.train(...) =  241.611 [loss]\n",
      "I0707 23:25:42.392640 15896 agents.py:92] training   25 of 25  : completed tf_agent.train(...) =   83.106 [loss]\n",
      "I0707 23:26:03.869261 15896 agents.py:92] current policy       : avg_reward=-7.114, avg_steps=500.000\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "ppoAgent = PpoAgent( gym_env_name = 'Berater-v1', fc_layers=(500,500,500) )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_average_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_average_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training (duration, learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import TrainingDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "training_duration=TrainingDuration( num_iterations = 1500,\n",
    "                                    num_episodes_per_iteration = 10,\n",
    "                                    max_steps_per_episode = 200,\n",
    "                                    num_epochs_per_iteration = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "I0707 23:26:04.313268 15896 agents.py:92] PpoAgent on Berater-v1 [fc_layers=(500, 500, 500), learning_rate=0.0001]\n",
      "I0707 23:26:04.315270 15896 agents.py:92] TrainingDuration 15000=1500*10 episodes [max 200 steps/episode, 5 epochs/iteration, policy eval every 50=5*10 episodes]\n",
      "I0707 23:26:13.625154 15896 agents.py:92] current policy       : avg_reward=-3.120, avg_steps=200.000\n",
      "I0707 23:26:19.857111 15896 agents.py:92] training    1 of 1500: completed tf_agent.train(...) = 4263.512 [loss]\n",
      "I0707 23:26:23.715109 15896 agents.py:92] training    2 of 1500: completed tf_agent.train(...) = 6305.006 [loss]\n",
      "I0707 23:26:28.373997 15896 agents.py:92] training    3 of 1500: completed tf_agent.train(...) = 4780.696 [loss]\n",
      "I0707 23:26:30.700997 15896 agents.py:92] training    4 of 1500: completed tf_agent.train(...) = 6012.851 [loss]\n",
      "I0707 23:26:32.871996 15896 agents.py:92] training    5 of 1500: completed tf_agent.train(...) = 3148.872 [loss]\n",
      "I0707 23:26:41.939996 15896 agents.py:92] current policy       : avg_reward=-3.250, avg_steps=200.000\n",
      "I0707 23:26:44.152004 15896 agents.py:92] training    6 of 1500: completed tf_agent.train(...) = 4495.249 [loss]\n",
      "I0707 23:26:46.312997 15896 agents.py:92] training    7 of 1500: completed tf_agent.train(...) = 4386.099 [loss]\n",
      "I0707 23:26:48.524996 15896 agents.py:92] training    8 of 1500: completed tf_agent.train(...) = 3964.764 [loss]\n",
      "I0707 23:26:50.841999 15896 agents.py:92] training    9 of 1500: completed tf_agent.train(...) = 3575.203 [loss]\n",
      "I0707 23:26:53.041997 15896 agents.py:92] training   10 of 1500: completed tf_agent.train(...) = 3071.078 [loss]\n",
      "I0707 23:27:02.141032 15896 agents.py:92] current policy       : avg_reward=-3.170, avg_steps=200.000\n",
      "I0707 23:27:04.369996 15896 agents.py:92] training   11 of 1500: completed tf_agent.train(...) = 1958.884 [loss]\n",
      "I0707 23:27:06.487997 15896 agents.py:92] training   12 of 1500: completed tf_agent.train(...) = 2264.662 [loss]\n",
      "I0707 23:27:08.923997 15896 agents.py:92] training   13 of 1500: completed tf_agent.train(...) = 2518.325 [loss]\n",
      "I0707 23:27:10.849998 15896 agents.py:92] training   14 of 1500: completed tf_agent.train(...) = 1180.805 [loss]\n",
      "I0707 23:27:13.149998 15896 agents.py:92] training   15 of 1500: completed tf_agent.train(...) = 1576.420 [loss]\n",
      "I0707 23:27:21.774035 15896 agents.py:92] current policy       : avg_reward=-3.867, avg_steps=200.000\n",
      "I0707 23:27:23.867998 15896 agents.py:92] training   16 of 1500: completed tf_agent.train(...) =  923.505 [loss]\n",
      "I0707 23:27:25.871663 15896 agents.py:92] training   17 of 1500: completed tf_agent.train(...) =  866.694 [loss]\n",
      "I0707 23:27:28.033662 15896 agents.py:92] training   18 of 1500: completed tf_agent.train(...) =  721.819 [loss]\n",
      "I0707 23:27:29.969661 15896 agents.py:92] training   19 of 1500: completed tf_agent.train(...) =  151.195 [loss]\n",
      "I0707 23:27:32.036660 15896 agents.py:92] training   20 of 1500: completed tf_agent.train(...) =  495.238 [loss]\n",
      "I0707 23:27:40.754690 15896 agents.py:92] current policy       : avg_reward=-3.167, avg_steps=200.000\n",
      "I0707 23:27:42.998660 15896 agents.py:92] training   21 of 1500: completed tf_agent.train(...) =  421.823 [loss]\n",
      "I0707 23:27:44.851517 15896 agents.py:92] training   22 of 1500: completed tf_agent.train(...) =  184.702 [loss]\n",
      "I0707 23:27:46.870672 15896 agents.py:92] training   23 of 1500: completed tf_agent.train(...) =  197.997 [loss]\n",
      "I0707 23:27:48.690673 15896 agents.py:92] training   24 of 1500: completed tf_agent.train(...) =  138.190 [loss]\n",
      "I0707 23:27:50.589670 15896 agents.py:92] training   25 of 1500: completed tf_agent.train(...) =  157.906 [loss]\n",
      "I0707 23:27:59.189706 15896 agents.py:92] current policy       : avg_reward=-2.917, avg_steps=200.000\n",
      "I0707 23:28:01.057672 15896 agents.py:92] training   26 of 1500: completed tf_agent.train(...) =  116.293 [loss]\n",
      "I0707 23:28:03.015669 15896 agents.py:92] training   27 of 1500: completed tf_agent.train(...) =  159.777 [loss]\n",
      "I0707 23:28:04.837673 15896 agents.py:92] training   28 of 1500: completed tf_agent.train(...) =  149.964 [loss]\n",
      "I0707 23:28:06.559671 15896 agents.py:92] training   29 of 1500: completed tf_agent.train(...) =  303.305 [loss]\n",
      "I0707 23:28:08.270670 15896 agents.py:92] training   30 of 1500: completed tf_agent.train(...) =  167.328 [loss]\n",
      "I0707 23:28:16.890705 15896 agents.py:92] current policy       : avg_reward=-5.047, avg_steps=200.000\n",
      "I0707 23:28:18.692671 15896 agents.py:92] training   31 of 1500: completed tf_agent.train(...) =  127.770 [loss]\n",
      "I0707 23:28:20.486670 15896 agents.py:92] training   32 of 1500: completed tf_agent.train(...) =   96.289 [loss]\n",
      "I0707 23:28:22.315670 15896 agents.py:92] training   33 of 1500: completed tf_agent.train(...) =   83.053 [loss]\n",
      "I0707 23:28:24.074670 15896 agents.py:92] training   34 of 1500: completed tf_agent.train(...) =  120.802 [loss]\n",
      "I0707 23:28:25.618671 15896 agents.py:92] training   35 of 1500: completed tf_agent.train(...) =  132.967 [loss]\n",
      "I0707 23:28:34.212707 15896 agents.py:92] current policy       : avg_reward=-2.284, avg_steps=200.000\n",
      "I0707 23:28:35.939671 15896 agents.py:92] training   36 of 1500: completed tf_agent.train(...) =  114.121 [loss]\n",
      "I0707 23:28:37.588673 15896 agents.py:92] training   37 of 1500: completed tf_agent.train(...) =  110.323 [loss]\n",
      "I0707 23:28:39.191674 15896 agents.py:92] training   38 of 1500: completed tf_agent.train(...) =   74.938 [loss]\n",
      "I0707 23:28:40.951672 15896 agents.py:92] training   39 of 1500: completed tf_agent.train(...) =   43.833 [loss]\n",
      "I0707 23:28:42.716734 15896 agents.py:92] training   40 of 1500: completed tf_agent.train(...) =   93.483 [loss]\n",
      "I0707 23:28:51.289785 15896 agents.py:92] current policy       : avg_reward=-2.004, avg_steps=200.000\n",
      "I0707 23:28:53.138504 15896 agents.py:92] training   41 of 1500: completed tf_agent.train(...) =  107.805 [loss]\n",
      "I0707 23:28:54.774503 15896 agents.py:92] training   42 of 1500: completed tf_agent.train(...) =   61.556 [loss]\n",
      "I0707 23:28:56.623504 15896 agents.py:92] training   43 of 1500: completed tf_agent.train(...) =  504.661 [loss]\n",
      "I0707 23:28:57.022505 15896 agents.py:92] training   44 of 1500: completed tf_agent.train(...) =  234.009 [loss]\n",
      "I0707 23:28:58.646504 15896 agents.py:92] training   45 of 1500: completed tf_agent.train(...) =   96.713 [loss]\n",
      "I0707 23:29:07.172587 15896 agents.py:92] current policy       : avg_reward=-2.514, avg_steps=200.000\n",
      "I0707 23:29:08.878504 15896 agents.py:92] training   46 of 1500: completed tf_agent.train(...) =  106.080 [loss]\n",
      "I0707 23:29:10.516504 15896 agents.py:92] training   47 of 1500: completed tf_agent.train(...) =   80.091 [loss]\n",
      "I0707 23:29:12.178506 15896 agents.py:92] training   48 of 1500: completed tf_agent.train(...) =   44.113 [loss]\n",
      "I0707 23:29:13.992506 15896 agents.py:92] training   49 of 1500: completed tf_agent.train(...) =   97.652 [loss]\n",
      "I0707 23:29:15.572503 15896 agents.py:92] training   50 of 1500: completed tf_agent.train(...) =   68.184 [loss]\n",
      "I0707 23:29:24.140890 15896 agents.py:92] current policy       : avg_reward=-3.612, avg_steps=200.000\n",
      "I0707 23:29:25.625892 15896 agents.py:92] training   51 of 1500: completed tf_agent.train(...) =   88.254 [loss]\n",
      "I0707 23:29:27.204891 15896 agents.py:92] training   52 of 1500: completed tf_agent.train(...) =   57.973 [loss]\n",
      "I0707 23:29:27.654891 15896 agents.py:92] training   53 of 1500: completed tf_agent.train(...) =   46.175 [loss]\n",
      "I0707 23:29:29.210890 15896 agents.py:92] training   54 of 1500: completed tf_agent.train(...) =   60.760 [loss]\n",
      "I0707 23:29:30.741890 15896 agents.py:92] training   55 of 1500: completed tf_agent.train(...) =   63.202 [loss]\n",
      "I0707 23:29:39.366889 15896 agents.py:92] current policy       : avg_reward=-3.341, avg_steps=200.000\n",
      "I0707 23:29:40.940915 15896 agents.py:92] training   56 of 1500: completed tf_agent.train(...) =   47.294 [loss]\n",
      "I0707 23:29:42.526287 15896 agents.py:92] training   57 of 1500: completed tf_agent.train(...) =   36.929 [loss]\n",
      "I0707 23:29:44.148195 15896 agents.py:92] training   58 of 1500: completed tf_agent.train(...) =   59.265 [loss]\n",
      "I0707 23:29:45.731900 15896 agents.py:92] training   59 of 1500: completed tf_agent.train(...) =   94.012 [loss]\n",
      "I0707 23:29:47.324899 15896 agents.py:92] training   60 of 1500: completed tf_agent.train(...) =   56.634 [loss]\n",
      "I0707 23:29:51.939900 15896 agents.py:92] current policy       : avg_reward=-0.708, avg_steps=105.500\n",
      "I0707 23:29:53.569899 15896 agents.py:92] training   61 of 1500: completed tf_agent.train(...) =   44.060 [loss]\n",
      "I0707 23:29:55.419900 15896 agents.py:92] training   62 of 1500: completed tf_agent.train(...) =   69.229 [loss]\n",
      "I0707 23:29:57.045380 15896 agents.py:92] training   63 of 1500: completed tf_agent.train(...) =  106.353 [loss]\n",
      "I0707 23:29:58.874653 15896 agents.py:92] training   64 of 1500: completed tf_agent.train(...) =   98.974 [loss]\n",
      "I0707 23:30:00.507280 15896 agents.py:92] training   65 of 1500: completed tf_agent.train(...) =   43.003 [loss]\n",
      "I0707 23:30:08.199758 15896 agents.py:92] current policy       : avg_reward=-4.268, avg_steps=181.200\n",
      "I0707 23:30:08.580729 15896 agents.py:92] training   66 of 1500: completed tf_agent.train(...) =   46.744 [loss]\n",
      "I0707 23:30:10.203729 15896 agents.py:92] training   67 of 1500: completed tf_agent.train(...) =   43.687 [loss]\n",
      "I0707 23:30:11.842728 15896 agents.py:92] training   68 of 1500: completed tf_agent.train(...) =   87.375 [loss]\n",
      "I0707 23:30:13.507731 15896 agents.py:92] training   69 of 1500: completed tf_agent.train(...) =   56.175 [loss]\n",
      "I0707 23:30:13.977731 15896 agents.py:92] training   70 of 1500: completed tf_agent.train(...) =   55.997 [loss]\n",
      "I0707 23:30:22.558732 15896 agents.py:92] current policy       : avg_reward=-3.483, avg_steps=200.000\n",
      "I0707 23:30:24.277727 15896 agents.py:92] training   71 of 1500: completed tf_agent.train(...) =   43.067 [loss]\n",
      "I0707 23:30:25.796743 15896 agents.py:92] training   72 of 1500: completed tf_agent.train(...) =   70.323 [loss]\n",
      "I0707 23:30:27.315805 15896 agents.py:92] training   73 of 1500: completed tf_agent.train(...) =   35.617 [loss]\n",
      "I0707 23:30:28.777029 15896 agents.py:92] training   74 of 1500: completed tf_agent.train(...) =   65.442 [loss]\n",
      "I0707 23:30:30.349030 15896 agents.py:92] training   75 of 1500: completed tf_agent.train(...) =   31.032 [loss]\n",
      "I0707 23:30:36.761222 15896 agents.py:92] current policy       : avg_reward=-1.237, avg_steps=143.800\n",
      "I0707 23:30:38.303191 15896 agents.py:92] training   76 of 1500: completed tf_agent.train(...) =   33.989 [loss]\n",
      "I0707 23:30:39.891350 15896 agents.py:92] training   77 of 1500: completed tf_agent.train(...) =   31.166 [loss]\n",
      "I0707 23:30:41.549858 15896 agents.py:92] training   78 of 1500: completed tf_agent.train(...) =   29.088 [loss]\n",
      "I0707 23:30:43.019860 15896 agents.py:92] training   79 of 1500: completed tf_agent.train(...) =   47.110 [loss]\n",
      "I0707 23:30:43.418859 15896 agents.py:92] training   80 of 1500: completed tf_agent.train(...) =   26.750 [loss]\n",
      "I0707 23:30:50.403894 15896 agents.py:92] current policy       : avg_reward=-1.752, avg_steps=162.900\n",
      "I0707 23:30:52.054857 15896 agents.py:92] training   81 of 1500: completed tf_agent.train(...) =   85.697 [loss]\n",
      "I0707 23:30:53.736858 15896 agents.py:92] training   82 of 1500: completed tf_agent.train(...) =  101.367 [loss]\n",
      "I0707 23:30:55.298860 15896 agents.py:92] training   83 of 1500: completed tf_agent.train(...) =   53.770 [loss]\n",
      "I0707 23:30:56.878859 15896 agents.py:92] training   84 of 1500: completed tf_agent.train(...) =   40.527 [loss]\n",
      "I0707 23:30:58.517857 15896 agents.py:92] training   85 of 1500: completed tf_agent.train(...) =   50.997 [loss]\n",
      "I0707 23:31:05.480890 15896 agents.py:92] current policy       : avg_reward=-1.271, avg_steps=162.300\n",
      "I0707 23:31:07.089055 15896 agents.py:92] training   86 of 1500: completed tf_agent.train(...) =   29.051 [loss]\n",
      "I0707 23:31:08.668828 15896 agents.py:92] training   87 of 1500: completed tf_agent.train(...) =   61.213 [loss]\n",
      "I0707 23:31:10.332827 15896 agents.py:92] training   88 of 1500: completed tf_agent.train(...) =   99.599 [loss]\n",
      "I0707 23:31:11.848828 15896 agents.py:92] training   89 of 1500: completed tf_agent.train(...) =   23.457 [loss]\n",
      "I0707 23:31:13.395827 15896 agents.py:92] training   90 of 1500: completed tf_agent.train(...) =   22.450 [loss]\n",
      "I0707 23:31:19.544828 15896 agents.py:92] current policy       : avg_reward=-1.730, avg_steps=143.300\n",
      "I0707 23:31:21.157828 15896 agents.py:92] training   91 of 1500: completed tf_agent.train(...) =   25.266 [loss]\n",
      "I0707 23:31:22.706833 15896 agents.py:92] training   92 of 1500: completed tf_agent.train(...) =   25.447 [loss]\n",
      "I0707 23:31:24.312454 15896 agents.py:92] training   93 of 1500: completed tf_agent.train(...) =   25.147 [loss]\n",
      "I0707 23:31:24.667976 15896 agents.py:92] training   94 of 1500: completed tf_agent.train(...) =   21.733 [loss]\n",
      "I0707 23:31:26.258109 15896 agents.py:92] training   95 of 1500: completed tf_agent.train(...) =   24.953 [loss]\n",
      "I0707 23:31:32.385144 15896 agents.py:92] current policy       : avg_reward=-1.212, avg_steps=143.900\n",
      "I0707 23:31:32.840110 15896 agents.py:92] training   96 of 1500: completed tf_agent.train(...) =   22.177 [loss]\n",
      "I0707 23:31:34.420108 15896 agents.py:92] training   97 of 1500: completed tf_agent.train(...) =   20.088 [loss]\n",
      "I0707 23:31:35.915111 15896 agents.py:92] training   98 of 1500: completed tf_agent.train(...) =   22.842 [loss]\n",
      "I0707 23:31:37.476109 15896 agents.py:92] training   99 of 1500: completed tf_agent.train(...) =   22.752 [loss]\n",
      "I0707 23:31:39.067109 15896 agents.py:92] training  100 of 1500: completed tf_agent.train(...) =   24.556 [loss]\n",
      "I0707 23:31:46.022644 15896 agents.py:92] current policy       : avg_reward=-1.679, avg_steps=162.500\n",
      "I0707 23:31:47.589609 15896 agents.py:92] training  101 of 1500: completed tf_agent.train(...) =   13.768 [loss]\n",
      "I0707 23:31:49.183607 15896 agents.py:92] training  102 of 1500: completed tf_agent.train(...) =   13.797 [loss]\n",
      "I0707 23:31:50.802608 15896 agents.py:92] training  103 of 1500: completed tf_agent.train(...) =   17.146 [loss]\n",
      "I0707 23:31:52.380608 15896 agents.py:92] training  104 of 1500: completed tf_agent.train(...) =   16.379 [loss]\n",
      "I0707 23:31:53.888607 15896 agents.py:92] training  105 of 1500: completed tf_agent.train(...) =   23.411 [loss]\n",
      "I0707 23:32:00.145638 15896 agents.py:92] current policy       : avg_reward=-1.572, avg_steps=144.400\n",
      "I0707 23:32:01.708607 15896 agents.py:92] training  106 of 1500: completed tf_agent.train(...) =   20.557 [loss]\n",
      "I0707 23:32:03.301608 15896 agents.py:92] training  107 of 1500: completed tf_agent.train(...) =   17.115 [loss]\n",
      "I0707 23:32:04.967608 15896 agents.py:92] training  108 of 1500: completed tf_agent.train(...) =   20.677 [loss]\n",
      "I0707 23:32:05.312609 15896 agents.py:92] training  109 of 1500: completed tf_agent.train(...) =   12.642 [loss]\n",
      "I0707 23:32:06.868587 15896 agents.py:92] training  110 of 1500: completed tf_agent.train(...) =   14.056 [loss]\n",
      "I0707 23:32:13.842783 15896 agents.py:92] current policy       : avg_reward=-1.579, avg_steps=162.700\n",
      "I0707 23:32:15.425789 15896 agents.py:92] training  111 of 1500: completed tf_agent.train(...) =   16.463 [loss]\n",
      "I0707 23:32:17.049784 15896 agents.py:92] training  112 of 1500: completed tf_agent.train(...) =   34.226 [loss]\n",
      "I0707 23:32:18.815785 15896 agents.py:92] training  113 of 1500: completed tf_agent.train(...) =   61.776 [loss]\n",
      "I0707 23:32:19.117785 15896 agents.py:92] training  114 of 1500: completed tf_agent.train(...) =   15.593 [loss]\n",
      "I0707 23:32:20.722785 15896 agents.py:92] training  115 of 1500: completed tf_agent.train(...) =   13.884 [loss]\n",
      "I0707 23:32:29.344785 15896 agents.py:92] current policy       : avg_reward=-2.955, avg_steps=200.000\n",
      "I0707 23:32:31.009366 15896 agents.py:92] training  116 of 1500: completed tf_agent.train(...) =   23.721 [loss]\n",
      "I0707 23:32:32.631926 15896 agents.py:92] training  117 of 1500: completed tf_agent.train(...) =   21.190 [loss]\n",
      "I0707 23:32:34.252674 15896 agents.py:92] training  118 of 1500: completed tf_agent.train(...) =   10.514 [loss]\n",
      "I0707 23:32:35.839674 15896 agents.py:92] training  119 of 1500: completed tf_agent.train(...) =   18.147 [loss]\n",
      "I0707 23:32:36.287676 15896 agents.py:92] training  120 of 1500: completed tf_agent.train(...) =   23.393 [loss]\n",
      "I0707 23:32:44.067674 15896 agents.py:92] current policy       : avg_reward=-1.847, avg_steps=181.700\n",
      "I0707 23:32:44.427677 15896 agents.py:92] training  121 of 1500: completed tf_agent.train(...) =   28.593 [loss]\n",
      "I0707 23:32:46.100673 15896 agents.py:92] training  122 of 1500: completed tf_agent.train(...) =   15.136 [loss]\n",
      "I0707 23:32:47.677674 15896 agents.py:92] training  123 of 1500: completed tf_agent.train(...) =   12.178 [loss]\n",
      "I0707 23:32:49.229675 15896 agents.py:92] training  124 of 1500: completed tf_agent.train(...) =   32.324 [loss]\n",
      "I0707 23:32:49.566678 15896 agents.py:92] training  125 of 1500: completed tf_agent.train(...) =   21.897 [loss]\n",
      "I0707 23:32:55.962674 15896 agents.py:92] current policy       : avg_reward=-1.562, avg_steps=144.300\n",
      "I0707 23:32:57.640673 15896 agents.py:92] training  126 of 1500: completed tf_agent.train(...) =   24.170 [loss]\n",
      "I0707 23:32:59.442673 15896 agents.py:92] training  127 of 1500: completed tf_agent.train(...) =   20.100 [loss]\n",
      "I0707 23:33:00.987673 15896 agents.py:92] training  128 of 1500: completed tf_agent.train(...) =   14.338 [loss]\n",
      "I0707 23:33:02.659675 15896 agents.py:92] training  129 of 1500: completed tf_agent.train(...) =   17.959 [loss]\n",
      "I0707 23:33:04.295673 15896 agents.py:92] training  130 of 1500: completed tf_agent.train(...) =   24.641 [loss]\n",
      "I0707 23:33:08.198673 15896 agents.py:92] current policy       : avg_reward=-0.542, avg_steps=88.700\n",
      "I0707 23:33:08.490674 15896 agents.py:92] training  131 of 1500: completed tf_agent.train(...) =    9.880 [loss]\n",
      "I0707 23:33:10.128673 15896 agents.py:92] training  132 of 1500: completed tf_agent.train(...) =    9.058 [loss]\n",
      "I0707 23:33:11.793674 15896 agents.py:92] training  133 of 1500: completed tf_agent.train(...) =   31.074 [loss]\n",
      "I0707 23:33:13.376673 15896 agents.py:92] training  134 of 1500: completed tf_agent.train(...) =   18.419 [loss]\n",
      "I0707 23:33:14.954675 15896 agents.py:92] training  135 of 1500: completed tf_agent.train(...) =    9.657 [loss]\n",
      "I0707 23:33:19.531675 15896 agents.py:92] current policy       : avg_reward=-1.213, avg_steps=106.200\n",
      "I0707 23:33:21.177675 15896 agents.py:92] training  136 of 1500: completed tf_agent.train(...) =   52.056 [loss]\n",
      "I0707 23:33:21.486676 15896 agents.py:92] training  137 of 1500: completed tf_agent.train(...) =   16.965 [loss]\n",
      "I0707 23:33:23.125673 15896 agents.py:92] training  138 of 1500: completed tf_agent.train(...) =   24.757 [loss]\n",
      "I0707 23:33:24.704674 15896 agents.py:92] training  139 of 1500: completed tf_agent.train(...) =   13.582 [loss]\n",
      "I0707 23:33:26.301674 15896 agents.py:92] training  140 of 1500: completed tf_agent.train(...) =   10.591 [loss]\n",
      "I0707 23:33:30.145709 15896 agents.py:92] current policy       : avg_reward=-0.630, avg_steps=87.100\n",
      "I0707 23:33:31.797676 15896 agents.py:92] training  141 of 1500: completed tf_agent.train(...) =   18.344 [loss]\n",
      "I0707 23:33:33.419675 15896 agents.py:92] training  142 of 1500: completed tf_agent.train(...) =   37.621 [loss]\n",
      "I0707 23:33:33.713673 15896 agents.py:92] training  143 of 1500: completed tf_agent.train(...) =   13.241 [loss]\n",
      "I0707 23:33:34.128678 15896 agents.py:92] training  144 of 1500: completed tf_agent.train(...) =    9.398 [loss]\n",
      "I0707 23:33:35.651674 15896 agents.py:92] training  145 of 1500: completed tf_agent.train(...) =   11.146 [loss]\n",
      "I0707 23:33:38.772678 15896 agents.py:92] current policy       : avg_reward=-0.287, avg_steps=69.700\n",
      "I0707 23:33:40.399674 15896 agents.py:92] training  146 of 1500: completed tf_agent.train(...) =   15.994 [loss]\n",
      "I0707 23:33:41.949490 15896 agents.py:92] training  147 of 1500: completed tf_agent.train(...) =    6.265 [loss]\n",
      "I0707 23:33:43.587884 15896 agents.py:92] training  148 of 1500: completed tf_agent.train(...) =   19.261 [loss]\n",
      "I0707 23:33:43.893888 15896 agents.py:92] training  149 of 1500: completed tf_agent.train(...) =   12.128 [loss]\n",
      "I0707 23:33:45.529887 15896 agents.py:92] training  150 of 1500: completed tf_agent.train(...) =    4.196 [loss]\n",
      "I0707 23:33:49.352920 15896 agents.py:92] current policy       : avg_reward=-0.720, avg_steps=88.000\n",
      "I0707 23:33:50.963939 15896 agents.py:92] training  151 of 1500: completed tf_agent.train(...) =    6.001 [loss]\n",
      "I0707 23:33:52.587939 15896 agents.py:92] training  152 of 1500: completed tf_agent.train(...) =   10.684 [loss]\n",
      "I0707 23:33:52.993942 15896 agents.py:92] training  153 of 1500: completed tf_agent.train(...) =   40.351 [loss]\n",
      "I0707 23:33:54.679939 15896 agents.py:92] training  154 of 1500: completed tf_agent.train(...) =    6.650 [loss]\n",
      "I0707 23:33:55.084943 15896 agents.py:92] training  155 of 1500: completed tf_agent.train(...) =   33.063 [loss]\n",
      "I0707 23:33:58.991940 15896 agents.py:92] current policy       : avg_reward=-0.376, avg_steps=88.200\n",
      "I0707 23:34:00.613939 15896 agents.py:92] training  156 of 1500: completed tf_agent.train(...) =    5.978 [loss]\n",
      "I0707 23:34:02.243939 15896 agents.py:92] training  157 of 1500: completed tf_agent.train(...) =    7.198 [loss]\n",
      "I0707 23:34:03.893939 15896 agents.py:92] training  158 of 1500: completed tf_agent.train(...) =    7.990 [loss]\n",
      "I0707 23:34:05.529939 15896 agents.py:92] training  159 of 1500: completed tf_agent.train(...) =    4.653 [loss]\n",
      "I0707 23:34:07.178940 15896 agents.py:92] training  160 of 1500: completed tf_agent.train(...) =   10.125 [loss]\n",
      "I0707 23:34:10.242939 15896 agents.py:92] current policy       : avg_reward=-0.132, avg_steps=69.800\n",
      "I0707 23:34:11.864939 15896 agents.py:92] training  161 of 1500: completed tf_agent.train(...) =    9.924 [loss]\n",
      "I0707 23:34:12.141938 15896 agents.py:92] training  162 of 1500: completed tf_agent.train(...) =    3.604 [loss]\n",
      "I0707 23:34:12.476938 15896 agents.py:92] training  163 of 1500: completed tf_agent.train(...) =   28.013 [loss]\n",
      "I0707 23:34:14.189939 15896 agents.py:92] training  164 of 1500: completed tf_agent.train(...) =    7.510 [loss]\n",
      "I0707 23:34:14.487939 15896 agents.py:92] training  165 of 1500: completed tf_agent.train(...) =    4.918 [loss]\n",
      "I0707 23:34:17.659938 15896 agents.py:92] current policy       : avg_reward=-0.142, avg_steps=71.400\n",
      "I0707 23:34:19.296937 15896 agents.py:92] training  166 of 1500: completed tf_agent.train(...) =    4.369 [loss]\n",
      "I0707 23:34:20.975939 15896 agents.py:92] training  167 of 1500: completed tf_agent.train(...) =   23.204 [loss]\n",
      "I0707 23:34:22.614939 15896 agents.py:92] training  168 of 1500: completed tf_agent.train(...) =    6.182 [loss]\n",
      "I0707 23:34:24.287939 15896 agents.py:92] training  169 of 1500: completed tf_agent.train(...) =    7.872 [loss]\n",
      "I0707 23:34:25.908939 15896 agents.py:92] training  170 of 1500: completed tf_agent.train(...) =    6.045 [loss]\n",
      "I0707 23:34:34.500939 15896 agents.py:92] current policy       : avg_reward=-1.870, avg_steps=200.000\n",
      "I0707 23:34:36.219938 15896 agents.py:92] training  171 of 1500: completed tf_agent.train(...) =   11.192 [loss]\n",
      "I0707 23:34:37.915249 15896 agents.py:92] training  172 of 1500: completed tf_agent.train(...) =    7.807 [loss]\n",
      "I0707 23:34:39.611054 15896 agents.py:92] training  173 of 1500: completed tf_agent.train(...) =    7.403 [loss]\n",
      "I0707 23:34:41.307053 15896 agents.py:92] training  174 of 1500: completed tf_agent.train(...) =    3.616 [loss]\n",
      "I0707 23:34:41.665057 15896 agents.py:92] training  175 of 1500: completed tf_agent.train(...) =    9.751 [loss]\n",
      "I0707 23:34:44.721089 15896 agents.py:92] current policy       : avg_reward=-0.308, avg_steps=69.300\n",
      "I0707 23:34:46.411053 15896 agents.py:92] training  176 of 1500: completed tf_agent.train(...) =    2.650 [loss]\n",
      "I0707 23:34:48.072053 15896 agents.py:92] training  177 of 1500: completed tf_agent.train(...) =   22.497 [loss]\n",
      "I0707 23:34:49.734068 15896 agents.py:92] training  178 of 1500: completed tf_agent.train(...) =    4.570 [loss]\n",
      "I0707 23:34:51.383053 15896 agents.py:92] training  179 of 1500: completed tf_agent.train(...) =   10.702 [loss]\n",
      "I0707 23:34:53.066053 15896 agents.py:92] training  180 of 1500: completed tf_agent.train(...) =   72.750 [loss]\n",
      "I0707 23:34:58.466058 15896 agents.py:92] current policy       : avg_reward=-1.386, avg_steps=124.700\n",
      "I0707 23:35:00.112005 15896 agents.py:92] training  181 of 1500: completed tf_agent.train(...) =    5.909 [loss]\n",
      "I0707 23:35:01.755164 15896 agents.py:92] training  182 of 1500: completed tf_agent.train(...) =    6.997 [loss]\n",
      "I0707 23:35:03.406163 15896 agents.py:92] training  183 of 1500: completed tf_agent.train(...) =    5.106 [loss]\n",
      "I0707 23:35:03.709164 15896 agents.py:92] training  184 of 1500: completed tf_agent.train(...) =    4.667 [loss]\n",
      "I0707 23:35:05.369163 15896 agents.py:92] training  185 of 1500: completed tf_agent.train(...) =    2.679 [loss]\n",
      "I0707 23:35:08.431164 15896 agents.py:92] current policy       : avg_reward=-0.460, avg_steps=70.000\n",
      "I0707 23:35:10.183166 15896 agents.py:92] training  186 of 1500: completed tf_agent.train(...) =  162.357 [loss]\n",
      "I0707 23:35:11.842167 15896 agents.py:92] training  187 of 1500: completed tf_agent.train(...) =    4.563 [loss]\n",
      "I0707 23:35:12.172172 15896 agents.py:92] training  188 of 1500: completed tf_agent.train(...) =    9.124 [loss]\n",
      "I0707 23:35:12.523165 15896 agents.py:92] training  189 of 1500: completed tf_agent.train(...) =    8.637 [loss]\n",
      "I0707 23:35:14.163164 15896 agents.py:92] training  190 of 1500: completed tf_agent.train(...) =    8.510 [loss]\n",
      "I0707 23:35:19.542162 15896 agents.py:92] current policy       : avg_reward=-1.571, avg_steps=125.000\n",
      "I0707 23:35:19.911169 15896 agents.py:92] training  191 of 1500: completed tf_agent.train(...) =   10.899 [loss]\n",
      "I0707 23:35:21.618164 15896 agents.py:92] training  192 of 1500: completed tf_agent.train(...) =    4.774 [loss]\n",
      "I0707 23:35:23.445166 15896 agents.py:92] training  193 of 1500: completed tf_agent.train(...) =   25.886 [loss]\n",
      "I0707 23:35:25.130166 15896 agents.py:92] training  194 of 1500: completed tf_agent.train(...) =    7.101 [loss]\n",
      "I0707 23:35:25.469166 15896 agents.py:92] training  195 of 1500: completed tf_agent.train(...) =    8.814 [loss]\n",
      "I0707 23:35:29.334164 15896 agents.py:92] current policy       : avg_reward=-0.513, avg_steps=87.900\n",
      "I0707 23:35:31.028164 15896 agents.py:92] training  196 of 1500: completed tf_agent.train(...) =    4.193 [loss]\n",
      "I0707 23:35:31.382166 15896 agents.py:92] training  197 of 1500: completed tf_agent.train(...) =   18.297 [loss]\n",
      "I0707 23:35:33.058165 15896 agents.py:92] training  198 of 1500: completed tf_agent.train(...) =    3.594 [loss]\n",
      "I0707 23:35:34.766170 15896 agents.py:92] training  199 of 1500: completed tf_agent.train(...) =    2.320 [loss]\n",
      "I0707 23:35:35.144168 15896 agents.py:92] training  200 of 1500: completed tf_agent.train(...) =   21.325 [loss]\n",
      "I0707 23:35:39.801164 15896 agents.py:92] current policy       : avg_reward=-0.842, avg_steps=106.700\n",
      "I0707 23:35:41.469166 15896 agents.py:92] training  201 of 1500: completed tf_agent.train(...) =    2.980 [loss]\n",
      "I0707 23:35:41.762166 15896 agents.py:92] training  202 of 1500: completed tf_agent.train(...) =    4.751 [loss]\n",
      "I0707 23:35:43.502163 15896 agents.py:92] training  203 of 1500: completed tf_agent.train(...) =    3.419 [loss]\n",
      "I0707 23:35:43.834163 15896 agents.py:92] training  204 of 1500: completed tf_agent.train(...) =   14.215 [loss]\n",
      "I0707 23:35:45.513819 15896 agents.py:92] training  205 of 1500: completed tf_agent.train(...) =    3.106 [loss]\n",
      "I0707 23:35:48.557816 15896 agents.py:92] current policy       : avg_reward=-0.494, avg_steps=69.100\n",
      "I0707 23:35:50.235817 15896 agents.py:92] training  206 of 1500: completed tf_agent.train(...) =    1.901 [loss]\n",
      "I0707 23:35:51.930820 15896 agents.py:92] training  207 of 1500: completed tf_agent.train(...) =   11.130 [loss]\n",
      "I0707 23:35:52.280821 15896 agents.py:92] training  208 of 1500: completed tf_agent.train(...) =    8.916 [loss]\n",
      "I0707 23:35:52.618818 15896 agents.py:92] training  209 of 1500: completed tf_agent.train(...) =    4.086 [loss]\n",
      "I0707 23:35:53.081817 15896 agents.py:92] training  210 of 1500: completed tf_agent.train(...) =  278.405 [loss]\n",
      "I0707 23:35:56.896821 15896 agents.py:92] current policy       : avg_reward=-0.490, avg_steps=88.300\n",
      "I0707 23:35:58.661817 15896 agents.py:92] training  211 of 1500: completed tf_agent.train(...) =    7.934 [loss]\n",
      "I0707 23:35:59.050818 15896 agents.py:92] training  212 of 1500: completed tf_agent.train(...) =    6.758 [loss]\n",
      "I0707 23:36:00.776816 15896 agents.py:92] training  213 of 1500: completed tf_agent.train(...) =   16.368 [loss]\n",
      "I0707 23:36:01.084824 15896 agents.py:92] training  214 of 1500: completed tf_agent.train(...) =    5.691 [loss]\n",
      "I0707 23:36:01.404819 15896 agents.py:92] training  215 of 1500: completed tf_agent.train(...) =   14.610 [loss]\n",
      "I0707 23:36:03.710817 15896 agents.py:92] current policy       : avg_reward=0.007, avg_steps=51.100\n",
      "I0707 23:36:05.448819 15896 agents.py:92] training  216 of 1500: completed tf_agent.train(...) =    4.152 [loss]\n",
      "I0707 23:36:05.771821 15896 agents.py:92] training  217 of 1500: completed tf_agent.train(...) =    4.358 [loss]\n",
      "I0707 23:36:07.526818 15896 agents.py:92] training  218 of 1500: completed tf_agent.train(...) =    7.183 [loss]\n",
      "I0707 23:36:09.268818 15896 agents.py:92] training  219 of 1500: completed tf_agent.train(...) =    3.351 [loss]\n",
      "I0707 23:36:09.613818 15896 agents.py:92] training  220 of 1500: completed tf_agent.train(...) =    3.284 [loss]\n",
      "I0707 23:36:13.485821 15896 agents.py:92] current policy       : avg_reward=-0.612, avg_steps=88.400\n",
      "I0707 23:36:13.838816 15896 agents.py:92] training  221 of 1500: completed tf_agent.train(...) =    3.535 [loss]\n",
      "I0707 23:36:15.699817 15896 agents.py:92] training  222 of 1500: completed tf_agent.train(...) =   83.968 [loss]\n",
      "I0707 23:36:17.504817 15896 agents.py:92] training  223 of 1500: completed tf_agent.train(...) =   16.245 [loss]\n",
      "I0707 23:36:17.808820 15896 agents.py:92] training  224 of 1500: completed tf_agent.train(...) =    6.577 [loss]\n",
      "I0707 23:36:18.135820 15896 agents.py:92] training  225 of 1500: completed tf_agent.train(...) =    7.903 [loss]\n",
      "I0707 23:36:21.996817 15896 agents.py:92] current policy       : avg_reward=-0.310, avg_steps=88.400\n",
      "I0707 23:36:22.330822 15896 agents.py:92] training  226 of 1500: completed tf_agent.train(...) =    5.356 [loss]\n",
      "I0707 23:36:24.088833 15896 agents.py:92] training  227 of 1500: completed tf_agent.train(...) =    7.083 [loss]\n",
      "I0707 23:36:25.846819 15896 agents.py:92] training  228 of 1500: completed tf_agent.train(...) =    5.079 [loss]\n",
      "I0707 23:36:27.565815 15896 agents.py:92] training  229 of 1500: completed tf_agent.train(...) =    6.966 [loss]\n",
      "I0707 23:36:27.892821 15896 agents.py:92] training  230 of 1500: completed tf_agent.train(...) =    8.299 [loss]\n",
      "I0707 23:36:31.792854 15896 agents.py:92] current policy       : avg_reward=-0.465, avg_steps=88.700\n",
      "I0707 23:36:32.181821 15896 agents.py:92] training  231 of 1500: completed tf_agent.train(...) =    6.810 [loss]\n",
      "I0707 23:36:32.490820 15896 agents.py:92] training  232 of 1500: completed tf_agent.train(...) =    4.155 [loss]\n",
      "I0707 23:36:32.842818 15896 agents.py:92] training  233 of 1500: completed tf_agent.train(...) =    3.834 [loss]\n",
      "I0707 23:36:34.556729 15896 agents.py:92] training  234 of 1500: completed tf_agent.train(...) =    3.527 [loss]\n",
      "I0707 23:36:34.888794 15896 agents.py:92] training  235 of 1500: completed tf_agent.train(...) =    3.703 [loss]\n",
      "I0707 23:36:38.748136 15896 agents.py:92] current policy       : avg_reward=-0.577, avg_steps=87.600\n",
      "I0707 23:36:39.108104 15896 agents.py:92] training  236 of 1500: completed tf_agent.train(...) =    5.580 [loss]\n",
      "I0707 23:36:40.845101 15896 agents.py:92] training  237 of 1500: completed tf_agent.train(...) =   38.676 [loss]\n",
      "I0707 23:36:42.557099 15896 agents.py:92] training  238 of 1500: completed tf_agent.train(...) =    3.880 [loss]\n",
      "I0707 23:36:44.290158 15896 agents.py:92] training  239 of 1500: completed tf_agent.train(...) =    3.202 [loss]\n",
      "I0707 23:36:46.132161 15896 agents.py:92] training  240 of 1500: completed tf_agent.train(...) =   44.697 [loss]\n",
      "I0707 23:36:51.729194 15896 agents.py:92] current policy       : avg_reward=-1.592, avg_steps=126.000\n",
      "I0707 23:36:53.598160 15896 agents.py:92] training  241 of 1500: completed tf_agent.train(...) =   10.549 [loss]\n",
      "I0707 23:36:55.470158 15896 agents.py:92] training  242 of 1500: completed tf_agent.train(...) =  170.281 [loss]\n",
      "I0707 23:36:57.238157 15896 agents.py:92] training  243 of 1500: completed tf_agent.train(...) =   12.725 [loss]\n",
      "I0707 23:36:59.129158 15896 agents.py:92] training  244 of 1500: completed tf_agent.train(...) =   52.513 [loss]\n",
      "I0707 23:37:00.897159 15896 agents.py:92] training  245 of 1500: completed tf_agent.train(...) =    9.264 [loss]\n",
      "I0707 23:37:05.570158 15896 agents.py:92] current policy       : avg_reward=-1.383, avg_steps=107.000\n",
      "I0707 23:37:07.420159 15896 agents.py:92] training  246 of 1500: completed tf_agent.train(...) =   50.332 [loss]\n",
      "I0707 23:37:09.214738 15896 agents.py:92] training  247 of 1500: completed tf_agent.train(...) =   24.301 [loss]\n",
      "I0707 23:37:09.594249 15896 agents.py:92] training  248 of 1500: completed tf_agent.train(...) =   29.685 [loss]\n",
      "I0707 23:37:10.024248 15896 agents.py:92] training  249 of 1500: completed tf_agent.train(...) =   13.525 [loss]\n",
      "I0707 23:37:10.480246 15896 agents.py:92] training  250 of 1500: completed tf_agent.train(...) =   20.357 [loss]\n",
      "I0707 23:37:15.145243 15896 agents.py:92] current policy       : avg_reward=-0.800, avg_steps=106.800\n",
      "I0707 23:37:16.917245 15896 agents.py:92] training  251 of 1500: completed tf_agent.train(...) =    7.172 [loss]\n",
      "I0707 23:37:18.696280 15896 agents.py:92] training  252 of 1500: completed tf_agent.train(...) =    4.754 [loss]\n",
      "I0707 23:37:19.143245 15896 agents.py:92] training  253 of 1500: completed tf_agent.train(...) =   12.518 [loss]\n",
      "I0707 23:37:19.522244 15896 agents.py:92] training  254 of 1500: completed tf_agent.train(...) =    7.915 [loss]\n",
      "I0707 23:37:21.288283 15896 agents.py:92] training  255 of 1500: completed tf_agent.train(...) =    3.242 [loss]\n",
      "I0707 23:37:25.157244 15896 agents.py:92] current policy       : avg_reward=-0.707, avg_steps=88.300\n",
      "I0707 23:37:26.911243 15896 agents.py:92] training  256 of 1500: completed tf_agent.train(...) =    3.769 [loss]\n",
      "I0707 23:37:27.235246 15896 agents.py:92] training  257 of 1500: completed tf_agent.train(...) =    5.717 [loss]\n",
      "I0707 23:37:27.619248 15896 agents.py:92] training  258 of 1500: completed tf_agent.train(...) =    4.441 [loss]\n",
      "I0707 23:37:27.978280 15896 agents.py:92] training  259 of 1500: completed tf_agent.train(...) =    8.484 [loss]\n",
      "I0707 23:37:28.324282 15896 agents.py:92] training  260 of 1500: completed tf_agent.train(...) =   20.967 [loss]\n",
      "I0707 23:37:31.413280 15896 agents.py:92] current policy       : avg_reward=-0.680, avg_steps=69.400\n",
      "I0707 23:37:33.352245 15896 agents.py:92] training  261 of 1500: completed tf_agent.train(...) =  250.003 [loss]\n",
      "I0707 23:37:35.234283 15896 agents.py:92] training  262 of 1500: completed tf_agent.train(...) =   17.113 [loss]\n",
      "I0707 23:37:35.579246 15896 agents.py:92] training  263 of 1500: completed tf_agent.train(...) =    9.041 [loss]\n",
      "I0707 23:37:35.963245 15896 agents.py:92] training  264 of 1500: completed tf_agent.train(...) =   12.846 [loss]\n",
      "I0707 23:37:37.781241 15896 agents.py:92] training  265 of 1500: completed tf_agent.train(...) =    7.433 [loss]\n",
      "I0707 23:37:40.858242 15896 agents.py:92] current policy       : avg_reward=-0.239, avg_steps=70.300\n",
      "I0707 23:37:41.276247 15896 agents.py:92] training  266 of 1500: completed tf_agent.train(...) =    6.436 [loss]\n",
      "I0707 23:37:41.606249 15896 agents.py:92] training  267 of 1500: completed tf_agent.train(...) =    5.731 [loss]\n",
      "I0707 23:37:42.059246 15896 agents.py:92] training  268 of 1500: completed tf_agent.train(...) =   48.983 [loss]\n",
      "I0707 23:37:42.419246 15896 agents.py:92] training  269 of 1500: completed tf_agent.train(...) =    3.312 [loss]\n",
      "I0707 23:37:44.235246 15896 agents.py:92] training  270 of 1500: completed tf_agent.train(...) =    5.291 [loss]\n",
      "I0707 23:37:46.535245 15896 agents.py:92] current policy       : avg_reward=0.153, avg_steps=51.300\n",
      "I0707 23:37:46.884248 15896 agents.py:92] training  271 of 1500: completed tf_agent.train(...) =    4.897 [loss]\n",
      "I0707 23:37:48.681246 15896 agents.py:92] training  272 of 1500: completed tf_agent.train(...) =    5.282 [loss]\n",
      "I0707 23:37:49.024245 15896 agents.py:92] training  273 of 1500: completed tf_agent.train(...) =    7.829 [loss]\n",
      "I0707 23:37:50.776244 15896 agents.py:92] training  274 of 1500: completed tf_agent.train(...) =    3.004 [loss]\n",
      "I0707 23:37:51.063245 15896 agents.py:92] training  275 of 1500: completed tf_agent.train(...) =    2.033 [loss]\n",
      "I0707 23:37:54.100243 15896 agents.py:92] current policy       : avg_reward=-0.859, avg_steps=70.000\n",
      "I0707 23:37:54.475247 15896 agents.py:92] training  276 of 1500: completed tf_agent.train(...) =    6.560 [loss]\n",
      "I0707 23:37:54.909245 15896 agents.py:92] training  277 of 1500: completed tf_agent.train(...) =    7.593 [loss]\n",
      "I0707 23:37:56.716247 15896 agents.py:92] training  278 of 1500: completed tf_agent.train(...) =   15.164 [loss]\n",
      "I0707 23:37:58.490513 15896 agents.py:92] training  279 of 1500: completed tf_agent.train(...) =    2.515 [loss]\n",
      "I0707 23:38:00.262512 15896 agents.py:92] training  280 of 1500: completed tf_agent.train(...) =    2.421 [loss]\n",
      "I0707 23:38:02.529546 15896 agents.py:92] current policy       : avg_reward=0.297, avg_steps=50.400\n",
      "I0707 23:38:03.134513 15896 agents.py:92] training  281 of 1500: completed tf_agent.train(...) =   73.097 [loss]\n",
      "I0707 23:38:03.537517 15896 agents.py:92] training  282 of 1500: completed tf_agent.train(...) =   13.458 [loss]\n",
      "I0707 23:38:05.351548 15896 agents.py:92] training  283 of 1500: completed tf_agent.train(...) =    6.159 [loss]\n",
      "I0707 23:38:05.770516 15896 agents.py:92] training  284 of 1500: completed tf_agent.train(...) =    5.427 [loss]\n",
      "I0707 23:38:07.574912 15896 agents.py:92] training  285 of 1500: completed tf_agent.train(...) =    4.390 [loss]\n",
      "I0707 23:38:11.448456 15896 agents.py:92] current policy       : avg_reward=-0.555, avg_steps=88.200\n",
      "I0707 23:38:11.804422 15896 agents.py:92] training  286 of 1500: completed tf_agent.train(...) =    4.048 [loss]\n",
      "I0707 23:38:12.130249 15896 agents.py:92] training  287 of 1500: completed tf_agent.train(...) =    2.044 [loss]\n",
      "I0707 23:38:12.468233 15896 agents.py:92] training  288 of 1500: completed tf_agent.train(...) =    2.403 [loss]\n",
      "I0707 23:38:14.393234 15896 agents.py:92] training  289 of 1500: completed tf_agent.train(...) =   71.981 [loss]\n",
      "I0707 23:38:14.748236 15896 agents.py:92] training  290 of 1500: completed tf_agent.train(...) =    3.764 [loss]\n",
      "I0707 23:38:18.622234 15896 agents.py:92] current policy       : avg_reward=-0.347, avg_steps=88.300\n",
      "I0707 23:38:20.518237 15896 agents.py:92] training  291 of 1500: completed tf_agent.train(...) =  773.157 [loss]\n",
      "I0707 23:38:22.321233 15896 agents.py:92] training  292 of 1500: completed tf_agent.train(...) =    8.871 [loss]\n",
      "I0707 23:38:22.651241 15896 agents.py:92] training  293 of 1500: completed tf_agent.train(...) =   13.817 [loss]\n",
      "I0707 23:38:23.099236 15896 agents.py:92] training  294 of 1500: completed tf_agent.train(...) =   10.660 [loss]\n",
      "I0707 23:38:23.433237 15896 agents.py:92] training  295 of 1500: completed tf_agent.train(...) =   12.421 [loss]\n",
      "I0707 23:38:25.729238 15896 agents.py:92] current policy       : avg_reward=0.139, avg_steps=51.500\n",
      "I0707 23:38:26.167236 15896 agents.py:92] training  296 of 1500: completed tf_agent.train(...) =    9.537 [loss]\n",
      "I0707 23:38:26.573268 15896 agents.py:92] training  297 of 1500: completed tf_agent.train(...) =    7.069 [loss]\n",
      "I0707 23:38:26.880270 15896 agents.py:92] training  298 of 1500: completed tf_agent.train(...) =    8.490 [loss]\n",
      "I0707 23:38:27.210238 15896 agents.py:92] training  299 of 1500: completed tf_agent.train(...) =    9.032 [loss]\n",
      "I0707 23:38:27.688236 15896 agents.py:92] training  300 of 1500: completed tf_agent.train(...) =    4.448 [loss]\n",
      "I0707 23:38:32.278233 15896 agents.py:92] current policy       : avg_reward=-1.325, avg_steps=106.200\n",
      "I0707 23:38:34.069233 15896 agents.py:92] training  301 of 1500: completed tf_agent.train(...) =    3.018 [loss]\n",
      "I0707 23:38:35.835235 15896 agents.py:92] training  302 of 1500: completed tf_agent.train(...) =    3.048 [loss]\n",
      "I0707 23:38:36.165241 15896 agents.py:92] training  303 of 1500: completed tf_agent.train(...) =    4.273 [loss]\n",
      "I0707 23:38:36.510271 15896 agents.py:92] training  304 of 1500: completed tf_agent.train(...) =    2.426 [loss]\n",
      "I0707 23:38:38.326239 15896 agents.py:92] training  305 of 1500: completed tf_agent.train(...) =    3.030 [loss]\n",
      "I0707 23:38:40.613234 15896 agents.py:92] current policy       : avg_reward=0.335, avg_steps=50.200\n",
      "I0707 23:38:40.961233 15896 agents.py:92] training  306 of 1500: completed tf_agent.train(...) =    3.462 [loss]\n",
      "I0707 23:38:41.337235 15896 agents.py:92] training  307 of 1500: completed tf_agent.train(...) =    4.562 [loss]\n",
      "I0707 23:38:41.810236 15896 agents.py:92] training  308 of 1500: completed tf_agent.train(...) =    9.587 [loss]\n",
      "I0707 23:38:42.187239 15896 agents.py:92] training  309 of 1500: completed tf_agent.train(...) =    4.395 [loss]\n",
      "I0707 23:38:42.545235 15896 agents.py:92] training  310 of 1500: completed tf_agent.train(...) =    4.086 [loss]\n",
      "I0707 23:38:45.223256 15896 agents.py:92] current policy       : avg_reward=0.196, avg_steps=51.200\n",
      "I0707 23:38:45.771235 15896 agents.py:92] training  311 of 1500: completed tf_agent.train(...) =    2.216 [loss]\n",
      "I0707 23:38:46.329233 15896 agents.py:92] training  312 of 1500: completed tf_agent.train(...) =   18.178 [loss]\n",
      "I0707 23:38:48.148236 15896 agents.py:92] training  313 of 1500: completed tf_agent.train(...) =    4.987 [loss]\n",
      "I0707 23:38:49.878237 15896 agents.py:92] training  314 of 1500: completed tf_agent.train(...) =    3.164 [loss]\n",
      "I0707 23:38:50.208234 15896 agents.py:92] training  315 of 1500: completed tf_agent.train(...) =    2.780 [loss]\n",
      "I0707 23:38:53.136233 15896 agents.py:92] current policy       : avg_reward=-0.168, avg_steps=69.900\n",
      "I0707 23:38:53.609239 15896 agents.py:92] training  316 of 1500: completed tf_agent.train(...) =    2.389 [loss]\n",
      "I0707 23:38:54.049237 15896 agents.py:92] training  317 of 1500: completed tf_agent.train(...) =    7.102 [loss]\n",
      "I0707 23:38:54.480236 15896 agents.py:92] training  318 of 1500: completed tf_agent.train(...) =    2.561 [loss]\n",
      "I0707 23:38:54.825243 15896 agents.py:92] training  319 of 1500: completed tf_agent.train(...) =    2.747 [loss]\n",
      "I0707 23:38:55.257236 15896 agents.py:92] training  320 of 1500: completed tf_agent.train(...) =    3.600 [loss]\n",
      "I0707 23:38:58.177233 15896 agents.py:92] current policy       : avg_reward=-0.307, avg_steps=69.700\n",
      "I0707 23:38:58.567237 15896 agents.py:92] training  321 of 1500: completed tf_agent.train(...) =   44.461 [loss]\n",
      "I0707 23:39:00.483233 15896 agents.py:92] training  322 of 1500: completed tf_agent.train(...) =  519.094 [loss]\n",
      "I0707 23:39:00.947236 15896 agents.py:92] training  323 of 1500: completed tf_agent.train(...) =   17.650 [loss]\n",
      "I0707 23:39:01.436236 15896 agents.py:92] training  324 of 1500: completed tf_agent.train(...) =   25.968 [loss]\n",
      "I0707 23:39:01.741240 15896 agents.py:92] training  325 of 1500: completed tf_agent.train(...) =   11.074 [loss]\n",
      "I0707 23:39:03.974233 15896 agents.py:92] current policy       : avg_reward=0.047, avg_steps=52.000\n",
      "I0707 23:39:05.777233 15896 agents.py:92] training  326 of 1500: completed tf_agent.train(...) =   15.875 [loss]\n",
      "I0707 23:39:06.193240 15896 agents.py:92] training  327 of 1500: completed tf_agent.train(...) =   12.317 [loss]\n",
      "I0707 23:39:06.608242 15896 agents.py:92] training  328 of 1500: completed tf_agent.train(...) =   10.671 [loss]\n",
      "I0707 23:39:07.065234 15896 agents.py:92] training  329 of 1500: completed tf_agent.train(...) =   32.288 [loss]\n",
      "I0707 23:39:07.551233 15896 agents.py:92] training  330 of 1500: completed tf_agent.train(...) =   27.596 [loss]\n",
      "I0707 23:39:10.460233 15896 agents.py:92] current policy       : avg_reward=-0.290, avg_steps=71.000\n",
      "I0707 23:39:10.937238 15896 agents.py:92] training  331 of 1500: completed tf_agent.train(...) =    7.865 [loss]\n",
      "I0707 23:39:11.353239 15896 agents.py:92] training  332 of 1500: completed tf_agent.train(...) =   11.173 [loss]\n",
      "I0707 23:39:11.760235 15896 agents.py:92] training  333 of 1500: completed tf_agent.train(...) =   10.765 [loss]\n",
      "I0707 23:39:13.616232 15896 agents.py:92] training  334 of 1500: completed tf_agent.train(...) =  179.734 [loss]\n",
      "I0707 23:39:13.958236 15896 agents.py:92] training  335 of 1500: completed tf_agent.train(...) =   10.550 [loss]\n",
      "I0707 23:39:16.164233 15896 agents.py:92] current policy       : avg_reward=0.058, avg_steps=51.100\n",
      "I0707 23:39:16.653237 15896 agents.py:92] training  336 of 1500: completed tf_agent.train(...) =    8.816 [loss]\n",
      "I0707 23:39:17.160237 15896 agents.py:92] training  337 of 1500: completed tf_agent.train(...) =   21.086 [loss]\n",
      "I0707 23:39:17.618240 15896 agents.py:92] training  338 of 1500: completed tf_agent.train(...) =   11.492 [loss]\n",
      "I0707 23:39:18.063240 15896 agents.py:92] training  339 of 1500: completed tf_agent.train(...) =   20.748 [loss]\n",
      "I0707 23:39:18.558237 15896 agents.py:92] training  340 of 1500: completed tf_agent.train(...) =   15.887 [loss]\n",
      "I0707 23:39:19.362237 15896 agents.py:92] current policy       : avg_reward=0.709, avg_steps=15.100\n",
      "I0707 23:39:21.382236 15896 agents.py:92] training  341 of 1500: completed tf_agent.train(...) =   92.410 [loss]\n",
      "I0707 23:39:21.694234 15896 agents.py:92] training  342 of 1500: completed tf_agent.train(...) =    6.416 [loss]\n",
      "I0707 23:39:22.291235 15896 agents.py:92] training  343 of 1500: completed tf_agent.train(...) =  328.544 [loss]\n",
      "I0707 23:39:22.597272 15896 agents.py:92] training  344 of 1500: completed tf_agent.train(...) =   12.968 [loss]\n",
      "I0707 23:39:22.944236 15896 agents.py:92] training  345 of 1500: completed tf_agent.train(...) =   14.848 [loss]\n",
      "I0707 23:39:25.846236 15896 agents.py:92] current policy       : avg_reward=-0.004, avg_steps=52.900\n",
      "I0707 23:39:26.253234 15896 agents.py:92] training  346 of 1500: completed tf_agent.train(...) =   12.203 [loss]\n",
      "I0707 23:39:28.693236 15896 agents.py:92] training  347 of 1500: completed tf_agent.train(...) =   12.391 [loss]\n",
      "I0707 23:39:29.203238 15896 agents.py:92] training  348 of 1500: completed tf_agent.train(...) =   12.183 [loss]\n",
      "I0707 23:39:31.581235 15896 agents.py:92] training  349 of 1500: completed tf_agent.train(...) =    6.056 [loss]\n",
      "I0707 23:39:31.921232 15896 agents.py:92] training  350 of 1500: completed tf_agent.train(...) =    8.829 [loss]\n",
      "I0707 23:39:35.424234 15896 agents.py:92] current policy       : avg_reward=-0.331, avg_steps=70.200\n",
      "I0707 23:39:35.890238 15896 agents.py:92] training  351 of 1500: completed tf_agent.train(...) =    7.668 [loss]\n",
      "I0707 23:39:36.346237 15896 agents.py:92] training  352 of 1500: completed tf_agent.train(...) =    3.332 [loss]\n",
      "I0707 23:39:36.845233 15896 agents.py:92] training  353 of 1500: completed tf_agent.train(...) =   51.726 [loss]\n",
      "I0707 23:39:37.316237 15896 agents.py:92] training  354 of 1500: completed tf_agent.train(...) =    8.256 [loss]\n",
      "I0707 23:39:37.733240 15896 agents.py:92] training  355 of 1500: completed tf_agent.train(...) =    4.478 [loss]\n",
      "I0707 23:39:40.316233 15896 agents.py:92] current policy       : avg_reward=0.028, avg_steps=51.100\n",
      "I0707 23:39:42.183264 15896 agents.py:92] training  356 of 1500: completed tf_agent.train(...) =    5.743 [loss]\n",
      "I0707 23:39:42.560237 15896 agents.py:92] training  357 of 1500: completed tf_agent.train(...) =    6.576 [loss]\n",
      "I0707 23:39:43.015242 15896 agents.py:92] training  358 of 1500: completed tf_agent.train(...) =    5.681 [loss]\n",
      "I0707 23:39:43.520235 15896 agents.py:92] training  359 of 1500: completed tf_agent.train(...) =    4.876 [loss]\n",
      "I0707 23:39:43.983235 15896 agents.py:92] training  360 of 1500: completed tf_agent.train(...) =    5.909 [loss]\n",
      "I0707 23:39:48.754252 15896 agents.py:92] current policy       : avg_reward=-0.995, avg_steps=107.100\n",
      "I0707 23:39:49.119238 15896 agents.py:92] training  361 of 1500: completed tf_agent.train(...) =    5.983 [loss]\n",
      "I0707 23:39:49.512235 15896 agents.py:92] training  362 of 1500: completed tf_agent.train(...) =    5.292 [loss]\n",
      "I0707 23:39:49.912233 15896 agents.py:92] training  363 of 1500: completed tf_agent.train(...) =    4.639 [loss]\n",
      "I0707 23:39:50.370237 15896 agents.py:92] training  364 of 1500: completed tf_agent.train(...) =    8.379 [loss]\n",
      "I0707 23:39:50.840237 15896 agents.py:92] training  365 of 1500: completed tf_agent.train(...) =    4.742 [loss]\n",
      "I0707 23:39:53.897233 15896 agents.py:92] current policy       : avg_reward=-0.315, avg_steps=69.900\n",
      "I0707 23:39:54.253238 15896 agents.py:92] training  366 of 1500: completed tf_agent.train(...) =    2.936 [loss]\n",
      "I0707 23:39:56.128236 15896 agents.py:92] training  367 of 1500: completed tf_agent.train(...) =   20.735 [loss]\n",
      "I0707 23:39:56.542238 15896 agents.py:92] training  368 of 1500: completed tf_agent.train(...) =    3.842 [loss]\n",
      "I0707 23:39:57.041240 15896 agents.py:92] training  369 of 1500: completed tf_agent.train(...) =   16.648 [loss]\n",
      "I0707 23:39:57.407233 15896 agents.py:92] training  370 of 1500: completed tf_agent.train(...) =    5.688 [loss]\n",
      "I0707 23:40:01.227234 15896 agents.py:92] current policy       : avg_reward=-0.381, avg_steps=88.900\n",
      "I0707 23:40:01.692237 15896 agents.py:92] training  371 of 1500: completed tf_agent.train(...) =    7.173 [loss]\n",
      "I0707 23:40:02.038235 15896 agents.py:92] training  372 of 1500: completed tf_agent.train(...) =    5.123 [loss]\n",
      "I0707 23:40:02.381242 15896 agents.py:92] training  373 of 1500: completed tf_agent.train(...) =    3.187 [loss]\n",
      "I0707 23:40:02.836234 15896 agents.py:92] training  374 of 1500: completed tf_agent.train(...) =    2.292 [loss]\n",
      "I0707 23:40:03.155233 15896 agents.py:92] training  375 of 1500: completed tf_agent.train(...) =    2.648 [loss]\n",
      "I0707 23:40:08.300234 15896 agents.py:92] current policy       : avg_reward=-1.095, avg_steps=125.500\n",
      "I0707 23:40:08.709236 15896 agents.py:92] training  376 of 1500: completed tf_agent.train(...) =    3.405 [loss]\n",
      "I0707 23:40:09.171236 15896 agents.py:92] training  377 of 1500: completed tf_agent.train(...) =    2.902 [loss]\n",
      "I0707 23:40:09.639237 15896 agents.py:92] training  378 of 1500: completed tf_agent.train(...) =    3.493 [loss]\n",
      "I0707 23:40:10.103237 15896 agents.py:92] training  379 of 1500: completed tf_agent.train(...) =    7.338 [loss]\n",
      "I0707 23:40:10.487240 15896 agents.py:92] training  380 of 1500: completed tf_agent.train(...) =    4.106 [loss]\n",
      "I0707 23:40:11.965233 15896 agents.py:92] current policy       : avg_reward=0.385, avg_steps=33.100\n",
      "I0707 23:40:12.415234 15896 agents.py:92] training  381 of 1500: completed tf_agent.train(...) =    3.415 [loss]\n",
      "I0707 23:40:12.842237 15896 agents.py:92] training  382 of 1500: completed tf_agent.train(...) =    3.048 [loss]\n",
      "I0707 23:40:13.383236 15896 agents.py:92] training  383 of 1500: completed tf_agent.train(...) =    4.895 [loss]\n",
      "I0707 23:40:13.803237 15896 agents.py:92] training  384 of 1500: completed tf_agent.train(...) =    2.827 [loss]\n",
      "I0707 23:40:15.850235 15896 agents.py:92] training  385 of 1500: completed tf_agent.train(...) =    7.866 [loss]\n",
      "I0707 23:40:17.421233 15896 agents.py:92] current policy       : avg_reward=0.053, avg_steps=32.500\n",
      "I0707 23:40:17.876243 15896 agents.py:92] training  386 of 1500: completed tf_agent.train(...) =    3.658 [loss]\n",
      "I0707 23:40:18.386237 15896 agents.py:92] training  387 of 1500: completed tf_agent.train(...) =    2.788 [loss]\n",
      "I0707 23:40:18.859240 15896 agents.py:92] training  388 of 1500: completed tf_agent.train(...) =    2.651 [loss]\n",
      "I0707 23:40:19.202233 15896 agents.py:92] training  389 of 1500: completed tf_agent.train(...) =    2.095 [loss]\n",
      "I0707 23:40:19.525238 15896 agents.py:92] training  390 of 1500: completed tf_agent.train(...) =    1.605 [loss]\n",
      "I0707 23:40:21.843233 15896 agents.py:92] current policy       : avg_reward=0.176, avg_steps=51.200\n",
      "I0707 23:40:22.331238 15896 agents.py:92] training  391 of 1500: completed tf_agent.train(...) =    7.638 [loss]\n",
      "I0707 23:40:22.791239 15896 agents.py:92] training  392 of 1500: completed tf_agent.train(...) =    2.183 [loss]\n",
      "I0707 23:40:23.310240 15896 agents.py:92] training  393 of 1500: completed tf_agent.train(...) =    2.115 [loss]\n",
      "I0707 23:40:23.800237 15896 agents.py:92] training  394 of 1500: completed tf_agent.train(...) =    1.855 [loss]\n",
      "I0707 23:40:24.176238 15896 agents.py:92] training  395 of 1500: completed tf_agent.train(...) =    3.186 [loss]\n",
      "I0707 23:40:25.679238 15896 agents.py:92] current policy       : avg_reward=0.513, avg_steps=32.600\n",
      "I0707 23:40:26.112239 15896 agents.py:92] training  396 of 1500: completed tf_agent.train(...) =    1.659 [loss]\n",
      "I0707 23:40:26.460232 15896 agents.py:92] training  397 of 1500: completed tf_agent.train(...) =    1.862 [loss]\n",
      "I0707 23:40:26.825235 15896 agents.py:92] training  398 of 1500: completed tf_agent.train(...) =    3.059 [loss]\n",
      "I0707 23:40:27.354234 15896 agents.py:92] training  399 of 1500: completed tf_agent.train(...) =    2.277 [loss]\n",
      "I0707 23:40:27.772258 15896 agents.py:92] training  400 of 1500: completed tf_agent.train(...) =    1.266 [loss]\n",
      "I0707 23:40:29.346269 15896 agents.py:92] current policy       : avg_reward=0.508, avg_steps=32.800\n",
      "I0707 23:40:29.828238 15896 agents.py:92] training  401 of 1500: completed tf_agent.train(...) =    7.763 [loss]\n",
      "I0707 23:40:30.248237 15896 agents.py:92] training  402 of 1500: completed tf_agent.train(...) =    2.031 [loss]\n",
      "I0707 23:40:30.732240 15896 agents.py:92] training  403 of 1500: completed tf_agent.train(...) =    2.454 [loss]\n",
      "I0707 23:40:31.099234 15896 agents.py:92] training  404 of 1500: completed tf_agent.train(...) =   16.880 [loss]\n",
      "I0707 23:40:31.496236 15896 agents.py:92] training  405 of 1500: completed tf_agent.train(...) =    1.881 [loss]\n",
      "I0707 23:40:34.401233 15896 agents.py:92] current policy       : avg_reward=-0.618, avg_steps=69.000\n",
      "I0707 23:40:34.891239 15896 agents.py:92] training  406 of 1500: completed tf_agent.train(...) =    2.983 [loss]\n",
      "I0707 23:40:35.358236 15896 agents.py:92] training  407 of 1500: completed tf_agent.train(...) =    2.947 [loss]\n",
      "I0707 23:40:37.219234 15896 agents.py:92] training  408 of 1500: completed tf_agent.train(...) =    1.536 [loss]\n",
      "I0707 23:40:38.982237 15896 agents.py:92] training  409 of 1500: completed tf_agent.train(...) =    1.970 [loss]\n",
      "I0707 23:40:39.440240 15896 agents.py:92] training  410 of 1500: completed tf_agent.train(...) =   10.473 [loss]\n",
      "I0707 23:40:41.776233 15896 agents.py:92] current policy       : avg_reward=0.161, avg_steps=52.900\n",
      "I0707 23:40:42.202241 15896 agents.py:92] training  411 of 1500: completed tf_agent.train(...) =    2.758 [loss]\n",
      "I0707 23:40:42.563235 15896 agents.py:92] training  412 of 1500: completed tf_agent.train(...) =    2.956 [loss]\n",
      "I0707 23:40:44.348233 15896 agents.py:92] training  413 of 1500: completed tf_agent.train(...) =    1.997 [loss]\n",
      "I0707 23:40:44.753238 15896 agents.py:92] training  414 of 1500: completed tf_agent.train(...) =    3.620 [loss]\n",
      "I0707 23:40:45.097254 15896 agents.py:92] training  415 of 1500: completed tf_agent.train(...) =    2.122 [loss]\n",
      "I0707 23:40:45.878237 15896 agents.py:92] current policy       : avg_reward=0.693, avg_steps=15.100\n",
      "I0707 23:40:46.294234 15896 agents.py:92] training  416 of 1500: completed tf_agent.train(...) =    1.449 [loss]\n",
      "I0707 23:40:46.665235 15896 agents.py:92] training  417 of 1500: completed tf_agent.train(...) =    1.947 [loss]\n",
      "I0707 23:40:48.512253 15896 agents.py:92] training  418 of 1500: completed tf_agent.train(...) =    1.373 [loss]\n",
      "I0707 23:40:48.974237 15896 agents.py:92] training  419 of 1500: completed tf_agent.train(...) =    1.534 [loss]\n",
      "I0707 23:40:49.364244 15896 agents.py:92] training  420 of 1500: completed tf_agent.train(...) =    2.445 [loss]\n",
      "I0707 23:40:50.134234 15896 agents.py:92] current policy       : avg_reward=0.709, avg_steps=14.500\n",
      "I0707 23:40:52.052234 15896 agents.py:92] training  421 of 1500: completed tf_agent.train(...) =    2.769 [loss]\n",
      "I0707 23:40:52.443239 15896 agents.py:92] training  422 of 1500: completed tf_agent.train(...) =    1.440 [loss]\n",
      "I0707 23:40:52.981233 15896 agents.py:92] training  423 of 1500: completed tf_agent.train(...) =   26.403 [loss]\n",
      "I0707 23:40:53.430242 15896 agents.py:92] training  424 of 1500: completed tf_agent.train(...) =    1.524 [loss]\n",
      "I0707 23:40:55.389270 15896 agents.py:92] training  425 of 1500: completed tf_agent.train(...) =    5.942 [loss]\n",
      "I0707 23:40:56.878233 15896 agents.py:92] current policy       : avg_reward=0.414, avg_steps=31.800\n",
      "I0707 23:40:57.384238 15896 agents.py:92] training  426 of 1500: completed tf_agent.train(...) =    1.575 [loss]\n",
      "I0707 23:40:57.826238 15896 agents.py:92] training  427 of 1500: completed tf_agent.train(...) =    1.411 [loss]\n",
      "I0707 23:40:58.228234 15896 agents.py:92] training  428 of 1500: completed tf_agent.train(...) =    6.365 [loss]\n",
      "I0707 23:40:58.538274 15896 agents.py:92] training  429 of 1500: completed tf_agent.train(...) =    1.955 [loss]\n",
      "I0707 23:41:00.376266 15896 agents.py:92] training  430 of 1500: completed tf_agent.train(...) =    2.244 [loss]\n",
      "I0707 23:41:02.655235 15896 agents.py:92] current policy       : avg_reward=0.306, avg_steps=51.000\n",
      "I0707 23:41:03.091238 15896 agents.py:92] training  431 of 1500: completed tf_agent.train(...) =    1.994 [loss]\n",
      "I0707 23:41:03.488236 15896 agents.py:92] training  432 of 1500: completed tf_agent.train(...) =    4.459 [loss]\n",
      "I0707 23:41:03.811234 15896 agents.py:92] training  433 of 1500: completed tf_agent.train(...) =    4.249 [loss]\n",
      "I0707 23:41:04.308245 15896 agents.py:92] training  434 of 1500: completed tf_agent.train(...) =    1.863 [loss]\n",
      "I0707 23:41:04.789237 15896 agents.py:92] training  435 of 1500: completed tf_agent.train(...) =    1.893 [loss]\n",
      "I0707 23:41:05.578234 15896 agents.py:92] current policy       : avg_reward=0.713, avg_steps=14.400\n",
      "I0707 23:41:05.952241 15896 agents.py:92] training  436 of 1500: completed tf_agent.train(...) =    2.667 [loss]\n",
      "I0707 23:41:06.427235 15896 agents.py:92] training  437 of 1500: completed tf_agent.train(...) =    2.720 [loss]\n",
      "I0707 23:41:06.904235 15896 agents.py:92] training  438 of 1500: completed tf_agent.train(...) =    1.751 [loss]\n",
      "I0707 23:41:07.266241 15896 agents.py:92] training  439 of 1500: completed tf_agent.train(...) =    2.196 [loss]\n",
      "I0707 23:41:07.616234 15896 agents.py:92] training  440 of 1500: completed tf_agent.train(...) =    1.828 [loss]\n",
      "I0707 23:41:09.130234 15896 agents.py:92] current policy       : avg_reward=0.386, avg_steps=32.800\n",
      "I0707 23:41:09.609241 15896 agents.py:92] training  441 of 1500: completed tf_agent.train(...) =    0.959 [loss]\n",
      "I0707 23:41:09.992237 15896 agents.py:92] training  442 of 1500: completed tf_agent.train(...) =    9.047 [loss]\n",
      "I0707 23:41:10.505238 15896 agents.py:92] training  443 of 1500: completed tf_agent.train(...) =   16.782 [loss]\n",
      "I0707 23:41:10.932239 15896 agents.py:92] training  444 of 1500: completed tf_agent.train(...) =    2.817 [loss]\n",
      "I0707 23:41:11.302232 15896 agents.py:92] training  445 of 1500: completed tf_agent.train(...) =    1.553 [loss]\n",
      "I0707 23:41:14.275233 15896 agents.py:92] current policy       : avg_reward=-0.182, avg_steps=69.500\n",
      "I0707 23:41:14.629235 15896 agents.py:92] training  446 of 1500: completed tf_agent.train(...) =    1.788 [loss]\n",
      "I0707 23:41:14.973234 15896 agents.py:92] training  447 of 1500: completed tf_agent.train(...) =    1.341 [loss]\n",
      "I0707 23:41:15.478235 15896 agents.py:92] training  448 of 1500: completed tf_agent.train(...) =    1.482 [loss]\n",
      "I0707 23:41:15.948237 15896 agents.py:92] training  449 of 1500: completed tf_agent.train(...) =    1.554 [loss]\n",
      "I0707 23:41:16.395240 15896 agents.py:92] training  450 of 1500: completed tf_agent.train(...) =    2.009 [loss]\n",
      "I0707 23:41:17.202235 15896 agents.py:92] current policy       : avg_reward=0.729, avg_steps=13.500\n",
      "I0707 23:41:17.583235 15896 agents.py:92] training  451 of 1500: completed tf_agent.train(...) =    1.279 [loss]\n",
      "I0707 23:41:17.963239 15896 agents.py:92] training  452 of 1500: completed tf_agent.train(...) =    4.807 [loss]\n",
      "I0707 23:41:18.423239 15896 agents.py:92] training  453 of 1500: completed tf_agent.train(...) =    1.224 [loss]\n",
      "I0707 23:41:18.918238 15896 agents.py:92] training  454 of 1500: completed tf_agent.train(...) =    1.922 [loss]\n",
      "I0707 23:41:19.462237 15896 agents.py:92] training  455 of 1500: completed tf_agent.train(...) =    4.961 [loss]\n",
      "I0707 23:41:21.225241 15896 agents.py:92] current policy       : avg_reward=0.506, avg_steps=33.300\n",
      "I0707 23:41:21.684234 15896 agents.py:92] training  456 of 1500: completed tf_agent.train(...) =    1.523 [loss]\n",
      "I0707 23:41:22.175237 15896 agents.py:92] training  457 of 1500: completed tf_agent.train(...) =    1.355 [loss]\n",
      "I0707 23:41:24.066233 15896 agents.py:92] training  458 of 1500: completed tf_agent.train(...) =    7.119 [loss]\n",
      "I0707 23:41:25.864235 15896 agents.py:92] training  459 of 1500: completed tf_agent.train(...) =    1.350 [loss]\n",
      "I0707 23:41:26.442234 15896 agents.py:92] training  460 of 1500: completed tf_agent.train(...) =    2.358 [loss]\n",
      "I0707 23:41:28.041234 15896 agents.py:92] current policy       : avg_reward=0.378, avg_steps=34.100\n",
      "I0707 23:41:28.646235 15896 agents.py:92] training  461 of 1500: completed tf_agent.train(...) =    1.272 [loss]\n",
      "I0707 23:41:29.145237 15896 agents.py:92] training  462 of 1500: completed tf_agent.train(...) =    1.902 [loss]\n",
      "I0707 23:41:29.604237 15896 agents.py:92] training  463 of 1500: completed tf_agent.train(...) =    1.650 [loss]\n",
      "I0707 23:41:30.055233 15896 agents.py:92] training  464 of 1500: completed tf_agent.train(...) =    2.129 [loss]\n",
      "I0707 23:41:30.447235 15896 agents.py:92] training  465 of 1500: completed tf_agent.train(...) =    1.754 [loss]\n",
      "I0707 23:41:32.647233 15896 agents.py:92] current policy       : avg_reward=0.193, avg_steps=50.800\n",
      "I0707 23:41:33.061234 15896 agents.py:92] training  466 of 1500: completed tf_agent.train(...) =    1.363 [loss]\n",
      "I0707 23:41:33.519235 15896 agents.py:92] training  467 of 1500: completed tf_agent.train(...) =    7.472 [loss]\n",
      "I0707 23:41:33.859236 15896 agents.py:92] training  468 of 1500: completed tf_agent.train(...) =    1.759 [loss]\n",
      "I0707 23:41:34.187238 15896 agents.py:92] training  469 of 1500: completed tf_agent.train(...) =    1.330 [loss]\n",
      "I0707 23:41:34.688234 15896 agents.py:92] training  470 of 1500: completed tf_agent.train(...) =    1.488 [loss]\n",
      "I0707 23:41:35.403234 15896 agents.py:92] current policy       : avg_reward=0.745, avg_steps=13.600\n",
      "I0707 23:41:35.745234 15896 agents.py:92] training  471 of 1500: completed tf_agent.train(...) =    1.151 [loss]\n",
      "I0707 23:41:36.207233 15896 agents.py:92] training  472 of 1500: completed tf_agent.train(...) =    1.543 [loss]\n",
      "I0707 23:41:36.534236 15896 agents.py:92] training  473 of 1500: completed tf_agent.train(...) =    0.980 [loss]\n",
      "I0707 23:41:36.920234 15896 agents.py:92] training  474 of 1500: completed tf_agent.train(...) =    5.583 [loss]\n",
      "I0707 23:41:37.352234 15896 agents.py:92] training  475 of 1500: completed tf_agent.train(...) =    0.870 [loss]\n",
      "I0707 23:41:39.582234 15896 agents.py:92] current policy       : avg_reward=0.188, avg_steps=51.500\n",
      "I0707 23:41:40.010239 15896 agents.py:92] training  476 of 1500: completed tf_agent.train(...) =    1.817 [loss]\n",
      "I0707 23:41:40.467240 15896 agents.py:92] training  477 of 1500: completed tf_agent.train(...) =    1.865 [loss]\n",
      "I0707 23:41:40.880236 15896 agents.py:92] training  478 of 1500: completed tf_agent.train(...) =    1.813 [loss]\n",
      "I0707 23:41:41.345241 15896 agents.py:92] training  479 of 1500: completed tf_agent.train(...) =    2.230 [loss]\n",
      "I0707 23:41:41.695237 15896 agents.py:92] training  480 of 1500: completed tf_agent.train(...) =    1.701 [loss]\n",
      "I0707 23:41:43.156232 15896 agents.py:92] current policy       : avg_reward=0.381, avg_steps=32.600\n",
      "I0707 23:41:43.662239 15896 agents.py:92] training  481 of 1500: completed tf_agent.train(...) =    1.997 [loss]\n",
      "I0707 23:41:44.113240 15896 agents.py:92] training  482 of 1500: completed tf_agent.train(...) =    1.192 [loss]\n",
      "I0707 23:41:44.592235 15896 agents.py:92] training  483 of 1500: completed tf_agent.train(...) =    2.200 [loss]\n",
      "I0707 23:41:45.053236 15896 agents.py:92] training  484 of 1500: completed tf_agent.train(...) =   21.868 [loss]\n",
      "I0707 23:41:45.481236 15896 agents.py:92] training  485 of 1500: completed tf_agent.train(...) =    1.120 [loss]\n",
      "I0707 23:41:47.740268 15896 agents.py:92] current policy       : avg_reward=0.178, avg_steps=52.900\n",
      "I0707 23:41:48.088235 15896 agents.py:92] training  486 of 1500: completed tf_agent.train(...) =    1.649 [loss]\n",
      "I0707 23:41:48.454237 15896 agents.py:92] training  487 of 1500: completed tf_agent.train(...) =    1.585 [loss]\n",
      "I0707 23:41:48.910237 15896 agents.py:92] training  488 of 1500: completed tf_agent.train(...) =    0.796 [loss]\n",
      "I0707 23:41:49.351236 15896 agents.py:92] training  489 of 1500: completed tf_agent.train(...) =    1.704 [loss]\n",
      "I0707 23:41:49.816235 15896 agents.py:92] training  490 of 1500: completed tf_agent.train(...) =    1.635 [loss]\n",
      "I0707 23:41:50.521236 15896 agents.py:92] current policy       : avg_reward=0.727, avg_steps=14.700\n",
      "I0707 23:41:50.836236 15896 agents.py:92] training  491 of 1500: completed tf_agent.train(...) =    1.196 [loss]\n",
      "I0707 23:41:51.277235 15896 agents.py:92] training  492 of 1500: completed tf_agent.train(...) =    1.298 [loss]\n",
      "I0707 23:41:51.734236 15896 agents.py:92] training  493 of 1500: completed tf_agent.train(...) =    2.828 [loss]\n",
      "I0707 23:41:52.116239 15896 agents.py:92] training  494 of 1500: completed tf_agent.train(...) =    1.090 [loss]\n",
      "I0707 23:41:52.455239 15896 agents.py:92] training  495 of 1500: completed tf_agent.train(...) =    1.599 [loss]\n",
      "I0707 23:41:53.938233 15896 agents.py:92] current policy       : avg_reward=0.395, avg_steps=33.300\n",
      "I0707 23:41:54.457236 15896 agents.py:92] training  496 of 1500: completed tf_agent.train(...) =    1.079 [loss]\n",
      "I0707 23:41:54.924237 15896 agents.py:92] training  497 of 1500: completed tf_agent.train(...) =    1.676 [loss]\n",
      "I0707 23:41:55.384237 15896 agents.py:92] training  498 of 1500: completed tf_agent.train(...) =    2.681 [loss]\n",
      "I0707 23:41:55.772239 15896 agents.py:92] training  499 of 1500: completed tf_agent.train(...) =    1.999 [loss]\n",
      "I0707 23:41:56.218237 15896 agents.py:92] training  500 of 1500: completed tf_agent.train(...) =    1.351 [loss]\n",
      "I0707 23:41:57.682234 15896 agents.py:92] current policy       : avg_reward=0.542, avg_steps=32.500\n",
      "I0707 23:41:58.185235 15896 agents.py:92] training  501 of 1500: completed tf_agent.train(...) =    1.192 [loss]\n",
      "I0707 23:41:58.566237 15896 agents.py:92] training  502 of 1500: completed tf_agent.train(...) =    0.934 [loss]\n",
      "I0707 23:41:59.036235 15896 agents.py:92] training  503 of 1500: completed tf_agent.train(...) =    1.689 [loss]\n",
      "I0707 23:41:59.463236 15896 agents.py:92] training  504 of 1500: completed tf_agent.train(...) =    0.917 [loss]\n",
      "I0707 23:41:59.924239 15896 agents.py:92] training  505 of 1500: completed tf_agent.train(...) =    1.598 [loss]\n",
      "I0707 23:42:01.482234 15896 agents.py:92] current policy       : avg_reward=0.358, avg_steps=32.900\n",
      "I0707 23:42:01.917234 15896 agents.py:92] training  506 of 1500: completed tf_agent.train(...) =    2.078 [loss]\n",
      "I0707 23:42:02.384235 15896 agents.py:92] training  507 of 1500: completed tf_agent.train(...) =    1.373 [loss]\n",
      "I0707 23:42:02.805238 15896 agents.py:92] training  508 of 1500: completed tf_agent.train(...) =    0.974 [loss]\n",
      "I0707 23:42:03.255238 15896 agents.py:92] training  509 of 1500: completed tf_agent.train(...) =    1.182 [loss]\n",
      "I0707 23:42:03.675238 15896 agents.py:92] training  510 of 1500: completed tf_agent.train(...) =    5.144 [loss]\n",
      "I0707 23:42:05.162233 15896 agents.py:92] current policy       : avg_reward=0.393, avg_steps=33.300\n",
      "I0707 23:42:05.619238 15896 agents.py:92] training  511 of 1500: completed tf_agent.train(...) =    1.011 [loss]\n",
      "I0707 23:42:06.052237 15896 agents.py:92] training  512 of 1500: completed tf_agent.train(...) =    1.662 [loss]\n",
      "I0707 23:42:06.503237 15896 agents.py:92] training  513 of 1500: completed tf_agent.train(...) =    0.991 [loss]\n",
      "I0707 23:42:06.918236 15896 agents.py:92] training  514 of 1500: completed tf_agent.train(...) =    1.009 [loss]\n",
      "I0707 23:42:07.367237 15896 agents.py:92] training  515 of 1500: completed tf_agent.train(...) =    1.183 [loss]\n",
      "I0707 23:42:08.127235 15896 agents.py:92] current policy       : avg_reward=0.713, avg_steps=15.300\n",
      "I0707 23:42:08.473233 15896 agents.py:92] training  516 of 1500: completed tf_agent.train(...) =    0.609 [loss]\n",
      "I0707 23:42:08.825238 15896 agents.py:92] training  517 of 1500: completed tf_agent.train(...) =    1.098 [loss]\n",
      "I0707 23:42:09.294234 15896 agents.py:92] training  518 of 1500: completed tf_agent.train(...) =    0.722 [loss]\n",
      "I0707 23:42:09.798236 15896 agents.py:92] training  519 of 1500: completed tf_agent.train(...) =    2.101 [loss]\n",
      "I0707 23:42:10.243239 15896 agents.py:92] training  520 of 1500: completed tf_agent.train(...) =    1.066 [loss]\n",
      "I0707 23:42:11.779233 15896 agents.py:92] current policy       : avg_reward=0.500, avg_steps=33.900\n",
      "I0707 23:42:12.252234 15896 agents.py:92] training  521 of 1500: completed tf_agent.train(...) =    0.647 [loss]\n",
      "I0707 23:42:12.727233 15896 agents.py:92] training  522 of 1500: completed tf_agent.train(...) =    0.932 [loss]\n",
      "I0707 23:42:13.200236 15896 agents.py:92] training  523 of 1500: completed tf_agent.train(...) =    1.524 [loss]\n",
      "I0707 23:42:13.640234 15896 agents.py:92] training  524 of 1500: completed tf_agent.train(...) =    1.301 [loss]\n",
      "I0707 23:42:15.582270 15896 agents.py:92] training  525 of 1500: completed tf_agent.train(...) =  155.329 [loss]\n",
      "I0707 23:42:18.456233 15896 agents.py:92] current policy       : avg_reward=-0.123, avg_steps=69.100\n",
      "I0707 23:42:18.975237 15896 agents.py:92] training  526 of 1500: completed tf_agent.train(...) =    4.099 [loss]\n",
      "I0707 23:42:19.549236 15896 agents.py:92] training  527 of 1500: completed tf_agent.train(...) =    6.278 [loss]\n",
      "I0707 23:42:20.004237 15896 agents.py:92] training  528 of 1500: completed tf_agent.train(...) =    4.073 [loss]\n",
      "I0707 23:42:20.436238 15896 agents.py:92] training  529 of 1500: completed tf_agent.train(...) =    5.039 [loss]\n",
      "I0707 23:42:20.891238 15896 agents.py:92] training  530 of 1500: completed tf_agent.train(...) =    2.395 [loss]\n",
      "I0707 23:42:24.997234 15896 agents.py:92] current policy       : avg_reward=-0.473, avg_steps=88.100\n",
      "I0707 23:42:25.340234 15896 agents.py:92] training  531 of 1500: completed tf_agent.train(...) =    2.224 [loss]\n",
      "I0707 23:42:25.728235 15896 agents.py:92] training  532 of 1500: completed tf_agent.train(...) =    2.026 [loss]\n",
      "I0707 23:42:26.299237 15896 agents.py:92] training  533 of 1500: completed tf_agent.train(...) =    1.795 [loss]\n",
      "I0707 23:42:26.723241 15896 agents.py:92] training  534 of 1500: completed tf_agent.train(...) =    2.176 [loss]\n",
      "I0707 23:42:27.190237 15896 agents.py:92] training  535 of 1500: completed tf_agent.train(...) =    5.590 [loss]\n",
      "I0707 23:42:31.846233 15896 agents.py:92] current policy       : avg_reward=-0.821, avg_steps=106.500\n",
      "I0707 23:42:32.300238 15896 agents.py:92] training  536 of 1500: completed tf_agent.train(...) =    1.892 [loss]\n",
      "I0707 23:42:32.753236 15896 agents.py:92] training  537 of 1500: completed tf_agent.train(...) =   70.637 [loss]\n",
      "I0707 23:42:33.065237 15896 agents.py:92] training  538 of 1500: completed tf_agent.train(...) =    2.148 [loss]\n",
      "I0707 23:42:33.571237 15896 agents.py:92] training  539 of 1500: completed tf_agent.train(...) =    2.456 [loss]\n",
      "I0707 23:42:33.912244 15896 agents.py:92] training  540 of 1500: completed tf_agent.train(...) =    4.805 [loss]\n",
      "I0707 23:42:36.345237 15896 agents.py:92] current policy       : avg_reward=0.203, avg_steps=51.900\n",
      "I0707 23:42:36.742233 15896 agents.py:92] training  541 of 1500: completed tf_agent.train(...) =    1.501 [loss]\n",
      "I0707 23:42:37.073233 15896 agents.py:92] training  542 of 1500: completed tf_agent.train(...) =    1.864 [loss]\n",
      "I0707 23:42:37.557236 15896 agents.py:92] training  543 of 1500: completed tf_agent.train(...) =    2.144 [loss]\n",
      "I0707 23:42:37.957237 15896 agents.py:92] training  544 of 1500: completed tf_agent.train(...) =    0.860 [loss]\n",
      "I0707 23:42:38.371241 15896 agents.py:92] training  545 of 1500: completed tf_agent.train(...) =    1.115 [loss]\n",
      "I0707 23:42:40.783233 15896 agents.py:92] current policy       : avg_reward=0.068, avg_steps=51.200\n",
      "I0707 23:42:41.158236 15896 agents.py:92] training  546 of 1500: completed tf_agent.train(...) =    1.294 [loss]\n",
      "I0707 23:42:41.573237 15896 agents.py:92] training  547 of 1500: completed tf_agent.train(...) =    1.786 [loss]\n",
      "I0707 23:42:42.004235 15896 agents.py:92] training  548 of 1500: completed tf_agent.train(...) =    1.156 [loss]\n",
      "I0707 23:42:42.442238 15896 agents.py:92] training  549 of 1500: completed tf_agent.train(...) =    1.452 [loss]\n",
      "I0707 23:42:42.906236 15896 agents.py:92] training  550 of 1500: completed tf_agent.train(...) =    2.060 [loss]\n",
      "I0707 23:42:45.894232 15896 agents.py:92] current policy       : avg_reward=-0.173, avg_steps=70.800\n",
      "I0707 23:42:46.305238 15896 agents.py:92] training  551 of 1500: completed tf_agent.train(...) =    1.475 [loss]\n",
      "I0707 23:42:46.742234 15896 agents.py:92] training  552 of 1500: completed tf_agent.train(...) =    1.290 [loss]\n",
      "I0707 23:42:47.190234 15896 agents.py:92] training  553 of 1500: completed tf_agent.train(...) =    1.054 [loss]\n",
      "I0707 23:42:47.687237 15896 agents.py:92] training  554 of 1500: completed tf_agent.train(...) =    2.823 [loss]\n",
      "I0707 23:42:48.131237 15896 agents.py:92] training  555 of 1500: completed tf_agent.train(...) =    3.568 [loss]\n",
      "I0707 23:42:49.592269 15896 agents.py:92] current policy       : avg_reward=0.553, avg_steps=33.100\n",
      "I0707 23:42:50.076235 15896 agents.py:92] training  556 of 1500: completed tf_agent.train(...) =    5.101 [loss]\n",
      "I0707 23:42:50.522238 15896 agents.py:92] training  557 of 1500: completed tf_agent.train(...) =    1.255 [loss]\n",
      "I0707 23:42:50.947238 15896 agents.py:92] training  558 of 1500: completed tf_agent.train(...) =    1.225 [loss]\n",
      "I0707 23:42:51.378235 15896 agents.py:92] training  559 of 1500: completed tf_agent.train(...) =    0.769 [loss]\n",
      "I0707 23:42:51.802238 15896 agents.py:92] training  560 of 1500: completed tf_agent.train(...) =    1.662 [loss]\n",
      "I0707 23:42:53.300231 15896 agents.py:92] current policy       : avg_reward=0.333, avg_steps=33.500\n",
      "I0707 23:42:53.733237 15896 agents.py:92] training  561 of 1500: completed tf_agent.train(...) =    1.887 [loss]\n",
      "I0707 23:42:54.192237 15896 agents.py:92] training  562 of 1500: completed tf_agent.train(...) =    2.557 [loss]\n",
      "I0707 23:42:54.605238 15896 agents.py:92] training  563 of 1500: completed tf_agent.train(...) =    2.316 [loss]\n",
      "I0707 23:42:55.074234 15896 agents.py:92] training  564 of 1500: completed tf_agent.train(...) =    1.179 [loss]\n",
      "I0707 23:42:55.485234 15896 agents.py:92] training  565 of 1500: completed tf_agent.train(...) =    1.018 [loss]\n",
      "I0707 23:42:56.159234 15896 agents.py:92] current policy       : avg_reward=0.734, avg_steps=13.100\n",
      "I0707 23:42:56.611236 15896 agents.py:92] training  566 of 1500: completed tf_agent.train(...) =    2.325 [loss]\n",
      "I0707 23:42:57.062238 15896 agents.py:92] training  567 of 1500: completed tf_agent.train(...) =    0.937 [loss]\n",
      "I0707 23:42:57.496236 15896 agents.py:92] training  568 of 1500: completed tf_agent.train(...) =    0.758 [loss]\n",
      "I0707 23:42:57.960236 15896 agents.py:92] training  569 of 1500: completed tf_agent.train(...) =    2.206 [loss]\n",
      "I0707 23:42:58.436237 15896 agents.py:92] training  570 of 1500: completed tf_agent.train(...) =    1.010 [loss]\n",
      "I0707 23:43:01.390234 15896 agents.py:92] current policy       : avg_reward=-0.307, avg_steps=69.700\n",
      "I0707 23:43:01.847237 15896 agents.py:92] training  571 of 1500: completed tf_agent.train(...) =    0.966 [loss]\n",
      "I0707 23:43:02.334232 15896 agents.py:92] training  572 of 1500: completed tf_agent.train(...) =    0.656 [loss]\n",
      "I0707 23:43:02.703266 15896 agents.py:92] training  573 of 1500: completed tf_agent.train(...) =    1.524 [loss]\n",
      "I0707 23:43:03.068237 15896 agents.py:92] training  574 of 1500: completed tf_agent.train(...) =    3.217 [loss]\n",
      "I0707 23:43:03.510241 15896 agents.py:92] training  575 of 1500: completed tf_agent.train(...) =    2.709 [loss]\n",
      "I0707 23:43:05.047234 15896 agents.py:92] current policy       : avg_reward=0.495, avg_steps=33.900\n",
      "I0707 23:43:05.461237 15896 agents.py:92] training  576 of 1500: completed tf_agent.train(...) =    0.792 [loss]\n",
      "I0707 23:43:05.932236 15896 agents.py:92] training  577 of 1500: completed tf_agent.train(...) =    2.141 [loss]\n",
      "I0707 23:43:06.366240 15896 agents.py:92] training  578 of 1500: completed tf_agent.train(...) =    0.744 [loss]\n",
      "I0707 23:43:06.814241 15896 agents.py:92] training  579 of 1500: completed tf_agent.train(...) =    2.331 [loss]\n",
      "I0707 23:43:07.248238 15896 agents.py:92] training  580 of 1500: completed tf_agent.train(...) =    1.212 [loss]\n",
      "I0707 23:43:08.721234 15896 agents.py:92] current policy       : avg_reward=0.492, avg_steps=33.400\n",
      "I0707 23:43:09.155238 15896 agents.py:92] training  581 of 1500: completed tf_agent.train(...) =    3.985 [loss]\n",
      "I0707 23:43:09.527233 15896 agents.py:92] training  582 of 1500: completed tf_agent.train(...) =    1.304 [loss]\n",
      "I0707 23:43:09.868263 15896 agents.py:92] training  583 of 1500: completed tf_agent.train(...) =    2.465 [loss]\n",
      "I0707 23:43:10.303239 15896 agents.py:92] training  584 of 1500: completed tf_agent.train(...) =    1.430 [loss]\n",
      "I0707 23:43:10.680234 15896 agents.py:92] training  585 of 1500: completed tf_agent.train(...) =    0.807 [loss]\n",
      "I0707 23:43:12.914233 15896 agents.py:92] current policy       : avg_reward=-0.134, avg_steps=52.000\n",
      "I0707 23:43:13.282238 15896 agents.py:92] training  586 of 1500: completed tf_agent.train(...) =    0.778 [loss]\n",
      "I0707 23:43:13.646236 15896 agents.py:92] training  587 of 1500: completed tf_agent.train(...) =    5.052 [loss]\n",
      "I0707 23:43:14.058238 15896 agents.py:92] training  588 of 1500: completed tf_agent.train(...) =    0.680 [loss]\n",
      "I0707 23:43:15.961274 15896 agents.py:92] training  589 of 1500: completed tf_agent.train(...) =   52.308 [loss]\n",
      "I0707 23:43:16.396237 15896 agents.py:92] training  590 of 1500: completed tf_agent.train(...) =    1.272 [loss]\n",
      "I0707 23:43:19.327233 15896 agents.py:92] current policy       : avg_reward=-0.117, avg_steps=70.200\n",
      "I0707 23:43:19.796236 15896 agents.py:92] training  591 of 1500: completed tf_agent.train(...) =    1.899 [loss]\n",
      "I0707 23:43:20.246232 15896 agents.py:92] training  592 of 1500: completed tf_agent.train(...) =    2.117 [loss]\n",
      "I0707 23:43:20.613235 15896 agents.py:92] training  593 of 1500: completed tf_agent.train(...) =    2.121 [loss]\n",
      "I0707 23:43:20.943238 15896 agents.py:92] training  594 of 1500: completed tf_agent.train(...) =    1.082 [loss]\n",
      "I0707 23:43:21.404234 15896 agents.py:92] training  595 of 1500: completed tf_agent.train(...) =    1.773 [loss]\n",
      "I0707 23:43:22.888233 15896 agents.py:92] current policy       : avg_reward=0.376, avg_steps=33.800\n",
      "I0707 23:43:23.344236 15896 agents.py:92] training  596 of 1500: completed tf_agent.train(...) =    0.805 [loss]\n",
      "I0707 23:43:23.775235 15896 agents.py:92] training  597 of 1500: completed tf_agent.train(...) =    1.991 [loss]\n",
      "I0707 23:43:25.539234 15896 agents.py:92] training  598 of 1500: completed tf_agent.train(...) =    1.471 [loss]\n",
      "I0707 23:43:25.938238 15896 agents.py:92] training  599 of 1500: completed tf_agent.train(...) =    1.340 [loss]\n",
      "I0707 23:43:26.398238 15896 agents.py:92] training  600 of 1500: completed tf_agent.train(...) =    3.112 [loss]\n",
      "I0707 23:43:27.164233 15896 agents.py:92] current policy       : avg_reward=0.724, avg_steps=15.200\n",
      "I0707 23:43:27.507235 15896 agents.py:92] training  601 of 1500: completed tf_agent.train(...) =    1.325 [loss]\n",
      "I0707 23:43:27.873235 15896 agents.py:92] training  602 of 1500: completed tf_agent.train(...) =    2.390 [loss]\n",
      "I0707 23:43:28.320234 15896 agents.py:92] training  603 of 1500: completed tf_agent.train(...) =    1.937 [loss]\n",
      "I0707 23:43:28.664233 15896 agents.py:92] training  604 of 1500: completed tf_agent.train(...) =    3.001 [loss]\n",
      "I0707 23:43:29.090240 15896 agents.py:92] training  605 of 1500: completed tf_agent.train(...) =    1.039 [loss]\n",
      "I0707 23:43:29.687233 15896 agents.py:92] current policy       : avg_reward=0.716, avg_steps=14.100\n",
      "I0707 23:43:30.042696 15896 agents.py:92] training  606 of 1500: completed tf_agent.train(...) =    1.280 [loss]\n",
      "I0707 23:43:30.514694 15896 agents.py:92] training  607 of 1500: completed tf_agent.train(...) =    1.339 [loss]\n",
      "I0707 23:43:30.938697 15896 agents.py:92] training  608 of 1500: completed tf_agent.train(...) =    1.176 [loss]\n",
      "I0707 23:43:31.397694 15896 agents.py:92] training  609 of 1500: completed tf_agent.train(...) =    2.066 [loss]\n",
      "I0707 23:43:31.821697 15896 agents.py:92] training  610 of 1500: completed tf_agent.train(...) =    0.655 [loss]\n",
      "I0707 23:43:34.780726 15896 agents.py:92] current policy       : avg_reward=-0.054, avg_steps=70.600\n",
      "I0707 23:43:35.233695 15896 agents.py:92] training  611 of 1500: completed tf_agent.train(...) =    1.401 [loss]\n",
      "I0707 23:43:35.678693 15896 agents.py:92] training  612 of 1500: completed tf_agent.train(...) =    1.046 [loss]\n",
      "I0707 23:43:36.026694 15896 agents.py:92] training  613 of 1500: completed tf_agent.train(...) =    0.537 [loss]\n",
      "I0707 23:43:36.359695 15896 agents.py:92] training  614 of 1500: completed tf_agent.train(...) =    0.626 [loss]\n",
      "I0707 23:43:36.794698 15896 agents.py:92] training  615 of 1500: completed tf_agent.train(...) =    0.603 [loss]\n",
      "I0707 23:43:39.021693 15896 agents.py:92] current policy       : avg_reward=0.336, avg_steps=52.000\n",
      "I0707 23:43:39.478693 15896 agents.py:92] training  616 of 1500: completed tf_agent.train(...) =    1.528 [loss]\n",
      "I0707 23:43:39.885694 15896 agents.py:92] training  617 of 1500: completed tf_agent.train(...) =    1.627 [loss]\n",
      "I0707 23:43:41.699694 15896 agents.py:92] training  618 of 1500: completed tf_agent.train(...) =    0.690 [loss]\n",
      "I0707 23:43:43.556738 15896 agents.py:92] training  619 of 1500: completed tf_agent.train(...) =   28.010 [loss]\n",
      "I0707 23:43:44.022693 15896 agents.py:92] training  620 of 1500: completed tf_agent.train(...) =    1.901 [loss]\n",
      "I0707 23:43:46.249729 15896 agents.py:92] current policy       : avg_reward=-0.195, avg_steps=52.200\n",
      "I0707 23:43:46.737697 15896 agents.py:92] training  621 of 1500: completed tf_agent.train(...) =    2.861 [loss]\n",
      "I0707 23:43:47.140695 15896 agents.py:92] training  622 of 1500: completed tf_agent.train(...) =    1.803 [loss]\n",
      "I0707 23:43:47.594692 15896 agents.py:92] training  623 of 1500: completed tf_agent.train(...) =    1.361 [loss]\n",
      "I0707 23:43:47.996694 15896 agents.py:92] training  624 of 1500: completed tf_agent.train(...) =    0.844 [loss]\n",
      "I0707 23:43:48.432693 15896 agents.py:92] training  625 of 1500: completed tf_agent.train(...) =    1.305 [loss]\n",
      "I0707 23:43:49.167696 15896 agents.py:92] current policy       : avg_reward=0.725, avg_steps=14.500\n",
      "I0707 23:43:49.606693 15896 agents.py:92] training  626 of 1500: completed tf_agent.train(...) =    0.820 [loss]\n",
      "I0707 23:43:50.016696 15896 agents.py:92] training  627 of 1500: completed tf_agent.train(...) =    1.147 [loss]\n",
      "I0707 23:43:50.459691 15896 agents.py:92] training  628 of 1500: completed tf_agent.train(...) =    0.878 [loss]\n",
      "I0707 23:43:50.879696 15896 agents.py:92] training  629 of 1500: completed tf_agent.train(...) =    1.357 [loss]\n",
      "I0707 23:43:51.339690 15896 agents.py:92] training  630 of 1500: completed tf_agent.train(...) =    1.154 [loss]\n",
      "I0707 23:43:55.838720 15896 agents.py:92] current policy       : avg_reward=-0.585, avg_steps=107.200\n",
      "I0707 23:43:56.198697 15896 agents.py:92] training  631 of 1500: completed tf_agent.train(...) =    2.217 [loss]\n",
      "I0707 23:43:56.669696 15896 agents.py:92] training  632 of 1500: completed tf_agent.train(...) =    1.285 [loss]\n",
      "I0707 23:43:57.140698 15896 agents.py:92] training  633 of 1500: completed tf_agent.train(...) =    1.327 [loss]\n",
      "I0707 23:43:57.557696 15896 agents.py:92] training  634 of 1500: completed tf_agent.train(...) =    1.055 [loss]\n",
      "I0707 23:43:57.997695 15896 agents.py:92] training  635 of 1500: completed tf_agent.train(...) =    0.805 [loss]\n",
      "I0707 23:43:59.481695 15896 agents.py:92] current policy       : avg_reward=0.519, avg_steps=33.600\n",
      "I0707 23:43:59.930693 15896 agents.py:92] training  636 of 1500: completed tf_agent.train(...) =    4.674 [loss]\n",
      "I0707 23:44:00.370697 15896 agents.py:92] training  637 of 1500: completed tf_agent.train(...) =    1.262 [loss]\n",
      "I0707 23:44:00.806697 15896 agents.py:92] training  638 of 1500: completed tf_agent.train(...) =    3.473 [loss]\n",
      "I0707 23:44:01.152699 15896 agents.py:92] training  639 of 1500: completed tf_agent.train(...) =    0.705 [loss]\n",
      "I0707 23:44:01.447696 15896 agents.py:92] training  640 of 1500: completed tf_agent.train(...) =    0.938 [loss]\n",
      "I0707 23:44:02.903724 15896 agents.py:92] current policy       : avg_reward=0.050, avg_steps=32.100\n",
      "I0707 23:44:03.336695 15896 agents.py:92] training  641 of 1500: completed tf_agent.train(...) =    0.824 [loss]\n",
      "I0707 23:44:03.694696 15896 agents.py:92] training  642 of 1500: completed tf_agent.train(...) =    0.893 [loss]\n",
      "I0707 23:44:04.054690 15896 agents.py:92] training  643 of 1500: completed tf_agent.train(...) =    0.762 [loss]\n",
      "I0707 23:44:04.492699 15896 agents.py:92] training  644 of 1500: completed tf_agent.train(...) =    0.963 [loss]\n",
      "I0707 23:44:04.940693 15896 agents.py:92] training  645 of 1500: completed tf_agent.train(...) =    2.087 [loss]\n",
      "I0707 23:44:06.434697 15896 agents.py:92] current policy       : avg_reward=0.498, avg_steps=33.500\n",
      "I0707 23:44:06.872695 15896 agents.py:92] training  646 of 1500: completed tf_agent.train(...) =    1.165 [loss]\n",
      "I0707 23:44:07.294695 15896 agents.py:92] training  647 of 1500: completed tf_agent.train(...) =    1.167 [loss]\n",
      "I0707 23:44:07.748693 15896 agents.py:92] training  648 of 1500: completed tf_agent.train(...) =    2.868 [loss]\n",
      "I0707 23:44:08.207695 15896 agents.py:92] training  649 of 1500: completed tf_agent.train(...) =    1.851 [loss]\n",
      "I0707 23:44:08.612699 15896 agents.py:92] training  650 of 1500: completed tf_agent.train(...) =    0.719 [loss]\n",
      "I0707 23:44:10.087722 15896 agents.py:92] current policy       : avg_reward=0.510, avg_steps=33.200\n",
      "I0707 23:44:10.583693 15896 agents.py:92] training  651 of 1500: completed tf_agent.train(...) =    1.370 [loss]\n",
      "I0707 23:44:11.044695 15896 agents.py:92] training  652 of 1500: completed tf_agent.train(...) =    2.333 [loss]\n",
      "I0707 23:44:11.470692 15896 agents.py:92] training  653 of 1500: completed tf_agent.train(...) =    0.790 [loss]\n",
      "I0707 23:44:11.852698 15896 agents.py:92] training  654 of 1500: completed tf_agent.train(...) =    0.831 [loss]\n",
      "I0707 23:44:12.334694 15896 agents.py:92] training  655 of 1500: completed tf_agent.train(...) =    0.716 [loss]\n",
      "I0707 23:44:13.799692 15896 agents.py:92] current policy       : avg_reward=0.517, avg_steps=33.100\n",
      "I0707 23:44:14.225693 15896 agents.py:92] training  656 of 1500: completed tf_agent.train(...) =    0.570 [loss]\n",
      "I0707 23:44:14.637695 15896 agents.py:92] training  657 of 1500: completed tf_agent.train(...) =    0.757 [loss]\n",
      "I0707 23:44:15.064694 15896 agents.py:92] training  658 of 1500: completed tf_agent.train(...) =    0.731 [loss]\n",
      "I0707 23:44:15.491697 15896 agents.py:92] training  659 of 1500: completed tf_agent.train(...) =   16.510 [loss]\n",
      "I0707 23:44:15.900697 15896 agents.py:92] training  660 of 1500: completed tf_agent.train(...) =    2.253 [loss]\n",
      "I0707 23:44:17.420695 15896 agents.py:92] current policy       : avg_reward=0.497, avg_steps=34.000\n",
      "I0707 23:44:17.849695 15896 agents.py:92] training  661 of 1500: completed tf_agent.train(...) =    1.190 [loss]\n",
      "I0707 23:44:18.285695 15896 agents.py:92] training  662 of 1500: completed tf_agent.train(...) =    0.782 [loss]\n",
      "I0707 23:44:18.695695 15896 agents.py:92] training  663 of 1500: completed tf_agent.train(...) =    2.830 [loss]\n",
      "I0707 23:44:19.145694 15896 agents.py:92] training  664 of 1500: completed tf_agent.train(...) =    0.757 [loss]\n",
      "I0707 23:44:19.537697 15896 agents.py:92] training  665 of 1500: completed tf_agent.train(...) =    0.719 [loss]\n",
      "I0707 23:44:20.214728 15896 agents.py:92] current policy       : avg_reward=0.716, avg_steps=13.100\n",
      "I0707 23:44:20.649697 15896 agents.py:92] training  666 of 1500: completed tf_agent.train(...) =    0.859 [loss]\n",
      "I0707 23:44:21.082695 15896 agents.py:92] training  667 of 1500: completed tf_agent.train(...) =    0.857 [loss]\n",
      "I0707 23:44:21.521694 15896 agents.py:92] training  668 of 1500: completed tf_agent.train(...) =    0.800 [loss]\n",
      "I0707 23:44:21.951699 15896 agents.py:92] training  669 of 1500: completed tf_agent.train(...) =    1.063 [loss]\n",
      "I0707 23:44:22.392694 15896 agents.py:92] training  670 of 1500: completed tf_agent.train(...) =    0.629 [loss]\n",
      "I0707 23:44:23.131691 15896 agents.py:92] current policy       : avg_reward=0.690, avg_steps=14.900\n",
      "I0707 23:44:23.547698 15896 agents.py:92] training  671 of 1500: completed tf_agent.train(...) =    0.704 [loss]\n",
      "I0707 23:44:23.995694 15896 agents.py:92] training  672 of 1500: completed tf_agent.train(...) =    1.107 [loss]\n",
      "I0707 23:44:24.427692 15896 agents.py:92] training  673 of 1500: completed tf_agent.train(...) =    0.804 [loss]\n",
      "I0707 23:44:24.859696 15896 agents.py:92] training  674 of 1500: completed tf_agent.train(...) =    0.641 [loss]\n",
      "I0707 23:44:25.296694 15896 agents.py:92] training  675 of 1500: completed tf_agent.train(...) =    0.544 [loss]\n",
      "I0707 23:44:26.808722 15896 agents.py:92] current policy       : avg_reward=0.322, avg_steps=33.700\n",
      "I0707 23:44:27.250699 15896 agents.py:92] training  676 of 1500: completed tf_agent.train(...) =    0.702 [loss]\n",
      "I0707 23:44:27.633691 15896 agents.py:92] training  677 of 1500: completed tf_agent.train(...) =    0.993 [loss]\n",
      "I0707 23:44:27.941697 15896 agents.py:92] training  678 of 1500: completed tf_agent.train(...) =    0.755 [loss]\n",
      "I0707 23:44:28.400694 15896 agents.py:92] training  679 of 1500: completed tf_agent.train(...) =    1.004 [loss]\n",
      "I0707 23:44:28.741694 15896 agents.py:92] training  680 of 1500: completed tf_agent.train(...) =    1.043 [loss]\n",
      "I0707 23:44:29.495692 15896 agents.py:92] current policy       : avg_reward=0.713, avg_steps=15.100\n",
      "I0707 23:44:29.827709 15896 agents.py:92] training  681 of 1500: completed tf_agent.train(...) =    1.176 [loss]\n",
      "I0707 23:44:30.225698 15896 agents.py:92] training  682 of 1500: completed tf_agent.train(...) =    4.817 [loss]\n",
      "I0707 23:44:30.660693 15896 agents.py:92] training  683 of 1500: completed tf_agent.train(...) =    0.868 [loss]\n",
      "I0707 23:44:31.007708 15896 agents.py:92] training  684 of 1500: completed tf_agent.train(...) =    1.536 [loss]\n",
      "I0707 23:44:31.416698 15896 agents.py:92] training  685 of 1500: completed tf_agent.train(...) =    1.029 [loss]\n",
      "I0707 23:44:32.906720 15896 agents.py:92] current policy       : avg_reward=0.505, avg_steps=33.400\n",
      "I0707 23:44:33.288698 15896 agents.py:92] training  686 of 1500: completed tf_agent.train(...) =    0.873 [loss]\n",
      "I0707 23:44:33.729695 15896 agents.py:92] training  687 of 1500: completed tf_agent.train(...) =    0.900 [loss]\n",
      "I0707 23:44:34.109695 15896 agents.py:92] training  688 of 1500: completed tf_agent.train(...) =    0.633 [loss]\n",
      "I0707 23:44:34.554693 15896 agents.py:92] training  689 of 1500: completed tf_agent.train(...) =    0.721 [loss]\n",
      "I0707 23:44:34.903714 15896 agents.py:92] training  690 of 1500: completed tf_agent.train(...) =    0.439 [loss]\n",
      "I0707 23:44:35.462707 15896 agents.py:92] current policy       : avg_reward=0.729, avg_steps=12.900\n",
      "I0707 23:44:35.906691 15896 agents.py:92] training  691 of 1500: completed tf_agent.train(...) =    0.661 [loss]\n",
      "I0707 23:44:36.351695 15896 agents.py:92] training  692 of 1500: completed tf_agent.train(...) =    0.788 [loss]\n",
      "I0707 23:44:36.785696 15896 agents.py:92] training  693 of 1500: completed tf_agent.train(...) =    0.634 [loss]\n",
      "I0707 23:44:37.218696 15896 agents.py:92] training  694 of 1500: completed tf_agent.train(...) =    0.497 [loss]\n",
      "I0707 23:44:37.616695 15896 agents.py:92] training  695 of 1500: completed tf_agent.train(...) =    0.675 [loss]\n",
      "I0707 23:44:38.357720 15896 agents.py:92] current policy       : avg_reward=0.693, avg_steps=15.000\n",
      "I0707 23:44:38.703692 15896 agents.py:92] training  696 of 1500: completed tf_agent.train(...) =    0.946 [loss]\n",
      "I0707 23:44:39.066693 15896 agents.py:92] training  697 of 1500: completed tf_agent.train(...) =    0.717 [loss]\n",
      "I0707 23:44:39.536693 15896 agents.py:92] training  698 of 1500: completed tf_agent.train(...) =    0.641 [loss]\n",
      "I0707 23:44:39.965695 15896 agents.py:92] training  699 of 1500: completed tf_agent.train(...) =    0.962 [loss]\n",
      "I0707 23:44:40.387697 15896 agents.py:92] training  700 of 1500: completed tf_agent.train(...) =    0.933 [loss]\n",
      "I0707 23:44:41.113693 15896 agents.py:92] current policy       : avg_reward=0.726, avg_steps=13.700\n",
      "I0707 23:44:41.536697 15896 agents.py:92] training  701 of 1500: completed tf_agent.train(...) =    0.611 [loss]\n",
      "I0707 23:44:41.947695 15896 agents.py:92] training  702 of 1500: completed tf_agent.train(...) =    0.444 [loss]\n",
      "I0707 23:44:42.397696 15896 agents.py:92] training  703 of 1500: completed tf_agent.train(...) =    0.718 [loss]\n",
      "I0707 23:44:42.809696 15896 agents.py:92] training  704 of 1500: completed tf_agent.train(...) =    0.847 [loss]\n",
      "I0707 23:44:43.282695 15896 agents.py:92] training  705 of 1500: completed tf_agent.train(...) =    2.873 [loss]\n",
      "I0707 23:44:44.010721 15896 agents.py:92] current policy       : avg_reward=0.700, avg_steps=14.800\n",
      "I0707 23:44:44.417693 15896 agents.py:92] training  706 of 1500: completed tf_agent.train(...) =    0.859 [loss]\n",
      "I0707 23:44:46.211691 15896 agents.py:92] training  707 of 1500: completed tf_agent.train(...) =    0.658 [loss]\n",
      "I0707 23:44:46.653694 15896 agents.py:92] training  708 of 1500: completed tf_agent.train(...) =    0.562 [loss]\n",
      "I0707 23:44:47.069692 15896 agents.py:92] training  709 of 1500: completed tf_agent.train(...) =    0.536 [loss]\n",
      "I0707 23:44:47.531692 15896 agents.py:92] training  710 of 1500: completed tf_agent.train(...) =    0.645 [loss]\n",
      "I0707 23:44:48.285691 15896 agents.py:92] current policy       : avg_reward=0.716, avg_steps=15.100\n",
      "I0707 23:44:48.634696 15896 agents.py:92] training  711 of 1500: completed tf_agent.train(...) =    0.680 [loss]\n",
      "I0707 23:44:49.036696 15896 agents.py:92] training  712 of 1500: completed tf_agent.train(...) =    2.454 [loss]\n",
      "I0707 23:44:49.507693 15896 agents.py:92] training  713 of 1500: completed tf_agent.train(...) =    0.829 [loss]\n",
      "I0707 23:44:49.911696 15896 agents.py:92] training  714 of 1500: completed tf_agent.train(...) =    0.937 [loss]\n",
      "I0707 23:44:51.731693 15896 agents.py:92] training  715 of 1500: completed tf_agent.train(...) =    0.733 [loss]\n",
      "I0707 23:44:52.494728 15896 agents.py:92] current policy       : avg_reward=0.697, avg_steps=15.300\n",
      "I0707 23:44:52.941696 15896 agents.py:92] training  716 of 1500: completed tf_agent.train(...) =    1.622 [loss]\n",
      "I0707 23:44:53.364696 15896 agents.py:92] training  717 of 1500: completed tf_agent.train(...) =    0.905 [loss]\n",
      "I0707 23:44:53.798696 15896 agents.py:92] training  718 of 1500: completed tf_agent.train(...) =    0.566 [loss]\n",
      "I0707 23:44:54.168691 15896 agents.py:92] training  719 of 1500: completed tf_agent.train(...) =    0.934 [loss]\n",
      "I0707 23:44:54.483691 15896 agents.py:92] training  720 of 1500: completed tf_agent.train(...) =    1.126 [loss]\n",
      "I0707 23:44:56.025727 15896 agents.py:92] current policy       : avg_reward=0.497, avg_steps=34.100\n",
      "I0707 23:44:56.376696 15896 agents.py:92] training  721 of 1500: completed tf_agent.train(...) =    0.540 [loss]\n",
      "I0707 23:44:56.759692 15896 agents.py:92] training  722 of 1500: completed tf_agent.train(...) =    1.163 [loss]\n",
      "I0707 23:44:57.234692 15896 agents.py:92] training  723 of 1500: completed tf_agent.train(...) =    1.084 [loss]\n",
      "I0707 23:44:57.710692 15896 agents.py:92] training  724 of 1500: completed tf_agent.train(...) =    0.802 [loss]\n",
      "I0707 23:44:58.163696 15896 agents.py:92] training  725 of 1500: completed tf_agent.train(...) =    0.619 [loss]\n",
      "I0707 23:44:59.698699 15896 agents.py:92] current policy       : avg_reward=0.503, avg_steps=34.000\n",
      "I0707 23:45:00.198691 15896 agents.py:92] training  726 of 1500: completed tf_agent.train(...) =    0.573 [loss]\n",
      "I0707 23:45:00.532693 15896 agents.py:92] training  727 of 1500: completed tf_agent.train(...) =    0.613 [loss]\n",
      "I0707 23:45:00.835694 15896 agents.py:92] training  728 of 1500: completed tf_agent.train(...) =    0.380 [loss]\n",
      "I0707 23:45:01.315693 15896 agents.py:92] training  729 of 1500: completed tf_agent.train(...) =    0.891 [loss]\n",
      "I0707 23:45:01.778698 15896 agents.py:92] training  730 of 1500: completed tf_agent.train(...) =    1.171 [loss]\n",
      "I0707 23:45:03.295726 15896 agents.py:92] current policy       : avg_reward=0.351, avg_steps=34.100\n",
      "I0707 23:45:03.739695 15896 agents.py:92] training  731 of 1500: completed tf_agent.train(...) =    0.824 [loss]\n",
      "I0707 23:45:04.199694 15896 agents.py:92] training  732 of 1500: completed tf_agent.train(...) =    1.388 [loss]\n",
      "I0707 23:45:04.606692 15896 agents.py:92] training  733 of 1500: completed tf_agent.train(...) =    1.448 [loss]\n",
      "I0707 23:45:05.056696 15896 agents.py:92] training  734 of 1500: completed tf_agent.train(...) =    0.811 [loss]\n",
      "I0707 23:45:05.495695 15896 agents.py:92] training  735 of 1500: completed tf_agent.train(...) =    1.298 [loss]\n",
      "I0707 23:45:06.286722 15896 agents.py:92] current policy       : avg_reward=0.700, avg_steps=15.100\n",
      "I0707 23:45:06.726694 15896 agents.py:92] training  736 of 1500: completed tf_agent.train(...) =    0.547 [loss]\n",
      "I0707 23:45:07.156694 15896 agents.py:92] training  737 of 1500: completed tf_agent.train(...) =    0.673 [loss]\n",
      "I0707 23:45:07.588693 15896 agents.py:92] training  738 of 1500: completed tf_agent.train(...) =    0.660 [loss]\n",
      "I0707 23:45:08.009695 15896 agents.py:92] training  739 of 1500: completed tf_agent.train(...) =    0.519 [loss]\n",
      "I0707 23:45:08.411696 15896 agents.py:92] training  740 of 1500: completed tf_agent.train(...) =    0.475 [loss]\n",
      "I0707 23:45:10.608696 15896 agents.py:92] current policy       : avg_reward=0.348, avg_steps=51.800\n",
      "I0707 23:45:11.057694 15896 agents.py:92] training  741 of 1500: completed tf_agent.train(...) =    1.269 [loss]\n",
      "I0707 23:45:11.519692 15896 agents.py:92] training  742 of 1500: completed tf_agent.train(...) =    0.824 [loss]\n",
      "I0707 23:45:11.970697 15896 agents.py:92] training  743 of 1500: completed tf_agent.train(...) =    0.877 [loss]\n",
      "I0707 23:45:12.407694 15896 agents.py:92] training  744 of 1500: completed tf_agent.train(...) =    0.428 [loss]\n",
      "I0707 23:45:12.861694 15896 agents.py:92] training  745 of 1500: completed tf_agent.train(...) =    0.583 [loss]\n",
      "I0707 23:45:13.573697 15896 agents.py:92] current policy       : avg_reward=0.729, avg_steps=13.600\n",
      "I0707 23:45:14.013692 15896 agents.py:92] training  746 of 1500: completed tf_agent.train(...) =    1.322 [loss]\n",
      "I0707 23:45:14.480697 15896 agents.py:92] training  747 of 1500: completed tf_agent.train(...) =    0.898 [loss]\n",
      "I0707 23:45:14.904698 15896 agents.py:92] training  748 of 1500: completed tf_agent.train(...) =    0.962 [loss]\n",
      "I0707 23:45:15.410693 15896 agents.py:92] training  749 of 1500: completed tf_agent.train(...) =    1.239 [loss]\n",
      "I0707 23:45:15.849696 15896 agents.py:92] training  750 of 1500: completed tf_agent.train(...) =    2.387 [loss]\n",
      "I0707 23:45:17.424726 15896 agents.py:92] current policy       : avg_reward=0.493, avg_steps=33.700\n",
      "I0707 23:45:17.899692 15896 agents.py:92] training  751 of 1500: completed tf_agent.train(...) =    0.746 [loss]\n",
      "I0707 23:45:18.241693 15896 agents.py:92] training  752 of 1500: completed tf_agent.train(...) =    0.678 [loss]\n",
      "I0707 23:45:18.617693 15896 agents.py:92] training  753 of 1500: completed tf_agent.train(...) =    0.940 [loss]\n",
      "I0707 23:45:19.088692 15896 agents.py:92] training  754 of 1500: completed tf_agent.train(...) =    0.618 [loss]\n",
      "I0707 23:45:19.551696 15896 agents.py:92] training  755 of 1500: completed tf_agent.train(...) =    0.633 [loss]\n",
      "I0707 23:45:20.257699 15896 agents.py:92] current policy       : avg_reward=0.718, avg_steps=14.300\n",
      "I0707 23:45:20.657697 15896 agents.py:92] training  756 of 1500: completed tf_agent.train(...) =    0.657 [loss]\n",
      "I0707 23:45:21.095694 15896 agents.py:92] training  757 of 1500: completed tf_agent.train(...) =    0.560 [loss]\n",
      "I0707 23:45:21.553693 15896 agents.py:92] training  758 of 1500: completed tf_agent.train(...) =    0.585 [loss]\n",
      "I0707 23:45:21.987694 15896 agents.py:92] training  759 of 1500: completed tf_agent.train(...) =    1.264 [loss]\n",
      "I0707 23:45:22.448697 15896 agents.py:92] training  760 of 1500: completed tf_agent.train(...) =    0.452 [loss]\n",
      "I0707 23:45:23.963699 15896 agents.py:92] current policy       : avg_reward=0.523, avg_steps=33.300\n",
      "I0707 23:45:24.420694 15896 agents.py:92] training  761 of 1500: completed tf_agent.train(...) =    0.852 [loss]\n",
      "I0707 23:45:24.899695 15896 agents.py:92] training  762 of 1500: completed tf_agent.train(...) =    2.978 [loss]\n",
      "I0707 23:45:25.336695 15896 agents.py:92] training  763 of 1500: completed tf_agent.train(...) =    0.777 [loss]\n",
      "I0707 23:45:25.837696 15896 agents.py:92] training  764 of 1500: completed tf_agent.train(...) =    0.753 [loss]\n",
      "I0707 23:45:26.291693 15896 agents.py:92] training  765 of 1500: completed tf_agent.train(...) =    0.766 [loss]\n",
      "I0707 23:45:27.101698 15896 agents.py:92] current policy       : avg_reward=0.725, avg_steps=13.900\n",
      "I0707 23:45:27.565696 15896 agents.py:92] training  766 of 1500: completed tf_agent.train(...) =    0.423 [loss]\n",
      "I0707 23:45:28.260696 15896 agents.py:92] training  767 of 1500: completed tf_agent.train(...) =    0.485 [loss]\n",
      "I0707 23:45:28.753697 15896 agents.py:92] training  768 of 1500: completed tf_agent.train(...) =    0.615 [loss]\n",
      "I0707 23:45:30.606693 15896 agents.py:92] training  769 of 1500: completed tf_agent.train(...) =    0.483 [loss]\n",
      "I0707 23:45:30.937694 15896 agents.py:92] training  770 of 1500: completed tf_agent.train(...) =    0.654 [loss]\n",
      "I0707 23:45:31.699722 15896 agents.py:92] current policy       : avg_reward=0.699, avg_steps=15.900\n",
      "I0707 23:45:32.046695 15896 agents.py:92] training  771 of 1500: completed tf_agent.train(...) =    0.418 [loss]\n",
      "I0707 23:45:32.516694 15896 agents.py:92] training  772 of 1500: completed tf_agent.train(...) =    0.754 [loss]\n",
      "I0707 23:45:32.989696 15896 agents.py:92] training  773 of 1500: completed tf_agent.train(...) =    0.594 [loss]\n",
      "I0707 23:45:33.338699 15896 agents.py:92] training  774 of 1500: completed tf_agent.train(...) =    0.550 [loss]\n",
      "I0707 23:45:33.674706 15896 agents.py:92] training  775 of 1500: completed tf_agent.train(...) =    0.744 [loss]\n",
      "I0707 23:45:34.405721 15896 agents.py:92] current policy       : avg_reward=0.738, avg_steps=14.000\n",
      "I0707 23:45:34.872693 15896 agents.py:92] training  776 of 1500: completed tf_agent.train(...) =    0.587 [loss]\n",
      "I0707 23:45:35.338693 15896 agents.py:92] training  777 of 1500: completed tf_agent.train(...) =    0.556 [loss]\n",
      "I0707 23:45:35.806694 15896 agents.py:92] training  778 of 1500: completed tf_agent.train(...) =    1.336 [loss]\n",
      "I0707 23:45:36.219696 15896 agents.py:92] training  779 of 1500: completed tf_agent.train(...) =    0.521 [loss]\n",
      "I0707 23:45:36.665693 15896 agents.py:92] training  780 of 1500: completed tf_agent.train(...) =    0.527 [loss]\n",
      "I0707 23:45:38.181695 15896 agents.py:92] current policy       : avg_reward=0.537, avg_steps=32.900\n",
      "I0707 23:45:38.636694 15896 agents.py:92] training  781 of 1500: completed tf_agent.train(...) =    0.637 [loss]\n",
      "I0707 23:45:39.084696 15896 agents.py:92] training  782 of 1500: completed tf_agent.train(...) =    0.530 [loss]\n",
      "I0707 23:45:39.547693 15896 agents.py:92] training  783 of 1500: completed tf_agent.train(...) =    0.726 [loss]\n",
      "I0707 23:45:40.005693 15896 agents.py:92] training  784 of 1500: completed tf_agent.train(...) =    0.750 [loss]\n",
      "I0707 23:45:40.508695 15896 agents.py:92] training  785 of 1500: completed tf_agent.train(...) =    2.126 [loss]\n",
      "I0707 23:45:41.324727 15896 agents.py:92] current policy       : avg_reward=0.683, avg_steps=16.300\n",
      "I0707 23:45:41.819695 15896 agents.py:92] training  786 of 1500: completed tf_agent.train(...) =    0.855 [loss]\n",
      "I0707 23:45:42.250693 15896 agents.py:92] training  787 of 1500: completed tf_agent.train(...) =    0.538 [loss]\n",
      "I0707 23:45:42.716696 15896 agents.py:92] training  788 of 1500: completed tf_agent.train(...) =    0.620 [loss]\n",
      "I0707 23:45:43.154692 15896 agents.py:92] training  789 of 1500: completed tf_agent.train(...) =    0.723 [loss]\n",
      "I0707 23:45:43.621693 15896 agents.py:92] training  790 of 1500: completed tf_agent.train(...) =    0.459 [loss]\n",
      "I0707 23:45:44.304721 15896 agents.py:92] current policy       : avg_reward=0.727, avg_steps=13.800\n",
      "I0707 23:45:44.657720 15896 agents.py:92] training  791 of 1500: completed tf_agent.train(...) =    0.508 [loss]\n",
      "I0707 23:45:45.187693 15896 agents.py:92] training  792 of 1500: completed tf_agent.train(...) =    0.765 [loss]\n",
      "I0707 23:45:45.639699 15896 agents.py:92] training  793 of 1500: completed tf_agent.train(...) =    0.646 [loss]\n",
      "I0707 23:45:46.108694 15896 agents.py:92] training  794 of 1500: completed tf_agent.train(...) =    0.698 [loss]\n",
      "I0707 23:45:46.554691 15896 agents.py:92] training  795 of 1500: completed tf_agent.train(...) =    0.493 [loss]\n",
      "I0707 23:45:47.310701 15896 agents.py:92] current policy       : avg_reward=0.730, avg_steps=13.400\n",
      "I0707 23:45:47.788695 15896 agents.py:92] training  796 of 1500: completed tf_agent.train(...) =    0.532 [loss]\n",
      "I0707 23:45:48.221697 15896 agents.py:92] training  797 of 1500: completed tf_agent.train(...) =    0.604 [loss]\n",
      "I0707 23:45:48.734695 15896 agents.py:92] training  798 of 1500: completed tf_agent.train(...) =    0.852 [loss]\n",
      "I0707 23:45:49.142693 15896 agents.py:92] training  799 of 1500: completed tf_agent.train(...) =    0.445 [loss]\n",
      "I0707 23:45:49.581693 15896 agents.py:92] training  800 of 1500: completed tf_agent.train(...) =    0.608 [loss]\n",
      "I0707 23:45:50.354696 15896 agents.py:92] current policy       : avg_reward=0.717, avg_steps=14.200\n",
      "I0707 23:45:50.728691 15896 agents.py:92] training  801 of 1500: completed tf_agent.train(...) =    0.755 [loss]\n",
      "I0707 23:45:51.129692 15896 agents.py:92] training  802 of 1500: completed tf_agent.train(...) =    0.525 [loss]\n",
      "I0707 23:45:51.575691 15896 agents.py:92] training  803 of 1500: completed tf_agent.train(...) =    0.534 [loss]\n",
      "I0707 23:45:52.077697 15896 agents.py:92] training  804 of 1500: completed tf_agent.train(...) =    2.756 [loss]\n",
      "I0707 23:45:52.688695 15896 agents.py:92] training  805 of 1500: completed tf_agent.train(...) =    0.594 [loss]\n",
      "I0707 23:45:53.455721 15896 agents.py:92] current policy       : avg_reward=0.713, avg_steps=14.700\n",
      "I0707 23:45:53.823692 15896 agents.py:92] training  806 of 1500: completed tf_agent.train(...) =    0.501 [loss]\n",
      "I0707 23:45:54.166696 15896 agents.py:92] training  807 of 1500: completed tf_agent.train(...) =    0.493 [loss]\n",
      "I0707 23:45:54.593696 15896 agents.py:92] training  808 of 1500: completed tf_agent.train(...) =    0.584 [loss]\n",
      "I0707 23:45:54.901693 15896 agents.py:92] training  809 of 1500: completed tf_agent.train(...) =    0.806 [loss]\n",
      "I0707 23:45:55.361694 15896 agents.py:92] training  810 of 1500: completed tf_agent.train(...) =    0.595 [loss]\n",
      "I0707 23:45:57.506698 15896 agents.py:92] current policy       : avg_reward=0.375, avg_steps=50.200\n",
      "I0707 23:45:57.955693 15896 agents.py:92] training  811 of 1500: completed tf_agent.train(...) =    2.896 [loss]\n",
      "I0707 23:45:58.350693 15896 agents.py:92] training  812 of 1500: completed tf_agent.train(...) =    1.855 [loss]\n",
      "I0707 23:45:58.710728 15896 agents.py:92] training  813 of 1500: completed tf_agent.train(...) =    0.666 [loss]\n",
      "I0707 23:45:59.209693 15896 agents.py:92] training  814 of 1500: completed tf_agent.train(...) =    0.512 [loss]\n",
      "I0707 23:45:59.644695 15896 agents.py:92] training  815 of 1500: completed tf_agent.train(...) =    1.326 [loss]\n",
      "I0707 23:46:01.183720 15896 agents.py:92] current policy       : avg_reward=0.512, avg_steps=34.300\n",
      "I0707 23:46:01.675693 15896 agents.py:92] training  816 of 1500: completed tf_agent.train(...) =    0.810 [loss]\n",
      "I0707 23:46:02.124697 15896 agents.py:92] training  817 of 1500: completed tf_agent.train(...) =    1.471 [loss]\n",
      "I0707 23:46:02.573695 15896 agents.py:92] training  818 of 1500: completed tf_agent.train(...) =    0.605 [loss]\n",
      "I0707 23:46:02.909693 15896 agents.py:92] training  819 of 1500: completed tf_agent.train(...) =    0.510 [loss]\n",
      "I0707 23:46:03.364694 15896 agents.py:92] training  820 of 1500: completed tf_agent.train(...) =    0.575 [loss]\n",
      "I0707 23:46:04.081727 15896 agents.py:92] current policy       : avg_reward=0.706, avg_steps=14.900\n",
      "I0707 23:46:04.581691 15896 agents.py:92] training  821 of 1500: completed tf_agent.train(...) =    6.739 [loss]\n",
      "I0707 23:46:04.921695 15896 agents.py:92] training  822 of 1500: completed tf_agent.train(...) =    0.854 [loss]\n",
      "I0707 23:46:05.281697 15896 agents.py:92] training  823 of 1500: completed tf_agent.train(...) =    0.892 [loss]\n",
      "I0707 23:46:05.638692 15896 agents.py:92] training  824 of 1500: completed tf_agent.train(...) =    0.593 [loss]\n",
      "I0707 23:46:06.073693 15896 agents.py:92] training  825 of 1500: completed tf_agent.train(...) =    0.583 [loss]\n",
      "I0707 23:46:08.241721 15896 agents.py:92] current policy       : avg_reward=0.345, avg_steps=50.700\n",
      "I0707 23:46:08.696696 15896 agents.py:92] training  826 of 1500: completed tf_agent.train(...) =    0.497 [loss]\n",
      "I0707 23:46:09.187695 15896 agents.py:92] training  827 of 1500: completed tf_agent.train(...) =    1.442 [loss]\n",
      "I0707 23:46:09.624693 15896 agents.py:92] training  828 of 1500: completed tf_agent.train(...) =    0.835 [loss]\n",
      "I0707 23:46:10.084694 15896 agents.py:92] training  829 of 1500: completed tf_agent.train(...) =    0.522 [loss]\n",
      "I0707 23:46:10.509695 15896 agents.py:92] training  830 of 1500: completed tf_agent.train(...) =    0.661 [loss]\n",
      "I0707 23:46:12.737726 15896 agents.py:92] current policy       : avg_reward=0.289, avg_steps=53.000\n",
      "I0707 23:46:13.111695 15896 agents.py:92] training  831 of 1500: completed tf_agent.train(...) =    0.612 [loss]\n",
      "I0707 23:46:13.507693 15896 agents.py:92] training  832 of 1500: completed tf_agent.train(...) =    6.428 [loss]\n",
      "I0707 23:46:13.964692 15896 agents.py:92] training  833 of 1500: completed tf_agent.train(...) =    0.845 [loss]\n",
      "I0707 23:46:14.378724 15896 agents.py:92] training  834 of 1500: completed tf_agent.train(...) =   76.093 [loss]\n",
      "I0707 23:46:14.716732 15896 agents.py:92] training  835 of 1500: completed tf_agent.train(...) =    4.842 [loss]\n",
      "I0707 23:46:15.478721 15896 agents.py:92] current policy       : avg_reward=0.730, avg_steps=15.100\n",
      "I0707 23:46:15.936697 15896 agents.py:92] training  836 of 1500: completed tf_agent.train(...) =    2.989 [loss]\n",
      "I0707 23:46:16.351694 15896 agents.py:92] training  837 of 1500: completed tf_agent.train(...) =    1.745 [loss]\n",
      "I0707 23:46:18.332690 15896 agents.py:92] training  838 of 1500: completed tf_agent.train(...) = 2160.477 [loss]\n",
      "I0707 23:46:20.362696 15896 agents.py:92] training  839 of 1500: completed tf_agent.train(...) = 1443.790 [loss]\n",
      "I0707 23:46:20.738727 15896 agents.py:92] training  840 of 1500: completed tf_agent.train(...) =  114.273 [loss]\n",
      "I0707 23:46:23.719701 15896 agents.py:92] current policy       : avg_reward=0.135, avg_steps=70.500\n",
      "I0707 23:46:24.318697 15896 agents.py:92] training  841 of 1500: completed tf_agent.train(...) =  748.819 [loss]\n",
      "I0707 23:46:24.807695 15896 agents.py:92] training  842 of 1500: completed tf_agent.train(...) =  403.250 [loss]\n",
      "I0707 23:46:25.234693 15896 agents.py:92] training  843 of 1500: completed tf_agent.train(...) =  100.483 [loss]\n",
      "I0707 23:46:27.377690 15896 agents.py:92] training  844 of 1500: completed tf_agent.train(...) =  795.151 [loss]\n",
      "I0707 23:46:27.816694 15896 agents.py:92] training  845 of 1500: completed tf_agent.train(...) =  130.510 [loss]\n",
      "I0707 23:46:29.248730 15896 agents.py:92] current policy       : avg_reward=0.543, avg_steps=32.800\n",
      "I0707 23:46:29.724695 15896 agents.py:92] training  846 of 1500: completed tf_agent.train(...) =   87.342 [loss]\n",
      "I0707 23:46:30.261697 15896 agents.py:92] training  847 of 1500: completed tf_agent.train(...) =   36.294 [loss]\n",
      "I0707 23:46:30.693696 15896 agents.py:92] training  848 of 1500: completed tf_agent.train(...) =   18.985 [loss]\n",
      "I0707 23:46:31.172696 15896 agents.py:92] training  849 of 1500: completed tf_agent.train(...) =   13.867 [loss]\n",
      "I0707 23:46:31.567702 15896 agents.py:92] training  850 of 1500: completed tf_agent.train(...) =   12.344 [loss]\n",
      "I0707 23:46:32.272691 15896 agents.py:92] current policy       : avg_reward=0.711, avg_steps=15.200\n",
      "I0707 23:46:32.748695 15896 agents.py:92] training  851 of 1500: completed tf_agent.train(...) =   19.984 [loss]\n",
      "I0707 23:46:33.189696 15896 agents.py:92] training  852 of 1500: completed tf_agent.train(...) =   28.757 [loss]\n",
      "I0707 23:46:33.576697 15896 agents.py:92] training  853 of 1500: completed tf_agent.train(...) =    7.674 [loss]\n",
      "I0707 23:46:33.957693 15896 agents.py:92] training  854 of 1500: completed tf_agent.train(...) =    9.649 [loss]\n",
      "I0707 23:46:34.470693 15896 agents.py:92] training  855 of 1500: completed tf_agent.train(...) =  195.732 [loss]\n",
      "I0707 23:46:35.220731 15896 agents.py:92] current policy       : avg_reward=0.720, avg_steps=15.400\n",
      "I0707 23:46:35.702692 15896 agents.py:92] training  856 of 1500: completed tf_agent.train(...) =    4.766 [loss]\n",
      "I0707 23:46:36.166697 15896 agents.py:92] training  857 of 1500: completed tf_agent.train(...) =    7.366 [loss]\n",
      "I0707 23:46:36.540693 15896 agents.py:92] training  858 of 1500: completed tf_agent.train(...) =    3.206 [loss]\n",
      "I0707 23:46:36.880693 15896 agents.py:92] training  859 of 1500: completed tf_agent.train(...) =    4.752 [loss]\n",
      "I0707 23:46:37.225699 15896 agents.py:92] training  860 of 1500: completed tf_agent.train(...) =    3.046 [loss]\n",
      "I0707 23:46:37.919696 15896 agents.py:92] current policy       : avg_reward=0.713, avg_steps=14.300\n",
      "I0707 23:46:38.569693 15896 agents.py:92] training  861 of 1500: completed tf_agent.train(...) =  799.115 [loss]\n",
      "I0707 23:46:38.905694 15896 agents.py:92] training  862 of 1500: completed tf_agent.train(...) =   12.075 [loss]\n",
      "I0707 23:46:39.266693 15896 agents.py:92] training  863 of 1500: completed tf_agent.train(...) =   65.547 [loss]\n",
      "I0707 23:46:39.717690 15896 agents.py:92] training  864 of 1500: completed tf_agent.train(...) =   43.824 [loss]\n",
      "I0707 23:46:40.155695 15896 agents.py:92] training  865 of 1500: completed tf_agent.train(...) =   12.010 [loss]\n",
      "I0707 23:46:40.914726 15896 agents.py:92] current policy       : avg_reward=0.716, avg_steps=15.000\n",
      "I0707 23:46:41.334726 15896 agents.py:92] training  866 of 1500: completed tf_agent.train(...) =    8.614 [loss]\n",
      "I0707 23:46:41.703722 15896 agents.py:92] training  867 of 1500: completed tf_agent.train(...) =   19.104 [loss]\n",
      "I0707 23:46:42.116696 15896 agents.py:92] training  868 of 1500: completed tf_agent.train(...) =    7.162 [loss]\n",
      "I0707 23:46:42.521692 15896 agents.py:92] training  869 of 1500: completed tf_agent.train(...) =   11.091 [loss]\n",
      "I0707 23:46:43.023700 15896 agents.py:92] training  870 of 1500: completed tf_agent.train(...) =  202.115 [loss]\n",
      "I0707 23:46:45.258695 15896 agents.py:92] current policy       : avg_reward=-0.509, avg_steps=51.200\n",
      "I0707 23:46:45.718699 15896 agents.py:92] training  871 of 1500: completed tf_agent.train(...) =    3.132 [loss]\n",
      "I0707 23:46:46.165695 15896 agents.py:92] training  872 of 1500: completed tf_agent.train(...) =   11.589 [loss]\n",
      "I0707 23:46:46.593694 15896 agents.py:92] training  873 of 1500: completed tf_agent.train(...) =   60.966 [loss]\n",
      "I0707 23:46:46.959696 15896 agents.py:92] training  874 of 1500: completed tf_agent.train(...) =    3.587 [loss]\n",
      "I0707 23:46:47.249694 15896 agents.py:92] training  875 of 1500: completed tf_agent.train(...) =    1.687 [loss]\n",
      "I0707 23:46:48.687697 15896 agents.py:92] current policy       : avg_reward=0.065, avg_steps=32.100\n",
      "I0707 23:46:49.144693 15896 agents.py:92] training  876 of 1500: completed tf_agent.train(...) =    2.464 [loss]\n",
      "I0707 23:46:51.053692 15896 agents.py:92] training  877 of 1500: completed tf_agent.train(...) =   21.277 [loss]\n",
      "I0707 23:46:51.463696 15896 agents.py:92] training  878 of 1500: completed tf_agent.train(...) =    9.589 [loss]\n",
      "I0707 23:46:51.927694 15896 agents.py:92] training  879 of 1500: completed tf_agent.train(...) =   60.011 [loss]\n",
      "I0707 23:46:52.441693 15896 agents.py:92] training  880 of 1500: completed tf_agent.train(...) =    5.697 [loss]\n",
      "I0707 23:46:54.626708 15896 agents.py:92] current policy       : avg_reward=-0.482, avg_steps=51.400\n",
      "I0707 23:46:54.989692 15896 agents.py:92] training  881 of 1500: completed tf_agent.train(...) =    4.675 [loss]\n",
      "I0707 23:46:55.382695 15896 agents.py:92] training  882 of 1500: completed tf_agent.train(...) =   10.475 [loss]\n",
      "I0707 23:46:55.802695 15896 agents.py:92] training  883 of 1500: completed tf_agent.train(...) =    1.326 [loss]\n",
      "I0707 23:46:56.235692 15896 agents.py:92] training  884 of 1500: completed tf_agent.train(...) =    1.194 [loss]\n",
      "I0707 23:46:56.644697 15896 agents.py:92] training  885 of 1500: completed tf_agent.train(...) =    1.915 [loss]\n",
      "I0707 23:46:57.373727 15896 agents.py:92] current policy       : avg_reward=0.713, avg_steps=14.300\n",
      "I0707 23:46:58.016692 15896 agents.py:92] training  886 of 1500: completed tf_agent.train(...) =  969.139 [loss]\n",
      "I0707 23:46:58.466691 15896 agents.py:92] training  887 of 1500: completed tf_agent.train(...) =  139.094 [loss]\n",
      "I0707 23:46:58.905695 15896 agents.py:92] training  888 of 1500: completed tf_agent.train(...) =   62.525 [loss]\n",
      "I0707 23:46:59.360695 15896 agents.py:92] training  889 of 1500: completed tf_agent.train(...) =   76.094 [loss]\n",
      "I0707 23:46:59.780695 15896 agents.py:92] training  890 of 1500: completed tf_agent.train(...) =   38.374 [loss]\n",
      "I0707 23:47:00.506721 15896 agents.py:92] current policy       : avg_reward=0.705, avg_steps=14.900\n",
      "I0707 23:47:00.930697 15896 agents.py:92] training  891 of 1500: completed tf_agent.train(...) =  127.197 [loss]\n",
      "I0707 23:47:01.433697 15896 agents.py:92] training  892 of 1500: completed tf_agent.train(...) =    7.449 [loss]\n",
      "I0707 23:47:01.896694 15896 agents.py:92] training  893 of 1500: completed tf_agent.train(...) =   21.602 [loss]\n",
      "I0707 23:47:02.361698 15896 agents.py:92] training  894 of 1500: completed tf_agent.train(...) =   10.977 [loss]\n",
      "I0707 23:47:02.845692 15896 agents.py:92] training  895 of 1500: completed tf_agent.train(...) =   16.790 [loss]\n",
      "I0707 23:47:04.337721 15896 agents.py:92] current policy       : avg_reward=0.527, avg_steps=32.600\n",
      "I0707 23:47:04.781694 15896 agents.py:92] training  896 of 1500: completed tf_agent.train(...) =    1.515 [loss]\n",
      "I0707 23:47:05.224693 15896 agents.py:92] training  897 of 1500: completed tf_agent.train(...) =   26.430 [loss]\n",
      "I0707 23:47:07.228695 15896 agents.py:92] training  898 of 1500: completed tf_agent.train(...) =  393.541 [loss]\n",
      "I0707 23:47:07.579692 15896 agents.py:92] training  899 of 1500: completed tf_agent.train(...) =    3.426 [loss]\n",
      "I0707 23:47:08.084696 15896 agents.py:92] training  900 of 1500: completed tf_agent.train(...) =  234.298 [loss]\n",
      "I0707 23:47:09.561696 15896 agents.py:92] current policy       : avg_reward=0.584, avg_steps=31.900\n",
      "I0707 23:47:09.915696 15896 agents.py:92] training  901 of 1500: completed tf_agent.train(...) =    2.511 [loss]\n",
      "I0707 23:47:10.365694 15896 agents.py:92] training  902 of 1500: completed tf_agent.train(...) =    6.020 [loss]\n",
      "I0707 23:47:10.800695 15896 agents.py:92] training  903 of 1500: completed tf_agent.train(...) =    2.237 [loss]\n",
      "I0707 23:47:11.267693 15896 agents.py:92] training  904 of 1500: completed tf_agent.train(...) =    3.941 [loss]\n",
      "I0707 23:47:11.667694 15896 agents.py:92] training  905 of 1500: completed tf_agent.train(...) =    2.881 [loss]\n",
      "I0707 23:47:12.325712 15896 agents.py:92] current policy       : avg_reward=0.722, avg_steps=14.900\n",
      "I0707 23:47:12.764691 15896 agents.py:92] training  906 of 1500: completed tf_agent.train(...) =    2.050 [loss]\n",
      "I0707 23:47:13.122691 15896 agents.py:92] training  907 of 1500: completed tf_agent.train(...) =    1.936 [loss]\n",
      "I0707 23:47:13.491740 15896 agents.py:92] training  908 of 1500: completed tf_agent.train(...) =    2.046 [loss]\n",
      "I0707 23:47:13.951696 15896 agents.py:92] training  909 of 1500: completed tf_agent.train(...) =    2.014 [loss]\n",
      "I0707 23:47:14.371695 15896 agents.py:92] training  910 of 1500: completed tf_agent.train(...) =   14.246 [loss]\n",
      "I0707 23:47:15.851720 15896 agents.py:92] current policy       : avg_reward=0.070, avg_steps=33.000\n",
      "I0707 23:47:16.273695 15896 agents.py:92] training  911 of 1500: completed tf_agent.train(...) =    4.738 [loss]\n",
      "I0707 23:47:16.720695 15896 agents.py:92] training  912 of 1500: completed tf_agent.train(...) =    2.948 [loss]\n",
      "I0707 23:47:17.165697 15896 agents.py:92] training  913 of 1500: completed tf_agent.train(...) =   31.618 [loss]\n",
      "I0707 23:47:17.661697 15896 agents.py:92] training  914 of 1500: completed tf_agent.train(...) =   55.092 [loss]\n",
      "I0707 23:47:18.089691 15896 agents.py:92] training  915 of 1500: completed tf_agent.train(...) =    1.678 [loss]\n",
      "I0707 23:47:18.838721 15896 agents.py:92] current policy       : avg_reward=0.715, avg_steps=15.000\n",
      "I0707 23:47:19.325695 15896 agents.py:92] training  916 of 1500: completed tf_agent.train(...) =    1.765 [loss]\n",
      "I0707 23:47:19.737694 15896 agents.py:92] training  917 of 1500: completed tf_agent.train(...) =    2.157 [loss]\n",
      "I0707 23:47:20.215694 15896 agents.py:92] training  918 of 1500: completed tf_agent.train(...) =    2.237 [loss]\n",
      "I0707 23:47:20.571704 15896 agents.py:92] training  919 of 1500: completed tf_agent.train(...) =    1.310 [loss]\n",
      "I0707 23:47:20.934695 15896 agents.py:92] training  920 of 1500: completed tf_agent.train(...) =    1.631 [loss]\n",
      "I0707 23:47:22.418721 15896 agents.py:92] current policy       : avg_reward=0.509, avg_steps=33.800\n",
      "I0707 23:47:22.777690 15896 agents.py:92] training  921 of 1500: completed tf_agent.train(...) =    1.708 [loss]\n",
      "I0707 23:47:23.161701 15896 agents.py:92] training  922 of 1500: completed tf_agent.train(...) =    2.540 [loss]\n",
      "I0707 23:47:23.620694 15896 agents.py:92] training  923 of 1500: completed tf_agent.train(...) =    1.515 [loss]\n",
      "I0707 23:47:24.033694 15896 agents.py:92] training  924 of 1500: completed tf_agent.train(...) =    8.342 [loss]\n",
      "I0707 23:47:24.515693 15896 agents.py:92] training  925 of 1500: completed tf_agent.train(...) =   13.717 [loss]\n",
      "I0707 23:47:25.278726 15896 agents.py:92] current policy       : avg_reward=0.718, avg_steps=15.000\n",
      "I0707 23:47:25.752693 15896 agents.py:92] training  926 of 1500: completed tf_agent.train(...) =    1.156 [loss]\n",
      "I0707 23:47:26.228693 15896 agents.py:92] training  927 of 1500: completed tf_agent.train(...) =    6.710 [loss]\n",
      "I0707 23:47:26.633694 15896 agents.py:92] training  928 of 1500: completed tf_agent.train(...) =    1.368 [loss]\n",
      "I0707 23:47:27.044695 15896 agents.py:92] training  929 of 1500: completed tf_agent.train(...) =    2.137 [loss]\n",
      "I0707 23:47:27.480694 15896 agents.py:92] training  930 of 1500: completed tf_agent.train(...) =    1.390 [loss]\n",
      "I0707 23:47:28.207695 15896 agents.py:92] current policy       : avg_reward=0.699, avg_steps=15.400\n",
      "I0707 23:47:28.637695 15896 agents.py:92] training  931 of 1500: completed tf_agent.train(...) =    2.168 [loss]\n",
      "I0707 23:47:29.079694 15896 agents.py:92] training  932 of 1500: completed tf_agent.train(...) =    1.476 [loss]\n",
      "I0707 23:47:29.541693 15896 agents.py:92] training  933 of 1500: completed tf_agent.train(...) =    1.686 [loss]\n",
      "I0707 23:47:29.975694 15896 agents.py:92] training  934 of 1500: completed tf_agent.train(...) =    1.248 [loss]\n",
      "I0707 23:47:30.417693 15896 agents.py:92] training  935 of 1500: completed tf_agent.train(...) =    2.499 [loss]\n",
      "I0707 23:47:31.137691 15896 agents.py:92] current policy       : avg_reward=0.734, avg_steps=14.700\n",
      "I0707 23:47:31.619693 15896 agents.py:92] training  936 of 1500: completed tf_agent.train(...) =    1.120 [loss]\n",
      "I0707 23:47:32.049693 15896 agents.py:92] training  937 of 1500: completed tf_agent.train(...) =    0.996 [loss]\n",
      "I0707 23:47:32.527693 15896 agents.py:92] training  938 of 1500: completed tf_agent.train(...) =    0.894 [loss]\n",
      "I0707 23:47:32.952698 15896 agents.py:92] training  939 of 1500: completed tf_agent.train(...) =    2.707 [loss]\n",
      "I0707 23:47:33.398693 15896 agents.py:92] training  940 of 1500: completed tf_agent.train(...) =    1.130 [loss]\n",
      "I0707 23:47:34.120691 15896 agents.py:92] current policy       : avg_reward=0.703, avg_steps=14.500\n",
      "I0707 23:47:34.498693 15896 agents.py:92] training  941 of 1500: completed tf_agent.train(...) =    0.984 [loss]\n",
      "I0707 23:47:34.856693 15896 agents.py:92] training  942 of 1500: completed tf_agent.train(...) =    1.130 [loss]\n",
      "I0707 23:47:35.296692 15896 agents.py:92] training  943 of 1500: completed tf_agent.train(...) =    1.257 [loss]\n",
      "I0707 23:47:35.724696 15896 agents.py:92] training  944 of 1500: completed tf_agent.train(...) =    0.786 [loss]\n",
      "I0707 23:47:36.205696 15896 agents.py:92] training  945 of 1500: completed tf_agent.train(...) =    5.506 [loss]\n",
      "I0707 23:47:36.985694 15896 agents.py:92] current policy       : avg_reward=0.714, avg_steps=15.400\n",
      "I0707 23:47:37.389693 15896 agents.py:92] training  946 of 1500: completed tf_agent.train(...) =    0.962 [loss]\n",
      "I0707 23:47:37.802695 15896 agents.py:92] training  947 of 1500: completed tf_agent.train(...) =    1.148 [loss]\n",
      "I0707 23:47:39.699728 15896 agents.py:92] training  948 of 1500: completed tf_agent.train(...) =   43.794 [loss]\n",
      "I0707 23:47:40.110696 15896 agents.py:92] training  949 of 1500: completed tf_agent.train(...) =    1.200 [loss]\n",
      "I0707 23:47:40.589696 15896 agents.py:92] training  950 of 1500: completed tf_agent.train(...) =    0.924 [loss]\n",
      "I0707 23:47:41.233692 15896 agents.py:92] current policy       : avg_reward=0.725, avg_steps=14.200\n",
      "I0707 23:47:41.586697 15896 agents.py:92] training  951 of 1500: completed tf_agent.train(...) =    1.181 [loss]\n",
      "I0707 23:47:42.041692 15896 agents.py:92] training  952 of 1500: completed tf_agent.train(...) =    1.212 [loss]\n",
      "I0707 23:47:42.389694 15896 agents.py:92] training  953 of 1500: completed tf_agent.train(...) =    1.654 [loss]\n",
      "I0707 23:47:42.842692 15896 agents.py:92] training  954 of 1500: completed tf_agent.train(...) =    0.769 [loss]\n",
      "I0707 23:47:43.271692 15896 agents.py:92] training  955 of 1500: completed tf_agent.train(...) =    5.715 [loss]\n",
      "I0707 23:47:44.809695 15896 agents.py:92] current policy       : avg_reward=0.045, avg_steps=34.100\n",
      "I0707 23:47:45.259957 15896 agents.py:92] training  956 of 1500: completed tf_agent.train(...) =    1.130 [loss]\n",
      "I0707 23:47:45.639958 15896 agents.py:92] training  957 of 1500: completed tf_agent.train(...) =    1.546 [loss]\n",
      "I0707 23:47:45.969959 15896 agents.py:92] training  958 of 1500: completed tf_agent.train(...) =    1.348 [loss]\n",
      "I0707 23:47:46.431958 15896 agents.py:92] training  959 of 1500: completed tf_agent.train(...) =    2.907 [loss]\n",
      "I0707 23:47:46.886957 15896 agents.py:92] training  960 of 1500: completed tf_agent.train(...) =    4.371 [loss]\n",
      "I0707 23:47:48.350989 15896 agents.py:92] current policy       : avg_reward=0.381, avg_steps=32.700\n",
      "I0707 23:47:48.821956 15896 agents.py:92] training  961 of 1500: completed tf_agent.train(...) =    0.765 [loss]\n",
      "I0707 23:47:49.221957 15896 agents.py:92] training  962 of 1500: completed tf_agent.train(...) =    1.077 [loss]\n",
      "I0707 23:47:49.677955 15896 agents.py:92] training  963 of 1500: completed tf_agent.train(...) =    1.266 [loss]\n",
      "I0707 23:47:50.109955 15896 agents.py:92] training  964 of 1500: completed tf_agent.train(...) =    0.839 [loss]\n",
      "I0707 23:47:50.562958 15896 agents.py:92] training  965 of 1500: completed tf_agent.train(...) =    0.647 [loss]\n",
      "I0707 23:47:52.070983 15896 agents.py:92] current policy       : avg_reward=0.376, avg_steps=34.500\n",
      "I0707 23:47:52.540956 15896 agents.py:92] training  966 of 1500: completed tf_agent.train(...) =    1.272 [loss]\n",
      "I0707 23:47:52.907952 15896 agents.py:92] training  967 of 1500: completed tf_agent.train(...) =    1.156 [loss]\n",
      "I0707 23:47:53.260989 15896 agents.py:92] training  968 of 1500: completed tf_agent.train(...) =    1.678 [loss]\n",
      "I0707 23:47:53.712956 15896 agents.py:92] training  969 of 1500: completed tf_agent.train(...) =    0.799 [loss]\n",
      "I0707 23:47:54.159956 15896 agents.py:92] training  970 of 1500: completed tf_agent.train(...) =    1.935 [loss]\n",
      "I0707 23:47:55.606995 15896 agents.py:92] current policy       : avg_reward=-0.285, avg_steps=32.500\n",
      "I0707 23:47:56.046959 15896 agents.py:92] training  971 of 1500: completed tf_agent.train(...) =    0.805 [loss]\n",
      "I0707 23:47:56.485956 15896 agents.py:92] training  972 of 1500: completed tf_agent.train(...) =    3.318 [loss]\n",
      "I0707 23:47:56.900958 15896 agents.py:92] training  973 of 1500: completed tf_agent.train(...) =    0.912 [loss]\n",
      "I0707 23:47:57.277957 15896 agents.py:92] training  974 of 1500: completed tf_agent.train(...) =    1.351 [loss]\n",
      "I0707 23:47:57.612953 15896 agents.py:92] training  975 of 1500: completed tf_agent.train(...) =    1.590 [loss]\n",
      "I0707 23:47:58.377988 15896 agents.py:92] current policy       : avg_reward=0.704, avg_steps=15.200\n",
      "I0707 23:47:58.864955 15896 agents.py:92] training  976 of 1500: completed tf_agent.train(...) =    1.169 [loss]\n",
      "I0707 23:47:59.274955 15896 agents.py:92] training  977 of 1500: completed tf_agent.train(...) =    1.114 [loss]\n",
      "I0707 23:47:59.726956 15896 agents.py:92] training  978 of 1500: completed tf_agent.train(...) =    0.986 [loss]\n",
      "I0707 23:48:00.141957 15896 agents.py:92] training  979 of 1500: completed tf_agent.train(...) =    0.752 [loss]\n",
      "I0707 23:48:01.946954 15896 agents.py:92] training  980 of 1500: completed tf_agent.train(...) =    3.477 [loss]\n",
      "I0707 23:48:02.668991 15896 agents.py:92] current policy       : avg_reward=0.710, avg_steps=14.400\n",
      "I0707 23:48:03.103956 15896 agents.py:92] training  981 of 1500: completed tf_agent.train(...) =    0.954 [loss]\n",
      "I0707 23:48:03.457957 15896 agents.py:92] training  982 of 1500: completed tf_agent.train(...) =    0.763 [loss]\n",
      "I0707 23:48:03.792990 15896 agents.py:92] training  983 of 1500: completed tf_agent.train(...) =    1.088 [loss]\n",
      "I0707 23:48:04.132955 15896 agents.py:92] training  984 of 1500: completed tf_agent.train(...) =    1.451 [loss]\n",
      "I0707 23:48:04.597953 15896 agents.py:92] training  985 of 1500: completed tf_agent.train(...) =    2.218 [loss]\n",
      "I0707 23:48:05.195956 15896 agents.py:92] current policy       : avg_reward=0.732, avg_steps=14.000\n",
      "I0707 23:48:05.590953 15896 agents.py:92] training  986 of 1500: completed tf_agent.train(...) =    0.880 [loss]\n",
      "I0707 23:48:06.044957 15896 agents.py:92] training  987 of 1500: completed tf_agent.train(...) =    0.852 [loss]\n",
      "I0707 23:48:06.368956 15896 agents.py:92] training  988 of 1500: completed tf_agent.train(...) =    0.676 [loss]\n",
      "I0707 23:48:06.847964 15896 agents.py:92] training  989 of 1500: completed tf_agent.train(...) =    1.833 [loss]\n",
      "I0707 23:48:07.304956 15896 agents.py:92] training  990 of 1500: completed tf_agent.train(...) =    1.221 [loss]\n",
      "I0707 23:48:08.803955 15896 agents.py:92] current policy       : avg_reward=0.540, avg_steps=33.900\n",
      "I0707 23:48:09.263961 15896 agents.py:92] training  991 of 1500: completed tf_agent.train(...) =    0.763 [loss]\n",
      "I0707 23:48:09.728955 15896 agents.py:92] training  992 of 1500: completed tf_agent.train(...) =    0.604 [loss]\n",
      "I0707 23:48:10.167957 15896 agents.py:92] training  993 of 1500: completed tf_agent.train(...) =    0.715 [loss]\n",
      "I0707 23:48:10.592957 15896 agents.py:92] training  994 of 1500: completed tf_agent.train(...) =    0.616 [loss]\n",
      "I0707 23:48:11.014957 15896 agents.py:92] training  995 of 1500: completed tf_agent.train(...) =    1.189 [loss]\n",
      "I0707 23:48:12.489961 15896 agents.py:92] current policy       : avg_reward=0.531, avg_steps=33.500\n",
      "I0707 23:48:12.894957 15896 agents.py:92] training  996 of 1500: completed tf_agent.train(...) =    0.850 [loss]\n",
      "I0707 23:48:13.273958 15896 agents.py:92] training  997 of 1500: completed tf_agent.train(...) =    0.704 [loss]\n",
      "I0707 23:48:13.704955 15896 agents.py:92] training  998 of 1500: completed tf_agent.train(...) =    0.729 [loss]\n",
      "I0707 23:48:14.173953 15896 agents.py:92] training  999 of 1500: completed tf_agent.train(...) =    1.060 [loss]\n",
      "I0707 23:48:14.547958 15896 agents.py:92] training 1000 of 1500: completed tf_agent.train(...) =    1.942 [loss]\n",
      "I0707 23:48:15.338961 15896 agents.py:92] current policy       : avg_reward=0.708, avg_steps=15.500\n",
      "I0707 23:48:17.338958 15896 agents.py:92] training 1001 of 1500: completed tf_agent.train(...) =   61.463 [loss]\n",
      "I0707 23:48:17.798955 15896 agents.py:92] training 1002 of 1500: completed tf_agent.train(...) =   17.055 [loss]\n",
      "I0707 23:48:18.157953 15896 agents.py:92] training 1003 of 1500: completed tf_agent.train(...) =    4.209 [loss]\n",
      "I0707 23:48:18.528958 15896 agents.py:92] training 1004 of 1500: completed tf_agent.train(...) =    1.645 [loss]\n",
      "I0707 23:48:20.366960 15896 agents.py:92] training 1005 of 1500: completed tf_agent.train(...) =    1.374 [loss]\n",
      "I0707 23:48:22.551059 15896 agents.py:92] current policy       : avg_reward=0.363, avg_steps=51.800\n",
      "I0707 23:48:23.008058 15896 agents.py:92] training 1006 of 1500: completed tf_agent.train(...) =    0.946 [loss]\n",
      "I0707 23:48:23.468060 15896 agents.py:92] training 1007 of 1500: completed tf_agent.train(...) =    4.312 [loss]\n",
      "I0707 23:48:23.872058 15896 agents.py:92] training 1008 of 1500: completed tf_agent.train(...) =    1.723 [loss]\n",
      "I0707 23:48:24.234057 15896 agents.py:92] training 1009 of 1500: completed tf_agent.train(...) =    1.078 [loss]\n",
      "I0707 23:48:24.743059 15896 agents.py:92] training 1010 of 1500: completed tf_agent.train(...) =    3.073 [loss]\n",
      "I0707 23:48:25.484059 15896 agents.py:92] current policy       : avg_reward=0.716, avg_steps=14.400\n",
      "I0707 23:48:25.861114 15896 agents.py:92] training 1011 of 1500: completed tf_agent.train(...) =    0.796 [loss]\n",
      "I0707 23:48:26.228058 15896 agents.py:92] training 1012 of 1500: completed tf_agent.train(...) =    1.810 [loss]\n",
      "I0707 23:48:26.667064 15896 agents.py:92] training 1013 of 1500: completed tf_agent.train(...) =    5.137 [loss]\n",
      "I0707 23:48:27.162063 15896 agents.py:92] training 1014 of 1500: completed tf_agent.train(...) =    1.347 [loss]\n",
      "I0707 23:48:27.616060 15896 agents.py:92] training 1015 of 1500: completed tf_agent.train(...) =    2.693 [loss]\n",
      "I0707 23:48:28.325059 15896 agents.py:92] current policy       : avg_reward=0.727, avg_steps=13.900\n",
      "I0707 23:48:28.689058 15896 agents.py:92] training 1016 of 1500: completed tf_agent.train(...) =    0.890 [loss]\n",
      "I0707 23:48:29.073059 15896 agents.py:92] training 1017 of 1500: completed tf_agent.train(...) =    0.964 [loss]\n",
      "I0707 23:48:29.533060 15896 agents.py:92] training 1018 of 1500: completed tf_agent.train(...) =    2.216 [loss]\n",
      "I0707 23:48:30.017063 15896 agents.py:92] training 1019 of 1500: completed tf_agent.train(...) =   17.290 [loss]\n",
      "I0707 23:48:30.494058 15896 agents.py:92] training 1020 of 1500: completed tf_agent.train(...) =    1.095 [loss]\n",
      "I0707 23:48:31.927060 15896 agents.py:92] current policy       : avg_reward=0.536, avg_steps=32.600\n",
      "I0707 23:48:32.343064 15896 agents.py:92] training 1021 of 1500: completed tf_agent.train(...) =    3.064 [loss]\n",
      "I0707 23:48:32.831062 15896 agents.py:92] training 1022 of 1500: completed tf_agent.train(...) =    2.143 [loss]\n",
      "I0707 23:48:33.275062 15896 agents.py:92] training 1023 of 1500: completed tf_agent.train(...) =    0.811 [loss]\n",
      "I0707 23:48:33.638102 15896 agents.py:92] training 1024 of 1500: completed tf_agent.train(...) =    1.337 [loss]\n",
      "I0707 23:48:34.034059 15896 agents.py:92] training 1025 of 1500: completed tf_agent.train(...) =    0.638 [loss]\n",
      "I0707 23:48:35.509058 15896 agents.py:92] current policy       : avg_reward=0.532, avg_steps=33.500\n",
      "I0707 23:48:35.978062 15896 agents.py:92] training 1026 of 1500: completed tf_agent.train(...) =    0.801 [loss]\n",
      "I0707 23:48:36.429062 15896 agents.py:92] training 1027 of 1500: completed tf_agent.train(...) =    1.226 [loss]\n",
      "I0707 23:48:36.823063 15896 agents.py:92] training 1028 of 1500: completed tf_agent.train(...) =    2.020 [loss]\n",
      "I0707 23:48:37.165062 15896 agents.py:92] training 1029 of 1500: completed tf_agent.train(...) =    0.727 [loss]\n",
      "I0707 23:48:37.589063 15896 agents.py:92] training 1030 of 1500: completed tf_agent.train(...) =    0.808 [loss]\n",
      "I0707 23:48:39.123059 15896 agents.py:92] current policy       : avg_reward=0.513, avg_steps=34.000\n",
      "I0707 23:48:39.572062 15896 agents.py:92] training 1031 of 1500: completed tf_agent.train(...) =    3.717 [loss]\n",
      "I0707 23:48:40.036062 15896 agents.py:92] training 1032 of 1500: completed tf_agent.train(...) =    0.701 [loss]\n",
      "I0707 23:48:40.410062 15896 agents.py:92] training 1033 of 1500: completed tf_agent.train(...) =    1.888 [loss]\n",
      "I0707 23:48:40.793061 15896 agents.py:92] training 1034 of 1500: completed tf_agent.train(...) =    0.614 [loss]\n",
      "I0707 23:48:41.254060 15896 agents.py:92] training 1035 of 1500: completed tf_agent.train(...) =    0.616 [loss]\n",
      "I0707 23:48:42.074059 15896 agents.py:92] current policy       : avg_reward=0.689, avg_steps=16.800\n",
      "I0707 23:48:42.580061 15896 agents.py:92] training 1036 of 1500: completed tf_agent.train(...) =    1.484 [loss]\n",
      "I0707 23:48:43.010062 15896 agents.py:92] training 1037 of 1500: completed tf_agent.train(...) =    0.633 [loss]\n",
      "I0707 23:48:43.479058 15896 agents.py:92] training 1038 of 1500: completed tf_agent.train(...) =    0.866 [loss]\n",
      "I0707 23:48:43.978059 15896 agents.py:92] training 1039 of 1500: completed tf_agent.train(...) =    0.621 [loss]\n",
      "I0707 23:48:44.435064 15896 agents.py:92] training 1040 of 1500: completed tf_agent.train(...) =    0.790 [loss]\n",
      "I0707 23:48:45.902059 15896 agents.py:92] current policy       : avg_reward=0.374, avg_steps=33.100\n",
      "I0707 23:48:46.386061 15896 agents.py:92] training 1041 of 1500: completed tf_agent.train(...) =    0.595 [loss]\n",
      "I0707 23:48:46.830060 15896 agents.py:92] training 1042 of 1500: completed tf_agent.train(...) =    0.681 [loss]\n",
      "I0707 23:48:47.240062 15896 agents.py:92] training 1043 of 1500: completed tf_agent.train(...) =    0.807 [loss]\n",
      "I0707 23:48:47.617058 15896 agents.py:92] training 1044 of 1500: completed tf_agent.train(...) =    1.138 [loss]\n",
      "I0707 23:48:47.973064 15896 agents.py:92] training 1045 of 1500: completed tf_agent.train(...) =    0.567 [loss]\n",
      "I0707 23:48:49.483059 15896 agents.py:92] current policy       : avg_reward=0.402, avg_steps=32.600\n",
      "I0707 23:48:49.943062 15896 agents.py:92] training 1046 of 1500: completed tf_agent.train(...) =    0.594 [loss]\n",
      "I0707 23:48:50.398063 15896 agents.py:92] training 1047 of 1500: completed tf_agent.train(...) =    2.127 [loss]\n",
      "I0707 23:48:50.832066 15896 agents.py:92] training 1048 of 1500: completed tf_agent.train(...) =    0.580 [loss]\n",
      "I0707 23:48:51.269060 15896 agents.py:92] training 1049 of 1500: completed tf_agent.train(...) =    0.779 [loss]\n",
      "I0707 23:48:51.724060 15896 agents.py:92] training 1050 of 1500: completed tf_agent.train(...) =    0.584 [loss]\n",
      "I0707 23:48:52.395058 15896 agents.py:92] current policy       : avg_reward=0.712, avg_steps=15.200\n",
      "I0707 23:48:52.894061 15896 agents.py:92] training 1051 of 1500: completed tf_agent.train(...) =    0.866 [loss]\n",
      "I0707 23:48:53.356062 15896 agents.py:92] training 1052 of 1500: completed tf_agent.train(...) =    7.346 [loss]\n",
      "I0707 23:48:53.726058 15896 agents.py:92] training 1053 of 1500: completed tf_agent.train(...) =    3.042 [loss]\n",
      "I0707 23:48:54.110059 15896 agents.py:92] training 1054 of 1500: completed tf_agent.train(...) =    0.672 [loss]\n",
      "I0707 23:48:54.597059 15896 agents.py:92] training 1055 of 1500: completed tf_agent.train(...) =    0.669 [loss]\n",
      "I0707 23:48:55.312057 15896 agents.py:92] current policy       : avg_reward=0.735, avg_steps=14.400\n",
      "I0707 23:48:55.668058 15896 agents.py:92] training 1056 of 1500: completed tf_agent.train(...) =    0.855 [loss]\n",
      "I0707 23:48:56.037061 15896 agents.py:92] training 1057 of 1500: completed tf_agent.train(...) =    1.282 [loss]\n",
      "I0707 23:48:56.496065 15896 agents.py:92] training 1058 of 1500: completed tf_agent.train(...) =    1.910 [loss]\n",
      "I0707 23:48:56.944062 15896 agents.py:92] training 1059 of 1500: completed tf_agent.train(...) =    0.820 [loss]\n",
      "I0707 23:48:57.365060 15896 agents.py:92] training 1060 of 1500: completed tf_agent.train(...) =    0.581 [loss]\n",
      "I0707 23:48:58.825058 15896 agents.py:92] current policy       : avg_reward=0.525, avg_steps=32.300\n",
      "I0707 23:48:59.320059 15896 agents.py:92] training 1061 of 1500: completed tf_agent.train(...) =    0.981 [loss]\n",
      "I0707 23:48:59.790064 15896 agents.py:92] training 1062 of 1500: completed tf_agent.train(...) =    1.085 [loss]\n",
      "I0707 23:49:01.588060 15896 agents.py:92] training 1063 of 1500: completed tf_agent.train(...) =    0.823 [loss]\n",
      "I0707 23:49:02.032059 15896 agents.py:92] training 1064 of 1500: completed tf_agent.train(...) =    0.532 [loss]\n",
      "I0707 23:49:02.421061 15896 agents.py:92] training 1065 of 1500: completed tf_agent.train(...) =    0.684 [loss]\n",
      "I0707 23:49:03.951058 15896 agents.py:92] current policy       : avg_reward=0.528, avg_steps=32.800\n",
      "I0707 23:49:04.375096 15896 agents.py:92] training 1066 of 1500: completed tf_agent.train(...) =    0.573 [loss]\n",
      "I0707 23:49:04.865061 15896 agents.py:92] training 1067 of 1500: completed tf_agent.train(...) =    0.628 [loss]\n",
      "I0707 23:49:05.247062 15896 agents.py:92] training 1068 of 1500: completed tf_agent.train(...) =    1.190 [loss]\n",
      "I0707 23:49:05.712059 15896 agents.py:92] training 1069 of 1500: completed tf_agent.train(...) =    0.697 [loss]\n",
      "I0707 23:49:06.160060 15896 agents.py:92] training 1070 of 1500: completed tf_agent.train(...) =    0.890 [loss]\n",
      "I0707 23:49:07.620059 15896 agents.py:92] current policy       : avg_reward=0.520, avg_steps=32.900\n",
      "I0707 23:49:08.026059 15896 agents.py:92] training 1071 of 1500: completed tf_agent.train(...) =    1.399 [loss]\n",
      "I0707 23:49:08.382098 15896 agents.py:92] training 1072 of 1500: completed tf_agent.train(...) =    0.560 [loss]\n",
      "I0707 23:49:08.862067 15896 agents.py:92] training 1073 of 1500: completed tf_agent.train(...) =    0.512 [loss]\n",
      "I0707 23:49:09.265061 15896 agents.py:92] training 1074 of 1500: completed tf_agent.train(...) =    0.593 [loss]\n",
      "I0707 23:49:09.715059 15896 agents.py:92] training 1075 of 1500: completed tf_agent.train(...) =    0.579 [loss]\n",
      "I0707 23:49:10.438060 15896 agents.py:92] current policy       : avg_reward=0.692, avg_steps=16.000\n",
      "I0707 23:49:10.920065 15896 agents.py:92] training 1076 of 1500: completed tf_agent.train(...) =    1.423 [loss]\n",
      "I0707 23:49:11.392059 15896 agents.py:92] training 1077 of 1500: completed tf_agent.train(...) =    0.719 [loss]\n",
      "I0707 23:49:11.841059 15896 agents.py:92] training 1078 of 1500: completed tf_agent.train(...) =    0.808 [loss]\n",
      "I0707 23:49:12.230061 15896 agents.py:92] training 1079 of 1500: completed tf_agent.train(...) =    0.798 [loss]\n",
      "I0707 23:49:12.594079 15896 agents.py:92] training 1080 of 1500: completed tf_agent.train(...) =    0.613 [loss]\n",
      "I0707 23:49:14.848062 15896 agents.py:92] current policy       : avg_reward=0.012, avg_steps=52.700\n",
      "I0707 23:49:15.306062 15896 agents.py:92] training 1081 of 1500: completed tf_agent.train(...) =    0.607 [loss]\n",
      "I0707 23:49:15.741065 15896 agents.py:92] training 1082 of 1500: completed tf_agent.train(...) =    2.713 [loss]\n",
      "I0707 23:49:16.254060 15896 agents.py:92] training 1083 of 1500: completed tf_agent.train(...) =   13.857 [loss]\n",
      "I0707 23:49:16.680059 15896 agents.py:92] training 1084 of 1500: completed tf_agent.train(...) =    1.127 [loss]\n",
      "I0707 23:49:17.183059 15896 agents.py:92] training 1085 of 1500: completed tf_agent.train(...) =    1.239 [loss]\n",
      "I0707 23:49:17.873089 15896 agents.py:92] current policy       : avg_reward=0.737, avg_steps=13.600\n",
      "I0707 23:49:18.319062 15896 agents.py:92] training 1086 of 1500: completed tf_agent.train(...) =    0.890 [loss]\n",
      "I0707 23:49:18.784065 15896 agents.py:92] training 1087 of 1500: completed tf_agent.train(...) =    0.617 [loss]\n",
      "I0707 23:49:19.211059 15896 agents.py:92] training 1088 of 1500: completed tf_agent.train(...) =    0.741 [loss]\n",
      "I0707 23:49:19.667066 15896 agents.py:92] training 1089 of 1500: completed tf_agent.train(...) =    0.806 [loss]\n",
      "I0707 23:49:20.111062 15896 agents.py:92] training 1090 of 1500: completed tf_agent.train(...) =    1.047 [loss]\n",
      "I0707 23:49:20.764058 15896 agents.py:92] current policy       : avg_reward=0.708, avg_steps=15.100\n",
      "I0707 23:49:21.136062 15896 agents.py:92] training 1091 of 1500: completed tf_agent.train(...) =    0.881 [loss]\n",
      "I0707 23:49:21.609058 15896 agents.py:92] training 1092 of 1500: completed tf_agent.train(...) =    1.221 [loss]\n",
      "I0707 23:49:22.057060 15896 agents.py:92] training 1093 of 1500: completed tf_agent.train(...) =    3.284 [loss]\n",
      "I0707 23:49:22.503062 15896 agents.py:92] training 1094 of 1500: completed tf_agent.train(...) =    1.191 [loss]\n",
      "I0707 23:49:22.959064 15896 agents.py:92] training 1095 of 1500: completed tf_agent.train(...) =    0.834 [loss]\n",
      "I0707 23:49:23.699059 15896 agents.py:92] current policy       : avg_reward=0.706, avg_steps=14.900\n",
      "I0707 23:49:24.166060 15896 agents.py:92] training 1096 of 1500: completed tf_agent.train(...) =    0.653 [loss]\n",
      "I0707 23:49:24.602062 15896 agents.py:92] training 1097 of 1500: completed tf_agent.train(...) =    0.893 [loss]\n",
      "I0707 23:49:25.074059 15896 agents.py:92] training 1098 of 1500: completed tf_agent.train(...) =    0.657 [loss]\n",
      "I0707 23:49:25.547062 15896 agents.py:92] training 1099 of 1500: completed tf_agent.train(...) =    0.814 [loss]\n",
      "I0707 23:49:26.010061 15896 agents.py:92] training 1100 of 1500: completed tf_agent.train(...) =    0.804 [loss]\n",
      "I0707 23:49:26.776058 15896 agents.py:92] current policy       : avg_reward=0.712, avg_steps=15.500\n",
      "I0707 23:49:27.243064 15896 agents.py:92] training 1101 of 1500: completed tf_agent.train(...) =    0.625 [loss]\n",
      "I0707 23:49:27.736061 15896 agents.py:92] training 1102 of 1500: completed tf_agent.train(...) =    1.265 [loss]\n",
      "I0707 23:49:28.190061 15896 agents.py:92] training 1103 of 1500: completed tf_agent.train(...) =    0.465 [loss]\n",
      "I0707 23:49:28.666066 15896 agents.py:92] training 1104 of 1500: completed tf_agent.train(...) =    3.261 [loss]\n",
      "I0707 23:49:29.185061 15896 agents.py:92] training 1105 of 1500: completed tf_agent.train(...) =    1.084 [loss]\n",
      "I0707 23:49:29.962059 15896 agents.py:92] current policy       : avg_reward=0.694, avg_steps=15.800\n",
      "I0707 23:49:30.405065 15896 agents.py:92] training 1106 of 1500: completed tf_agent.train(...) =    0.598 [loss]\n",
      "I0707 23:49:30.851064 15896 agents.py:92] training 1107 of 1500: completed tf_agent.train(...) =    0.681 [loss]\n",
      "I0707 23:49:31.248061 15896 agents.py:92] training 1108 of 1500: completed tf_agent.train(...) =    0.883 [loss]\n",
      "I0707 23:49:31.714065 15896 agents.py:92] training 1109 of 1500: completed tf_agent.train(...) =    0.585 [loss]\n",
      "I0707 23:49:32.171060 15896 agents.py:92] training 1110 of 1500: completed tf_agent.train(...) =    2.485 [loss]\n",
      "I0707 23:49:33.617058 15896 agents.py:92] current policy       : avg_reward=0.085, avg_steps=32.800\n",
      "I0707 23:49:35.503059 15896 agents.py:92] training 1111 of 1500: completed tf_agent.train(...) =   70.775 [loss]\n",
      "I0707 23:49:35.857067 15896 agents.py:92] training 1112 of 1500: completed tf_agent.train(...) =    0.797 [loss]\n",
      "I0707 23:49:36.234060 15896 agents.py:92] training 1113 of 1500: completed tf_agent.train(...) =    1.572 [loss]\n",
      "I0707 23:49:36.692063 15896 agents.py:92] training 1114 of 1500: completed tf_agent.train(...) =    0.951 [loss]\n",
      "I0707 23:49:37.168059 15896 agents.py:92] training 1115 of 1500: completed tf_agent.train(...) =    1.276 [loss]\n",
      "I0707 23:49:37.905060 15896 agents.py:92] current policy       : avg_reward=0.703, avg_steps=15.000\n",
      "I0707 23:49:38.357067 15896 agents.py:92] training 1116 of 1500: completed tf_agent.train(...) =    0.845 [loss]\n",
      "I0707 23:49:38.716060 15896 agents.py:92] training 1117 of 1500: completed tf_agent.train(...) =    1.228 [loss]\n",
      "I0707 23:49:39.056063 15896 agents.py:92] training 1118 of 1500: completed tf_agent.train(...) =    0.756 [loss]\n",
      "I0707 23:49:39.437058 15896 agents.py:92] training 1119 of 1500: completed tf_agent.train(...) =    1.507 [loss]\n",
      "I0707 23:49:39.901062 15896 agents.py:92] training 1120 of 1500: completed tf_agent.train(...) =    0.688 [loss]\n",
      "I0707 23:49:41.407058 15896 agents.py:92] current policy       : avg_reward=0.526, avg_steps=33.700\n",
      "I0707 23:49:41.875355 15896 agents.py:92] training 1121 of 1500: completed tf_agent.train(...) =    0.672 [loss]\n",
      "I0707 23:49:42.323358 15896 agents.py:92] training 1122 of 1500: completed tf_agent.train(...) =    0.946 [loss]\n",
      "I0707 23:49:42.812350 15896 agents.py:92] training 1123 of 1500: completed tf_agent.train(...) =    1.476 [loss]\n",
      "I0707 23:49:43.247358 15896 agents.py:92] training 1124 of 1500: completed tf_agent.train(...) =    0.692 [loss]\n",
      "I0707 23:49:43.678352 15896 agents.py:92] training 1125 of 1500: completed tf_agent.train(...) =    0.460 [loss]\n",
      "I0707 23:49:44.430352 15896 agents.py:92] current policy       : avg_reward=0.713, avg_steps=14.800\n",
      "I0707 23:49:44.866354 15896 agents.py:92] training 1126 of 1500: completed tf_agent.train(...) =    0.529 [loss]\n",
      "I0707 23:49:45.309351 15896 agents.py:92] training 1127 of 1500: completed tf_agent.train(...) =    1.177 [loss]\n",
      "I0707 23:49:45.713355 15896 agents.py:92] training 1128 of 1500: completed tf_agent.train(...) =    0.588 [loss]\n",
      "I0707 23:49:46.143354 15896 agents.py:92] training 1129 of 1500: completed tf_agent.train(...) =    0.556 [loss]\n",
      "I0707 23:49:46.603355 15896 agents.py:92] training 1130 of 1500: completed tf_agent.train(...) =    0.547 [loss]\n",
      "I0707 23:49:47.277351 15896 agents.py:92] current policy       : avg_reward=0.723, avg_steps=14.900\n",
      "I0707 23:49:47.642388 15896 agents.py:92] training 1131 of 1500: completed tf_agent.train(...) =    0.567 [loss]\n",
      "I0707 23:49:48.140355 15896 agents.py:92] training 1132 of 1500: completed tf_agent.train(...) =    0.653 [loss]\n",
      "I0707 23:49:48.548355 15896 agents.py:92] training 1133 of 1500: completed tf_agent.train(...) =    0.835 [loss]\n",
      "I0707 23:49:49.020357 15896 agents.py:92] training 1134 of 1500: completed tf_agent.train(...) =    0.519 [loss]\n",
      "I0707 23:49:49.462356 15896 agents.py:92] training 1135 of 1500: completed tf_agent.train(...) =    1.958 [loss]\n",
      "I0707 23:49:50.172355 15896 agents.py:92] current policy       : avg_reward=0.728, avg_steps=13.700\n",
      "I0707 23:49:50.606352 15896 agents.py:92] training 1136 of 1500: completed tf_agent.train(...) =    0.633 [loss]\n",
      "I0707 23:49:51.058356 15896 agents.py:92] training 1137 of 1500: completed tf_agent.train(...) =    0.495 [loss]\n",
      "I0707 23:49:51.523352 15896 agents.py:92] training 1138 of 1500: completed tf_agent.train(...) =    0.769 [loss]\n",
      "I0707 23:49:51.940353 15896 agents.py:92] training 1139 of 1500: completed tf_agent.train(...) =    0.601 [loss]\n",
      "I0707 23:49:52.415358 15896 agents.py:92] training 1140 of 1500: completed tf_agent.train(...) =    0.564 [loss]\n",
      "I0707 23:49:53.163359 15896 agents.py:92] current policy       : avg_reward=0.691, avg_steps=15.400\n",
      "I0707 23:49:53.631353 15896 agents.py:92] training 1141 of 1500: completed tf_agent.train(...) =    0.564 [loss]\n",
      "I0707 23:49:55.434365 15896 agents.py:92] training 1142 of 1500: completed tf_agent.train(...) =    0.524 [loss]\n",
      "I0707 23:49:55.877355 15896 agents.py:92] training 1143 of 1500: completed tf_agent.train(...) =    0.540 [loss]\n",
      "I0707 23:49:56.375353 15896 agents.py:92] training 1144 of 1500: completed tf_agent.train(...) =    1.819 [loss]\n",
      "I0707 23:49:56.736353 15896 agents.py:92] training 1145 of 1500: completed tf_agent.train(...) =    0.646 [loss]\n",
      "I0707 23:49:57.307354 15896 agents.py:92] current policy       : avg_reward=0.730, avg_steps=13.400\n",
      "I0707 23:49:57.784355 15896 agents.py:92] training 1146 of 1500: completed tf_agent.train(...) =    0.495 [loss]\n",
      "I0707 23:49:58.128355 15896 agents.py:92] training 1147 of 1500: completed tf_agent.train(...) =    0.613 [loss]\n",
      "I0707 23:49:58.495351 15896 agents.py:92] training 1148 of 1500: completed tf_agent.train(...) =    0.494 [loss]\n",
      "I0707 23:49:58.992359 15896 agents.py:92] training 1149 of 1500: completed tf_agent.train(...) =    0.583 [loss]\n",
      "I0707 23:49:59.444355 15896 agents.py:92] training 1150 of 1500: completed tf_agent.train(...) =    0.513 [loss]\n",
      "I0707 23:50:00.233351 15896 agents.py:92] current policy       : avg_reward=0.699, avg_steps=16.300\n",
      "I0707 23:50:00.666354 15896 agents.py:92] training 1151 of 1500: completed tf_agent.train(...) =    0.558 [loss]\n",
      "I0707 23:50:01.159352 15896 agents.py:92] training 1152 of 1500: completed tf_agent.train(...) =    1.642 [loss]\n",
      "I0707 23:50:01.539355 15896 agents.py:92] training 1153 of 1500: completed tf_agent.train(...) =    0.594 [loss]\n",
      "I0707 23:50:01.866353 15896 agents.py:92] training 1154 of 1500: completed tf_agent.train(...) =    0.572 [loss]\n",
      "I0707 23:50:02.212356 15896 agents.py:92] training 1155 of 1500: completed tf_agent.train(...) =    0.517 [loss]\n",
      "I0707 23:50:02.998352 15896 agents.py:92] current policy       : avg_reward=0.705, avg_steps=15.700\n",
      "I0707 23:50:03.362356 15896 agents.py:92] training 1156 of 1500: completed tf_agent.train(...) =    0.573 [loss]\n",
      "I0707 23:50:03.732352 15896 agents.py:92] training 1157 of 1500: completed tf_agent.train(...) =    0.874 [loss]\n",
      "I0707 23:50:04.193354 15896 agents.py:92] training 1158 of 1500: completed tf_agent.train(...) =    4.235 [loss]\n",
      "I0707 23:50:04.553357 15896 agents.py:92] training 1159 of 1500: completed tf_agent.train(...) =    0.643 [loss]\n",
      "I0707 23:50:04.879351 15896 agents.py:92] training 1160 of 1500: completed tf_agent.train(...) =    0.558 [loss]\n",
      "I0707 23:50:06.274359 15896 agents.py:92] current policy       : avg_reward=0.402, avg_steps=31.700\n",
      "I0707 23:50:06.651357 15896 agents.py:92] training 1161 of 1500: completed tf_agent.train(...) =    0.472 [loss]\n",
      "I0707 23:50:06.998354 15896 agents.py:92] training 1162 of 1500: completed tf_agent.train(...) =    1.666 [loss]\n",
      "I0707 23:50:07.397354 15896 agents.py:92] training 1163 of 1500: completed tf_agent.train(...) =    0.513 [loss]\n",
      "I0707 23:50:07.773357 15896 agents.py:92] training 1164 of 1500: completed tf_agent.train(...) =    0.475 [loss]\n",
      "I0707 23:50:08.125354 15896 agents.py:92] training 1165 of 1500: completed tf_agent.train(...) =    0.620 [loss]\n",
      "I0707 23:50:08.894351 15896 agents.py:92] current policy       : avg_reward=0.711, avg_steps=15.400\n",
      "I0707 23:50:09.283357 15896 agents.py:92] training 1166 of 1500: completed tf_agent.train(...) =    1.296 [loss]\n",
      "I0707 23:50:09.728352 15896 agents.py:92] training 1167 of 1500: completed tf_agent.train(...) =    0.633 [loss]\n",
      "I0707 23:50:10.165356 15896 agents.py:92] training 1168 of 1500: completed tf_agent.train(...) =    0.583 [loss]\n",
      "I0707 23:50:10.593354 15896 agents.py:92] training 1169 of 1500: completed tf_agent.train(...) =    0.495 [loss]\n",
      "I0707 23:50:11.071353 15896 agents.py:92] training 1170 of 1500: completed tf_agent.train(...) =    0.524 [loss]\n",
      "I0707 23:50:13.342351 15896 agents.py:92] current policy       : avg_reward=0.348, avg_steps=51.700\n",
      "I0707 23:50:13.716390 15896 agents.py:92] training 1171 of 1500: completed tf_agent.train(...) =    0.738 [loss]\n",
      "I0707 23:50:14.093354 15896 agents.py:92] training 1172 of 1500: completed tf_agent.train(...) =    0.599 [loss]\n",
      "I0707 23:50:14.557351 15896 agents.py:92] training 1173 of 1500: completed tf_agent.train(...) =    0.521 [loss]\n",
      "I0707 23:50:14.907354 15896 agents.py:92] training 1174 of 1500: completed tf_agent.train(...) =    0.449 [loss]\n",
      "I0707 23:50:15.371353 15896 agents.py:92] training 1175 of 1500: completed tf_agent.train(...) =    0.462 [loss]\n",
      "I0707 23:50:16.081353 15896 agents.py:92] current policy       : avg_reward=0.708, avg_steps=15.200\n",
      "I0707 23:50:16.486352 15896 agents.py:92] training 1176 of 1500: completed tf_agent.train(...) =    0.567 [loss]\n",
      "I0707 23:50:16.946355 15896 agents.py:92] training 1177 of 1500: completed tf_agent.train(...) =    5.608 [loss]\n",
      "I0707 23:50:17.397355 15896 agents.py:92] training 1178 of 1500: completed tf_agent.train(...) =    0.761 [loss]\n",
      "I0707 23:50:17.870354 15896 agents.py:92] training 1179 of 1500: completed tf_agent.train(...) =    0.620 [loss]\n",
      "I0707 23:50:18.289354 15896 agents.py:92] training 1180 of 1500: completed tf_agent.train(...) =    0.528 [loss]\n",
      "I0707 23:50:19.016351 15896 agents.py:92] current policy       : avg_reward=0.728, avg_steps=14.600\n",
      "I0707 23:50:19.374354 15896 agents.py:92] training 1181 of 1500: completed tf_agent.train(...) =    0.485 [loss]\n",
      "I0707 23:50:19.721354 15896 agents.py:92] training 1182 of 1500: completed tf_agent.train(...) =    0.569 [loss]\n",
      "I0707 23:50:20.153350 15896 agents.py:92] training 1183 of 1500: completed tf_agent.train(...) =    1.285 [loss]\n",
      "I0707 23:50:20.495383 15896 agents.py:92] training 1184 of 1500: completed tf_agent.train(...) =    2.529 [loss]\n",
      "I0707 23:50:20.876352 15896 agents.py:92] training 1185 of 1500: completed tf_agent.train(...) =    1.070 [loss]\n",
      "I0707 23:50:23.068351 15896 agents.py:92] current policy       : avg_reward=0.060, avg_steps=52.100\n",
      "I0707 23:50:23.557355 15896 agents.py:92] training 1186 of 1500: completed tf_agent.train(...) =    1.304 [loss]\n",
      "I0707 23:50:23.972352 15896 agents.py:92] training 1187 of 1500: completed tf_agent.train(...) =    0.567 [loss]\n",
      "I0707 23:50:24.440355 15896 agents.py:92] training 1188 of 1500: completed tf_agent.train(...) =    1.085 [loss]\n",
      "I0707 23:50:24.871354 15896 agents.py:92] training 1189 of 1500: completed tf_agent.train(...) =    1.920 [loss]\n",
      "I0707 23:50:25.322360 15896 agents.py:92] training 1190 of 1500: completed tf_agent.train(...) =    0.496 [loss]\n",
      "I0707 23:50:26.122382 15896 agents.py:92] current policy       : avg_reward=0.707, avg_steps=16.000\n",
      "I0707 23:50:26.578354 15896 agents.py:92] training 1191 of 1500: completed tf_agent.train(...) =    0.849 [loss]\n",
      "I0707 23:50:27.023352 15896 agents.py:92] training 1192 of 1500: completed tf_agent.train(...) =    0.559 [loss]\n",
      "I0707 23:50:27.469353 15896 agents.py:92] training 1193 of 1500: completed tf_agent.train(...) =    0.451 [loss]\n",
      "I0707 23:50:27.928354 15896 agents.py:92] training 1194 of 1500: completed tf_agent.train(...) =    0.633 [loss]\n",
      "I0707 23:50:28.383354 15896 agents.py:92] training 1195 of 1500: completed tf_agent.train(...) =    0.451 [loss]\n",
      "I0707 23:50:29.121387 15896 agents.py:92] current policy       : avg_reward=0.712, avg_steps=15.000\n",
      "I0707 23:50:29.464353 15896 agents.py:92] training 1196 of 1500: completed tf_agent.train(...) =    0.854 [loss]\n",
      "I0707 23:50:29.960351 15896 agents.py:92] training 1197 of 1500: completed tf_agent.train(...) =    0.539 [loss]\n",
      "I0707 23:50:30.397355 15896 agents.py:92] training 1198 of 1500: completed tf_agent.train(...) =    0.393 [loss]\n",
      "I0707 23:50:30.824357 15896 agents.py:92] training 1199 of 1500: completed tf_agent.train(...) =    2.712 [loss]\n",
      "I0707 23:50:31.257354 15896 agents.py:92] training 1200 of 1500: completed tf_agent.train(...) =    0.391 [loss]\n",
      "I0707 23:50:31.937379 15896 agents.py:92] current policy       : avg_reward=0.706, avg_steps=15.500\n",
      "I0707 23:50:32.317352 15896 agents.py:92] training 1201 of 1500: completed tf_agent.train(...) =    0.694 [loss]\n",
      "I0707 23:50:32.760351 15896 agents.py:92] training 1202 of 1500: completed tf_agent.train(...) =    0.455 [loss]\n",
      "I0707 23:50:33.137352 15896 agents.py:92] training 1203 of 1500: completed tf_agent.train(...) =    0.433 [loss]\n",
      "I0707 23:50:33.477374 15896 agents.py:92] training 1204 of 1500: completed tf_agent.train(...) =    0.603 [loss]\n",
      "I0707 23:50:33.859354 15896 agents.py:92] training 1205 of 1500: completed tf_agent.train(...) =    0.509 [loss]\n",
      "I0707 23:50:34.437387 15896 agents.py:92] current policy       : avg_reward=0.750, avg_steps=12.800\n",
      "I0707 23:50:34.833356 15896 agents.py:92] training 1206 of 1500: completed tf_agent.train(...) =    0.449 [loss]\n",
      "I0707 23:50:35.292354 15896 agents.py:92] training 1207 of 1500: completed tf_agent.train(...) =    0.465 [loss]\n",
      "I0707 23:50:35.696355 15896 agents.py:92] training 1208 of 1500: completed tf_agent.train(...) =    0.531 [loss]\n",
      "I0707 23:50:36.159353 15896 agents.py:92] training 1209 of 1500: completed tf_agent.train(...) =    0.563 [loss]\n",
      "I0707 23:50:36.565354 15896 agents.py:92] training 1210 of 1500: completed tf_agent.train(...) =    0.430 [loss]\n",
      "I0707 23:50:37.310351 15896 agents.py:92] current policy       : avg_reward=0.706, avg_steps=15.400\n",
      "I0707 23:50:37.815352 15896 agents.py:92] training 1211 of 1500: completed tf_agent.train(...) =    2.924 [loss]\n",
      "I0707 23:50:38.229356 15896 agents.py:92] training 1212 of 1500: completed tf_agent.train(...) =    0.542 [loss]\n",
      "I0707 23:50:38.701353 15896 agents.py:92] training 1213 of 1500: completed tf_agent.train(...) =    0.495 [loss]\n",
      "I0707 23:50:39.173355 15896 agents.py:92] training 1214 of 1500: completed tf_agent.train(...) =    0.457 [loss]\n",
      "I0707 23:50:39.636358 15896 agents.py:92] training 1215 of 1500: completed tf_agent.train(...) =    0.748 [loss]\n",
      "I0707 23:50:41.122351 15896 agents.py:92] current policy       : avg_reward=0.530, avg_steps=34.000\n",
      "I0707 23:50:41.523355 15896 agents.py:92] training 1216 of 1500: completed tf_agent.train(...) =    0.451 [loss]\n",
      "I0707 23:50:42.007353 15896 agents.py:92] training 1217 of 1500: completed tf_agent.train(...) =    0.952 [loss]\n",
      "I0707 23:50:42.489353 15896 agents.py:92] training 1218 of 1500: completed tf_agent.train(...) =    0.414 [loss]\n",
      "I0707 23:50:42.961351 15896 agents.py:92] training 1219 of 1500: completed tf_agent.train(...) =    0.394 [loss]\n",
      "I0707 23:50:43.387354 15896 agents.py:92] training 1220 of 1500: completed tf_agent.train(...) =    0.447 [loss]\n",
      "I0707 23:50:44.065382 15896 agents.py:92] current policy       : avg_reward=0.727, avg_steps=13.900\n",
      "I0707 23:50:44.441389 15896 agents.py:92] training 1221 of 1500: completed tf_agent.train(...) =    0.574 [loss]\n",
      "I0707 23:50:44.857353 15896 agents.py:92] training 1222 of 1500: completed tf_agent.train(...) =    0.436 [loss]\n",
      "I0707 23:50:45.308354 15896 agents.py:92] training 1223 of 1500: completed tf_agent.train(...) =    0.565 [loss]\n",
      "I0707 23:50:45.766354 15896 agents.py:92] training 1224 of 1500: completed tf_agent.train(...) =    0.418 [loss]\n",
      "I0707 23:50:46.225354 15896 agents.py:92] training 1225 of 1500: completed tf_agent.train(...) =    0.453 [loss]\n",
      "I0707 23:50:46.846360 15896 agents.py:92] current policy       : avg_reward=0.723, avg_steps=14.000\n",
      "I0707 23:50:47.244354 15896 agents.py:92] training 1226 of 1500: completed tf_agent.train(...) =    0.960 [loss]\n",
      "I0707 23:50:47.684355 15896 agents.py:92] training 1227 of 1500: completed tf_agent.train(...) =    0.868 [loss]\n",
      "I0707 23:50:48.145354 15896 agents.py:92] training 1228 of 1500: completed tf_agent.train(...) =    0.459 [loss]\n",
      "I0707 23:50:48.626354 15896 agents.py:92] training 1229 of 1500: completed tf_agent.train(...) =    0.753 [loss]\n",
      "I0707 23:50:49.040352 15896 agents.py:92] training 1230 of 1500: completed tf_agent.train(...) =    0.445 [loss]\n",
      "I0707 23:50:49.765351 15896 agents.py:92] current policy       : avg_reward=0.714, avg_steps=14.600\n",
      "I0707 23:50:50.090358 15896 agents.py:92] training 1231 of 1500: completed tf_agent.train(...) =    0.705 [loss]\n",
      "I0707 23:50:50.604352 15896 agents.py:92] training 1232 of 1500: completed tf_agent.train(...) =    0.533 [loss]\n",
      "I0707 23:50:50.916393 15896 agents.py:92] training 1233 of 1500: completed tf_agent.train(...) =    0.480 [loss]\n",
      "I0707 23:50:51.307350 15896 agents.py:92] training 1234 of 1500: completed tf_agent.train(...) =    0.452 [loss]\n",
      "I0707 23:50:51.745353 15896 agents.py:92] training 1235 of 1500: completed tf_agent.train(...) =    0.404 [loss]\n",
      "I0707 23:50:53.233352 15896 agents.py:92] current policy       : avg_reward=0.388, avg_steps=32.800\n",
      "I0707 23:50:53.661355 15896 agents.py:92] training 1236 of 1500: completed tf_agent.train(...) =    0.391 [loss]\n",
      "I0707 23:50:54.113357 15896 agents.py:92] training 1237 of 1500: completed tf_agent.train(...) =    0.418 [loss]\n",
      "I0707 23:50:54.544353 15896 agents.py:92] training 1238 of 1500: completed tf_agent.train(...) =    0.670 [loss]\n",
      "I0707 23:50:54.944356 15896 agents.py:92] training 1239 of 1500: completed tf_agent.train(...) =    0.539 [loss]\n",
      "I0707 23:50:55.276389 15896 agents.py:92] training 1240 of 1500: completed tf_agent.train(...) =    0.824 [loss]\n",
      "I0707 23:50:56.757366 15896 agents.py:92] current policy       : avg_reward=0.498, avg_steps=32.900\n",
      "I0707 23:50:57.193354 15896 agents.py:92] training 1241 of 1500: completed tf_agent.train(...) =    0.471 [loss]\n",
      "I0707 23:50:57.670350 15896 agents.py:92] training 1242 of 1500: completed tf_agent.train(...) =    0.537 [loss]\n",
      "I0707 23:50:58.044390 15896 agents.py:92] training 1243 of 1500: completed tf_agent.train(...) =    0.425 [loss]\n",
      "I0707 23:50:58.416355 15896 agents.py:92] training 1244 of 1500: completed tf_agent.train(...) =    0.575 [loss]\n",
      "I0707 23:50:58.898356 15896 agents.py:92] training 1245 of 1500: completed tf_agent.train(...) =    0.719 [loss]\n",
      "I0707 23:51:01.832351 15896 agents.py:92] current policy       : avg_reward=-0.661, avg_steps=70.300\n",
      "I0707 23:51:02.294356 15896 agents.py:92] training 1246 of 1500: completed tf_agent.train(...) =    0.610 [loss]\n",
      "I0707 23:51:02.671354 15896 agents.py:92] training 1247 of 1500: completed tf_agent.train(...) =    0.679 [loss]\n",
      "I0707 23:51:03.017392 15896 agents.py:92] training 1248 of 1500: completed tf_agent.train(...) =    0.551 [loss]\n",
      "I0707 23:51:03.453357 15896 agents.py:92] training 1249 of 1500: completed tf_agent.train(...) =    1.319 [loss]\n",
      "I0707 23:51:03.910356 15896 agents.py:92] training 1250 of 1500: completed tf_agent.train(...) =    0.898 [loss]\n",
      "I0707 23:51:05.463351 15896 agents.py:92] current policy       : avg_reward=0.483, avg_steps=34.800\n",
      "I0707 23:51:05.928071 15896 agents.py:92] training 1251 of 1500: completed tf_agent.train(...) =    1.272 [loss]\n",
      "I0707 23:51:06.289069 15896 agents.py:92] training 1252 of 1500: completed tf_agent.train(...) =    1.047 [loss]\n",
      "I0707 23:51:06.633072 15896 agents.py:92] training 1253 of 1500: completed tf_agent.train(...) =    0.570 [loss]\n",
      "I0707 23:51:07.093070 15896 agents.py:92] training 1254 of 1500: completed tf_agent.train(...) =    0.614 [loss]\n",
      "I0707 23:51:07.533070 15896 agents.py:92] training 1255 of 1500: completed tf_agent.train(...) =    1.369 [loss]\n",
      "I0707 23:51:09.056101 15896 agents.py:92] current policy       : avg_reward=0.539, avg_steps=34.400\n",
      "I0707 23:51:09.451098 15896 agents.py:92] training 1256 of 1500: completed tf_agent.train(...) =    0.829 [loss]\n",
      "I0707 23:51:11.460068 15896 agents.py:92] training 1257 of 1500: completed tf_agent.train(...) =  314.368 [loss]\n",
      "I0707 23:51:11.832068 15896 agents.py:92] training 1258 of 1500: completed tf_agent.train(...) =    2.075 [loss]\n",
      "I0707 23:51:12.255070 15896 agents.py:92] training 1259 of 1500: completed tf_agent.train(...) =    3.350 [loss]\n",
      "I0707 23:51:12.699071 15896 agents.py:92] training 1260 of 1500: completed tf_agent.train(...) =    2.535 [loss]\n",
      "I0707 23:51:13.361065 15896 agents.py:92] current policy       : avg_reward=0.730, avg_steps=14.700\n",
      "I0707 23:51:13.746070 15896 agents.py:92] training 1261 of 1500: completed tf_agent.train(...) =    1.642 [loss]\n",
      "I0707 23:51:14.163068 15896 agents.py:92] training 1262 of 1500: completed tf_agent.train(...) =    1.225 [loss]\n",
      "I0707 23:51:14.628069 15896 agents.py:92] training 1263 of 1500: completed tf_agent.train(...) =    1.434 [loss]\n",
      "I0707 23:51:15.057072 15896 agents.py:92] training 1264 of 1500: completed tf_agent.train(...) =    1.192 [loss]\n",
      "I0707 23:51:15.516067 15896 agents.py:92] training 1265 of 1500: completed tf_agent.train(...) =    1.143 [loss]\n",
      "I0707 23:51:16.189065 15896 agents.py:92] current policy       : avg_reward=0.751, avg_steps=13.000\n",
      "I0707 23:51:16.497070 15896 agents.py:92] training 1266 of 1500: completed tf_agent.train(...) =    1.814 [loss]\n",
      "I0707 23:51:16.867066 15896 agents.py:92] training 1267 of 1500: completed tf_agent.train(...) =    1.286 [loss]\n",
      "I0707 23:51:17.316069 15896 agents.py:92] training 1268 of 1500: completed tf_agent.train(...) =    1.469 [loss]\n",
      "I0707 23:51:17.746068 15896 agents.py:92] training 1269 of 1500: completed tf_agent.train(...) =    0.840 [loss]\n",
      "I0707 23:51:18.164068 15896 agents.py:92] training 1270 of 1500: completed tf_agent.train(...) =    0.852 [loss]\n",
      "I0707 23:51:19.628102 15896 agents.py:92] current policy       : avg_reward=0.546, avg_steps=32.900\n",
      "I0707 23:51:20.061069 15896 agents.py:92] training 1271 of 1500: completed tf_agent.train(...) =    1.092 [loss]\n",
      "I0707 23:51:20.528067 15896 agents.py:92] training 1272 of 1500: completed tf_agent.train(...) =    1.846 [loss]\n",
      "I0707 23:51:20.927075 15896 agents.py:92] training 1273 of 1500: completed tf_agent.train(...) =    0.697 [loss]\n",
      "I0707 23:51:21.310068 15896 agents.py:92] training 1274 of 1500: completed tf_agent.train(...) =    1.163 [loss]\n",
      "I0707 23:51:21.682067 15896 agents.py:92] training 1275 of 1500: completed tf_agent.train(...) =    2.178 [loss]\n",
      "I0707 23:51:22.473066 15896 agents.py:92] current policy       : avg_reward=0.714, avg_steps=15.300\n",
      "I0707 23:51:22.825070 15896 agents.py:92] training 1276 of 1500: completed tf_agent.train(...) =    0.749 [loss]\n",
      "I0707 23:51:23.294069 15896 agents.py:92] training 1277 of 1500: completed tf_agent.train(...) =    0.934 [loss]\n",
      "I0707 23:51:23.636070 15896 agents.py:92] training 1278 of 1500: completed tf_agent.train(...) =    0.885 [loss]\n",
      "I0707 23:51:24.029072 15896 agents.py:92] training 1279 of 1500: completed tf_agent.train(...) =    1.460 [loss]\n",
      "I0707 23:51:24.465069 15896 agents.py:92] training 1280 of 1500: completed tf_agent.train(...) =    0.749 [loss]\n",
      "I0707 23:51:25.161067 15896 agents.py:92] current policy       : avg_reward=0.745, avg_steps=13.800\n",
      "I0707 23:51:25.628068 15896 agents.py:92] training 1281 of 1500: completed tf_agent.train(...) =    0.581 [loss]\n",
      "I0707 23:51:26.011069 15896 agents.py:92] training 1282 of 1500: completed tf_agent.train(...) =    0.675 [loss]\n",
      "I0707 23:51:26.342065 15896 agents.py:92] training 1283 of 1500: completed tf_agent.train(...) =    0.463 [loss]\n",
      "I0707 23:51:26.791068 15896 agents.py:92] training 1284 of 1500: completed tf_agent.train(...) =    0.544 [loss]\n",
      "I0707 23:51:27.190073 15896 agents.py:92] training 1285 of 1500: completed tf_agent.train(...) =    0.753 [loss]\n",
      "I0707 23:51:27.934067 15896 agents.py:92] current policy       : avg_reward=0.721, avg_steps=14.200\n",
      "I0707 23:51:28.416069 15896 agents.py:92] training 1286 of 1500: completed tf_agent.train(...) =    0.564 [loss]\n",
      "I0707 23:51:28.834069 15896 agents.py:92] training 1287 of 1500: completed tf_agent.train(...) =    0.488 [loss]\n",
      "I0707 23:51:29.283068 15896 agents.py:92] training 1288 of 1500: completed tf_agent.train(...) =    0.560 [loss]\n",
      "I0707 23:51:29.708068 15896 agents.py:92] training 1289 of 1500: completed tf_agent.train(...) =    0.797 [loss]\n",
      "I0707 23:51:30.169069 15896 agents.py:92] training 1290 of 1500: completed tf_agent.train(...) =    0.685 [loss]\n",
      "I0707 23:51:30.818068 15896 agents.py:92] current policy       : avg_reward=0.724, avg_steps=14.600\n",
      "I0707 23:51:31.181067 15896 agents.py:92] training 1291 of 1500: completed tf_agent.train(...) =    0.565 [loss]\n",
      "I0707 23:51:32.967112 15896 agents.py:92] training 1292 of 1500: completed tf_agent.train(...) =    0.448 [loss]\n",
      "I0707 23:51:33.337066 15896 agents.py:92] training 1293 of 1500: completed tf_agent.train(...) =    0.564 [loss]\n",
      "I0707 23:51:33.705069 15896 agents.py:92] training 1294 of 1500: completed tf_agent.train(...) =    0.754 [loss]\n",
      "I0707 23:51:34.139067 15896 agents.py:92] training 1295 of 1500: completed tf_agent.train(...) =    0.489 [loss]\n",
      "I0707 23:51:34.889063 15896 agents.py:92] current policy       : avg_reward=0.705, avg_steps=15.900\n",
      "I0707 23:51:35.329068 15896 agents.py:92] training 1296 of 1500: completed tf_agent.train(...) =    0.450 [loss]\n",
      "I0707 23:51:35.815066 15896 agents.py:92] training 1297 of 1500: completed tf_agent.train(...) =    0.567 [loss]\n",
      "I0707 23:51:36.262069 15896 agents.py:92] training 1298 of 1500: completed tf_agent.train(...) =    0.497 [loss]\n",
      "I0707 23:51:36.674071 15896 agents.py:92] training 1299 of 1500: completed tf_agent.train(...) =    0.649 [loss]\n",
      "I0707 23:51:37.099071 15896 agents.py:92] training 1300 of 1500: completed tf_agent.train(...) =    0.776 [loss]\n",
      "I0707 23:51:37.844067 15896 agents.py:92] current policy       : avg_reward=0.722, avg_steps=15.000\n",
      "I0707 23:51:38.252070 15896 agents.py:92] training 1301 of 1500: completed tf_agent.train(...) =    0.491 [loss]\n",
      "I0707 23:51:38.668067 15896 agents.py:92] training 1302 of 1500: completed tf_agent.train(...) =    0.502 [loss]\n",
      "I0707 23:51:39.128065 15896 agents.py:92] training 1303 of 1500: completed tf_agent.train(...) =    0.571 [loss]\n",
      "I0707 23:51:39.567068 15896 agents.py:92] training 1304 of 1500: completed tf_agent.train(...) =    0.985 [loss]\n",
      "I0707 23:51:40.027070 15896 agents.py:92] training 1305 of 1500: completed tf_agent.train(...) =    0.612 [loss]\n",
      "I0707 23:51:40.662073 15896 agents.py:92] current policy       : avg_reward=0.738, avg_steps=14.000\n",
      "I0707 23:51:41.028070 15896 agents.py:92] training 1306 of 1500: completed tf_agent.train(...) =    0.674 [loss]\n",
      "I0707 23:51:41.508067 15896 agents.py:92] training 1307 of 1500: completed tf_agent.train(...) =    0.509 [loss]\n",
      "I0707 23:51:41.920073 15896 agents.py:92] training 1308 of 1500: completed tf_agent.train(...) =    0.453 [loss]\n",
      "I0707 23:51:42.397069 15896 agents.py:92] training 1309 of 1500: completed tf_agent.train(...) =    0.444 [loss]\n",
      "I0707 23:51:42.815069 15896 agents.py:92] training 1310 of 1500: completed tf_agent.train(...) =    0.513 [loss]\n",
      "I0707 23:51:43.571096 15896 agents.py:92] current policy       : avg_reward=0.714, avg_steps=15.000\n",
      "I0707 23:51:43.987070 15896 agents.py:92] training 1311 of 1500: completed tf_agent.train(...) =    0.524 [loss]\n",
      "I0707 23:51:44.337068 15896 agents.py:92] training 1312 of 1500: completed tf_agent.train(...) =    7.424 [loss]\n",
      "I0707 23:51:44.778068 15896 agents.py:92] training 1313 of 1500: completed tf_agent.train(...) =    1.244 [loss]\n",
      "I0707 23:51:45.249066 15896 agents.py:92] training 1314 of 1500: completed tf_agent.train(...) =    0.723 [loss]\n",
      "I0707 23:51:45.683070 15896 agents.py:92] training 1315 of 1500: completed tf_agent.train(...) =    0.566 [loss]\n",
      "I0707 23:51:47.152067 15896 agents.py:92] current policy       : avg_reward=0.401, avg_steps=33.000\n",
      "I0707 23:51:47.629068 15896 agents.py:92] training 1316 of 1500: completed tf_agent.train(...) =    0.546 [loss]\n",
      "I0707 23:51:48.082070 15896 agents.py:92] training 1317 of 1500: completed tf_agent.train(...) =    0.654 [loss]\n",
      "I0707 23:51:48.522069 15896 agents.py:92] training 1318 of 1500: completed tf_agent.train(...) =    0.489 [loss]\n",
      "I0707 23:51:48.970070 15896 agents.py:92] training 1319 of 1500: completed tf_agent.train(...) =    0.429 [loss]\n",
      "I0707 23:51:49.417069 15896 agents.py:92] training 1320 of 1500: completed tf_agent.train(...) =    0.552 [loss]\n",
      "I0707 23:51:50.933066 15896 agents.py:92] current policy       : avg_reward=0.373, avg_steps=34.700\n",
      "I0707 23:51:51.336067 15896 agents.py:92] training 1321 of 1500: completed tf_agent.train(...) =    3.958 [loss]\n",
      "I0707 23:51:51.674106 15896 agents.py:92] training 1322 of 1500: completed tf_agent.train(...) =    0.522 [loss]\n",
      "I0707 23:51:52.162069 15896 agents.py:92] training 1323 of 1500: completed tf_agent.train(...) =    0.469 [loss]\n",
      "I0707 23:51:52.603069 15896 agents.py:92] training 1324 of 1500: completed tf_agent.train(...) =    2.231 [loss]\n",
      "I0707 23:51:53.025065 15896 agents.py:92] training 1325 of 1500: completed tf_agent.train(...) =    0.690 [loss]\n",
      "I0707 23:51:53.728096 15896 agents.py:92] current policy       : avg_reward=0.728, avg_steps=14.200\n",
      "I0707 23:51:54.147071 15896 agents.py:92] training 1326 of 1500: completed tf_agent.train(...) =    0.477 [loss]\n",
      "I0707 23:51:54.603067 15896 agents.py:92] training 1327 of 1500: completed tf_agent.train(...) =    0.401 [loss]\n",
      "I0707 23:51:54.991069 15896 agents.py:92] training 1328 of 1500: completed tf_agent.train(...) =    1.034 [loss]\n",
      "I0707 23:51:55.406084 15896 agents.py:92] training 1329 of 1500: completed tf_agent.train(...) =    0.522 [loss]\n",
      "I0707 23:51:55.756069 15896 agents.py:92] training 1330 of 1500: completed tf_agent.train(...) =    0.639 [loss]\n",
      "I0707 23:51:56.545066 15896 agents.py:92] current policy       : avg_reward=0.705, avg_steps=15.400\n",
      "I0707 23:51:57.029068 15896 agents.py:92] training 1331 of 1500: completed tf_agent.train(...) =    1.252 [loss]\n",
      "I0707 23:51:57.389067 15896 agents.py:92] training 1332 of 1500: completed tf_agent.train(...) =    0.532 [loss]\n",
      "I0707 23:51:57.760066 15896 agents.py:92] training 1333 of 1500: completed tf_agent.train(...) =    7.933 [loss]\n",
      "I0707 23:51:58.193069 15896 agents.py:92] training 1334 of 1500: completed tf_agent.train(...) =    0.893 [loss]\n",
      "I0707 23:51:58.684068 15896 agents.py:92] training 1335 of 1500: completed tf_agent.train(...) =    0.602 [loss]\n",
      "I0707 23:51:59.428065 15896 agents.py:92] current policy       : avg_reward=0.708, avg_steps=15.400\n",
      "I0707 23:51:59.909068 15896 agents.py:92] training 1336 of 1500: completed tf_agent.train(...) =    0.983 [loss]\n",
      "I0707 23:52:00.384069 15896 agents.py:92] training 1337 of 1500: completed tf_agent.train(...) =   15.905 [loss]\n",
      "I0707 23:52:00.966069 15896 agents.py:92] training 1338 of 1500: completed tf_agent.train(...) =  169.128 [loss]\n",
      "I0707 23:52:01.482065 15896 agents.py:92] training 1339 of 1500: completed tf_agent.train(...) =   15.959 [loss]\n",
      "I0707 23:52:01.922067 15896 agents.py:92] training 1340 of 1500: completed tf_agent.train(...) =    3.477 [loss]\n",
      "I0707 23:52:02.698066 15896 agents.py:92] current policy       : avg_reward=0.699, avg_steps=15.800\n",
      "I0707 23:52:03.158071 15896 agents.py:92] training 1341 of 1500: completed tf_agent.train(...) =    2.198 [loss]\n",
      "I0707 23:52:03.540096 15896 agents.py:92] training 1342 of 1500: completed tf_agent.train(...) =    2.543 [loss]\n",
      "I0707 23:52:03.920067 15896 agents.py:92] training 1343 of 1500: completed tf_agent.train(...) =    1.664 [loss]\n",
      "I0707 23:52:04.336072 15896 agents.py:92] training 1344 of 1500: completed tf_agent.train(...) =    0.981 [loss]\n",
      "I0707 23:52:04.793066 15896 agents.py:92] training 1345 of 1500: completed tf_agent.train(...) =    1.075 [loss]\n",
      "I0707 23:52:06.212068 15896 agents.py:92] current policy       : avg_reward=0.553, avg_steps=32.300\n",
      "I0707 23:52:06.656070 15896 agents.py:92] training 1346 of 1500: completed tf_agent.train(...) =    1.274 [loss]\n",
      "I0707 23:52:07.155071 15896 agents.py:92] training 1347 of 1500: completed tf_agent.train(...) =    0.994 [loss]\n",
      "I0707 23:52:07.588068 15896 agents.py:92] training 1348 of 1500: completed tf_agent.train(...) =    0.851 [loss]\n",
      "I0707 23:52:08.057070 15896 agents.py:92] training 1349 of 1500: completed tf_agent.train(...) =    0.701 [loss]\n",
      "I0707 23:52:08.528069 15896 agents.py:92] training 1350 of 1500: completed tf_agent.train(...) =    1.009 [loss]\n",
      "I0707 23:52:09.201069 15896 agents.py:92] current policy       : avg_reward=0.744, avg_steps=12.600\n",
      "I0707 23:52:09.686070 15896 agents.py:92] training 1351 of 1500: completed tf_agent.train(...) =    8.126 [loss]\n",
      "I0707 23:52:10.120071 15896 agents.py:92] training 1352 of 1500: completed tf_agent.train(...) =    1.078 [loss]\n",
      "I0707 23:52:10.586069 15896 agents.py:92] training 1353 of 1500: completed tf_agent.train(...) =    1.199 [loss]\n",
      "I0707 23:52:11.029075 15896 agents.py:92] training 1354 of 1500: completed tf_agent.train(...) =    1.105 [loss]\n",
      "I0707 23:52:11.484068 15896 agents.py:92] training 1355 of 1500: completed tf_agent.train(...) =    0.704 [loss]\n",
      "I0707 23:52:12.163066 15896 agents.py:92] current policy       : avg_reward=0.727, avg_steps=13.900\n",
      "I0707 23:52:12.597071 15896 agents.py:92] training 1356 of 1500: completed tf_agent.train(...) =    0.797 [loss]\n",
      "I0707 23:52:13.060068 15896 agents.py:92] training 1357 of 1500: completed tf_agent.train(...) =    1.249 [loss]\n",
      "I0707 23:52:13.515069 15896 agents.py:92] training 1358 of 1500: completed tf_agent.train(...) =    1.104 [loss]\n",
      "I0707 23:52:13.957069 15896 agents.py:92] training 1359 of 1500: completed tf_agent.train(...) =    2.071 [loss]\n",
      "I0707 23:52:15.812067 15896 agents.py:92] training 1360 of 1500: completed tf_agent.train(...) =    0.604 [loss]\n",
      "I0707 23:52:16.552066 15896 agents.py:92] current policy       : avg_reward=0.708, avg_steps=15.000\n",
      "I0707 23:52:16.887066 15896 agents.py:92] training 1361 of 1500: completed tf_agent.train(...) =    0.582 [loss]\n",
      "I0707 23:52:17.375068 15896 agents.py:92] training 1362 of 1500: completed tf_agent.train(...) =    1.649 [loss]\n",
      "I0707 23:52:17.786069 15896 agents.py:92] training 1363 of 1500: completed tf_agent.train(...) =    0.569 [loss]\n",
      "I0707 23:52:18.168069 15896 agents.py:92] training 1364 of 1500: completed tf_agent.train(...) =    0.515 [loss]\n",
      "I0707 23:52:18.496068 15896 agents.py:92] training 1365 of 1500: completed tf_agent.train(...) =    0.729 [loss]\n",
      "I0707 23:52:19.251072 15896 agents.py:92] current policy       : avg_reward=0.721, avg_steps=14.500\n",
      "I0707 23:52:19.683071 15896 agents.py:92] training 1366 of 1500: completed tf_agent.train(...) =    0.732 [loss]\n",
      "I0707 23:52:20.081069 15896 agents.py:92] training 1367 of 1500: completed tf_agent.train(...) =    0.725 [loss]\n",
      "I0707 23:52:20.495069 15896 agents.py:92] training 1368 of 1500: completed tf_agent.train(...) =    0.630 [loss]\n",
      "I0707 23:52:20.931070 15896 agents.py:92] training 1369 of 1500: completed tf_agent.train(...) =    0.841 [loss]\n",
      "I0707 23:52:21.291066 15896 agents.py:92] training 1370 of 1500: completed tf_agent.train(...) =    0.558 [loss]\n",
      "I0707 23:52:22.016067 15896 agents.py:92] current policy       : avg_reward=0.715, avg_steps=14.700\n",
      "I0707 23:52:22.482066 15896 agents.py:92] training 1371 of 1500: completed tf_agent.train(...) =    0.495 [loss]\n",
      "I0707 23:52:22.901071 15896 agents.py:92] training 1372 of 1500: completed tf_agent.train(...) =    0.393 [loss]\n",
      "I0707 23:52:23.349067 15896 agents.py:92] training 1373 of 1500: completed tf_agent.train(...) =    1.007 [loss]\n",
      "I0707 23:52:23.756074 15896 agents.py:92] training 1374 of 1500: completed tf_agent.train(...) =    0.417 [loss]\n",
      "I0707 23:52:24.229072 15896 agents.py:92] training 1375 of 1500: completed tf_agent.train(...) =    0.416 [loss]\n",
      "I0707 23:52:24.998066 15896 agents.py:92] current policy       : avg_reward=0.717, avg_steps=15.400\n",
      "I0707 23:52:25.437072 15896 agents.py:92] training 1376 of 1500: completed tf_agent.train(...) =    0.406 [loss]\n",
      "I0707 23:52:25.881066 15896 agents.py:92] training 1377 of 1500: completed tf_agent.train(...) =    0.457 [loss]\n",
      "I0707 23:52:26.281069 15896 agents.py:92] training 1378 of 1500: completed tf_agent.train(...) =    0.646 [loss]\n",
      "I0707 23:52:26.718070 15896 agents.py:92] training 1379 of 1500: completed tf_agent.train(...) =    0.509 [loss]\n",
      "I0707 23:52:27.140075 15896 agents.py:92] training 1380 of 1500: completed tf_agent.train(...) =    0.793 [loss]\n",
      "I0707 23:52:27.910096 15896 agents.py:92] current policy       : avg_reward=0.705, avg_steps=14.500\n",
      "I0707 23:52:28.265068 15896 agents.py:92] training 1381 of 1500: completed tf_agent.train(...) =    1.763 [loss]\n",
      "I0707 23:52:28.702069 15896 agents.py:92] training 1382 of 1500: completed tf_agent.train(...) =    0.413 [loss]\n",
      "I0707 23:52:29.137069 15896 agents.py:92] training 1383 of 1500: completed tf_agent.train(...) =    0.383 [loss]\n",
      "I0707 23:52:29.565070 15896 agents.py:92] training 1384 of 1500: completed tf_agent.train(...) =    0.542 [loss]\n",
      "I0707 23:52:29.918067 15896 agents.py:92] training 1385 of 1500: completed tf_agent.train(...) =    1.208 [loss]\n",
      "I0707 23:52:30.536100 15896 agents.py:92] current policy       : avg_reward=0.728, avg_steps=14.300\n",
      "I0707 23:52:30.998074 15896 agents.py:92] training 1386 of 1500: completed tf_agent.train(...) =    0.475 [loss]\n",
      "I0707 23:52:31.333068 15896 agents.py:92] training 1387 of 1500: completed tf_agent.train(...) =    1.221 [loss]\n",
      "I0707 23:52:31.739067 15896 agents.py:92] training 1388 of 1500: completed tf_agent.train(...) =    0.403 [loss]\n",
      "I0707 23:52:32.195068 15896 agents.py:92] training 1389 of 1500: completed tf_agent.train(...) =    0.647 [loss]\n",
      "I0707 23:52:32.567074 15896 agents.py:92] training 1390 of 1500: completed tf_agent.train(...) =    0.425 [loss]\n",
      "I0707 23:52:33.174102 15896 agents.py:92] current policy       : avg_reward=0.711, avg_steps=13.700\n",
      "I0707 23:52:33.632068 15896 agents.py:92] training 1391 of 1500: completed tf_agent.train(...) =    0.686 [loss]\n",
      "I0707 23:52:34.083074 15896 agents.py:92] training 1392 of 1500: completed tf_agent.train(...) =    0.720 [loss]\n",
      "I0707 23:52:34.506068 15896 agents.py:92] training 1393 of 1500: completed tf_agent.train(...) =    0.445 [loss]\n",
      "I0707 23:52:34.933065 15896 agents.py:92] training 1394 of 1500: completed tf_agent.train(...) =    0.422 [loss]\n",
      "I0707 23:52:35.377070 15896 agents.py:92] training 1395 of 1500: completed tf_agent.train(...) =    0.415 [loss]\n",
      "I0707 23:52:36.183104 15896 agents.py:92] current policy       : avg_reward=0.698, avg_steps=16.300\n",
      "I0707 23:52:36.542065 15896 agents.py:92] training 1396 of 1500: completed tf_agent.train(...) =    0.452 [loss]\n",
      "I0707 23:52:37.009069 15896 agents.py:92] training 1397 of 1500: completed tf_agent.train(...) =    0.457 [loss]\n",
      "I0707 23:52:37.443070 15896 agents.py:92] training 1398 of 1500: completed tf_agent.train(...) =    0.364 [loss]\n",
      "I0707 23:52:37.904067 15896 agents.py:92] training 1399 of 1500: completed tf_agent.train(...) =    0.820 [loss]\n",
      "I0707 23:52:38.384069 15896 agents.py:92] training 1400 of 1500: completed tf_agent.train(...) =    0.357 [loss]\n",
      "I0707 23:52:39.130067 15896 agents.py:92] current policy       : avg_reward=0.704, avg_steps=16.400\n",
      "I0707 23:52:39.604069 15896 agents.py:92] training 1401 of 1500: completed tf_agent.train(...) =    0.356 [loss]\n",
      "I0707 23:52:40.042071 15896 agents.py:92] training 1402 of 1500: completed tf_agent.train(...) =    0.435 [loss]\n",
      "I0707 23:52:40.430073 15896 agents.py:92] training 1403 of 1500: completed tf_agent.train(...) =    0.372 [loss]\n",
      "I0707 23:52:40.895074 15896 agents.py:92] training 1404 of 1500: completed tf_agent.train(...) =    0.484 [loss]\n",
      "I0707 23:52:41.334069 15896 agents.py:92] training 1405 of 1500: completed tf_agent.train(...) =    0.507 [loss]\n",
      "I0707 23:52:42.005103 15896 agents.py:92] current policy       : avg_reward=0.737, avg_steps=12.900\n",
      "I0707 23:52:42.475070 15896 agents.py:92] training 1406 of 1500: completed tf_agent.train(...) =    0.603 [loss]\n",
      "I0707 23:52:42.892069 15896 agents.py:92] training 1407 of 1500: completed tf_agent.train(...) =    1.939 [loss]\n",
      "I0707 23:52:43.322068 15896 agents.py:92] training 1408 of 1500: completed tf_agent.train(...) =    0.559 [loss]\n",
      "I0707 23:52:43.736073 15896 agents.py:92] training 1409 of 1500: completed tf_agent.train(...) =    1.068 [loss]\n",
      "I0707 23:52:44.221070 15896 agents.py:92] training 1410 of 1500: completed tf_agent.train(...) =    0.425 [loss]\n",
      "I0707 23:52:44.956070 15896 agents.py:92] current policy       : avg_reward=0.737, avg_steps=14.200\n",
      "I0707 23:52:45.380070 15896 agents.py:92] training 1411 of 1500: completed tf_agent.train(...) =    0.549 [loss]\n",
      "I0707 23:52:45.833068 15896 agents.py:92] training 1412 of 1500: completed tf_agent.train(...) =    0.595 [loss]\n",
      "I0707 23:52:46.301068 15896 agents.py:92] training 1413 of 1500: completed tf_agent.train(...) =    0.423 [loss]\n",
      "I0707 23:52:46.617065 15896 agents.py:92] training 1414 of 1500: completed tf_agent.train(...) =    0.624 [loss]\n",
      "I0707 23:52:47.005064 15896 agents.py:92] training 1415 of 1500: completed tf_agent.train(...) =    0.443 [loss]\n",
      "I0707 23:52:47.731066 15896 agents.py:92] current policy       : avg_reward=0.701, avg_steps=15.000\n",
      "I0707 23:52:48.174072 15896 agents.py:92] training 1416 of 1500: completed tf_agent.train(...) =    2.874 [loss]\n",
      "I0707 23:52:48.605069 15896 agents.py:92] training 1417 of 1500: completed tf_agent.train(...) =    0.413 [loss]\n",
      "I0707 23:52:49.107069 15896 agents.py:92] training 1418 of 1500: completed tf_agent.train(...) =   22.694 [loss]\n",
      "I0707 23:52:49.530066 15896 agents.py:92] training 1419 of 1500: completed tf_agent.train(...) =    0.478 [loss]\n",
      "I0707 23:52:49.968069 15896 agents.py:92] training 1420 of 1500: completed tf_agent.train(...) =    0.451 [loss]\n",
      "I0707 23:52:50.647066 15896 agents.py:92] current policy       : avg_reward=0.741, avg_steps=13.000\n",
      "I0707 23:52:51.094082 15896 agents.py:92] training 1421 of 1500: completed tf_agent.train(...) =    0.455 [loss]\n",
      "I0707 23:52:51.536068 15896 agents.py:92] training 1422 of 1500: completed tf_agent.train(...) =    0.495 [loss]\n",
      "I0707 23:52:51.997068 15896 agents.py:92] training 1423 of 1500: completed tf_agent.train(...) =    0.512 [loss]\n",
      "I0707 23:52:52.445067 15896 agents.py:92] training 1424 of 1500: completed tf_agent.train(...) =    0.441 [loss]\n",
      "I0707 23:52:52.862070 15896 agents.py:92] training 1425 of 1500: completed tf_agent.train(...) =    0.435 [loss]\n",
      "I0707 23:52:53.543067 15896 agents.py:92] current policy       : avg_reward=0.724, avg_steps=15.800\n",
      "I0707 23:52:53.940069 15896 agents.py:92] training 1426 of 1500: completed tf_agent.train(...) =    0.676 [loss]\n",
      "I0707 23:52:54.403066 15896 agents.py:92] training 1427 of 1500: completed tf_agent.train(...) =    0.442 [loss]\n",
      "I0707 23:52:54.839077 15896 agents.py:92] training 1428 of 1500: completed tf_agent.train(...) =    0.483 [loss]\n",
      "I0707 23:52:55.251071 15896 agents.py:92] training 1429 of 1500: completed tf_agent.train(...) =    0.627 [loss]\n",
      "I0707 23:52:55.697072 15896 agents.py:92] training 1430 of 1500: completed tf_agent.train(...) =    0.341 [loss]\n",
      "I0707 23:52:56.421066 15896 agents.py:92] current policy       : avg_reward=0.742, avg_steps=12.700\n",
      "I0707 23:52:56.765112 15896 agents.py:92] training 1431 of 1500: completed tf_agent.train(...) =    1.055 [loss]\n",
      "I0707 23:52:57.134066 15896 agents.py:92] training 1432 of 1500: completed tf_agent.train(...) =    0.438 [loss]\n",
      "I0707 23:52:57.601069 15896 agents.py:92] training 1433 of 1500: completed tf_agent.train(...) =    0.370 [loss]\n",
      "I0707 23:52:57.970069 15896 agents.py:92] training 1434 of 1500: completed tf_agent.train(...) =    0.342 [loss]\n",
      "I0707 23:52:58.319067 15896 agents.py:92] training 1435 of 1500: completed tf_agent.train(...) =    0.565 [loss]\n",
      "I0707 23:52:59.123102 15896 agents.py:92] current policy       : avg_reward=0.720, avg_steps=15.700\n",
      "I0707 23:52:59.464072 15896 agents.py:92] training 1436 of 1500: completed tf_agent.train(...) =    0.386 [loss]\n",
      "I0707 23:52:59.813069 15896 agents.py:92] training 1437 of 1500: completed tf_agent.train(...) =    0.767 [loss]\n",
      "I0707 23:53:00.190065 15896 agents.py:92] training 1438 of 1500: completed tf_agent.train(...) =    0.658 [loss]\n",
      "I0707 23:53:00.645067 15896 agents.py:92] training 1439 of 1500: completed tf_agent.train(...) =    0.463 [loss]\n",
      "I0707 23:53:00.996098 15896 agents.py:92] training 1440 of 1500: completed tf_agent.train(...) =    0.428 [loss]\n",
      "I0707 23:53:01.687102 15896 agents.py:92] current policy       : avg_reward=0.708, avg_steps=15.400\n",
      "I0707 23:53:02.144103 15896 agents.py:92] training 1441 of 1500: completed tf_agent.train(...) =    0.455 [loss]\n",
      "I0707 23:53:02.623068 15896 agents.py:92] training 1442 of 1500: completed tf_agent.train(...) =    0.423 [loss]\n",
      "I0707 23:53:03.058068 15896 agents.py:92] training 1443 of 1500: completed tf_agent.train(...) =    0.345 [loss]\n",
      "I0707 23:53:03.512071 15896 agents.py:92] training 1444 of 1500: completed tf_agent.train(...) =    0.357 [loss]\n",
      "I0707 23:53:03.956070 15896 agents.py:92] training 1445 of 1500: completed tf_agent.train(...) =    0.461 [loss]\n",
      "I0707 23:53:05.458066 15896 agents.py:92] current policy       : avg_reward=0.351, avg_steps=34.600\n",
      "I0707 23:53:05.941183 15896 agents.py:92] training 1446 of 1500: completed tf_agent.train(...) =    0.412 [loss]\n",
      "I0707 23:53:06.378182 15896 agents.py:92] training 1447 of 1500: completed tf_agent.train(...) =    0.480 [loss]\n",
      "I0707 23:53:06.763183 15896 agents.py:92] training 1448 of 1500: completed tf_agent.train(...) =    0.449 [loss]\n",
      "I0707 23:53:07.073219 15896 agents.py:92] training 1449 of 1500: completed tf_agent.train(...) =    0.695 [loss]\n",
      "I0707 23:53:07.429181 15896 agents.py:92] training 1450 of 1500: completed tf_agent.train(...) =    0.318 [loss]\n",
      "I0707 23:53:08.115185 15896 agents.py:92] current policy       : avg_reward=0.724, avg_steps=13.900\n",
      "I0707 23:53:08.496187 15896 agents.py:92] training 1451 of 1500: completed tf_agent.train(...) =    1.366 [loss]\n",
      "I0707 23:53:08.946187 15896 agents.py:92] training 1452 of 1500: completed tf_agent.train(...) =    0.434 [loss]\n",
      "I0707 23:53:09.337182 15896 agents.py:92] training 1453 of 1500: completed tf_agent.train(...) =    0.360 [loss]\n",
      "I0707 23:53:09.686217 15896 agents.py:92] training 1454 of 1500: completed tf_agent.train(...) =    0.433 [loss]\n",
      "I0707 23:53:10.129186 15896 agents.py:92] training 1455 of 1500: completed tf_agent.train(...) =    0.979 [loss]\n",
      "I0707 23:53:10.869212 15896 agents.py:92] current policy       : avg_reward=0.708, avg_steps=15.200\n",
      "I0707 23:53:11.318186 15896 agents.py:92] training 1456 of 1500: completed tf_agent.train(...) =    0.377 [loss]\n",
      "I0707 23:53:11.728182 15896 agents.py:92] training 1457 of 1500: completed tf_agent.train(...) =    0.412 [loss]\n",
      "I0707 23:53:12.196184 15896 agents.py:92] training 1458 of 1500: completed tf_agent.train(...) =    0.383 [loss]\n",
      "I0707 23:53:12.624185 15896 agents.py:92] training 1459 of 1500: completed tf_agent.train(...) =    0.491 [loss]\n",
      "I0707 23:53:13.098183 15896 agents.py:92] training 1460 of 1500: completed tf_agent.train(...) =    0.460 [loss]\n",
      "I0707 23:53:14.570181 15896 agents.py:92] current policy       : avg_reward=0.359, avg_steps=33.900\n",
      "I0707 23:53:15.017184 15896 agents.py:92] training 1461 of 1500: completed tf_agent.train(...) =    0.500 [loss]\n",
      "I0707 23:53:15.427183 15896 agents.py:92] training 1462 of 1500: completed tf_agent.train(...) =    1.038 [loss]\n",
      "I0707 23:53:15.892184 15896 agents.py:92] training 1463 of 1500: completed tf_agent.train(...) =    0.412 [loss]\n",
      "I0707 23:53:16.306185 15896 agents.py:92] training 1464 of 1500: completed tf_agent.train(...) =    0.416 [loss]\n",
      "I0707 23:53:16.741184 15896 agents.py:92] training 1465 of 1500: completed tf_agent.train(...) =    0.856 [loss]\n",
      "I0707 23:53:18.220181 15896 agents.py:92] current policy       : avg_reward=0.478, avg_steps=33.500\n",
      "I0707 23:53:18.625194 15896 agents.py:92] training 1466 of 1500: completed tf_agent.train(...) =    1.102 [loss]\n",
      "I0707 23:53:18.977185 15896 agents.py:92] training 1467 of 1500: completed tf_agent.train(...) =    2.544 [loss]\n",
      "I0707 23:53:19.459184 15896 agents.py:92] training 1468 of 1500: completed tf_agent.train(...) =    0.818 [loss]\n",
      "I0707 23:53:19.798182 15896 agents.py:92] training 1469 of 1500: completed tf_agent.train(...) =    0.909 [loss]\n",
      "I0707 23:53:20.177181 15896 agents.py:92] training 1470 of 1500: completed tf_agent.train(...) =    1.026 [loss]\n",
      "I0707 23:53:21.653181 15896 agents.py:92] current policy       : avg_reward=0.524, avg_steps=33.900\n",
      "I0707 23:53:22.028202 15896 agents.py:92] training 1471 of 1500: completed tf_agent.train(...) =    0.430 [loss]\n",
      "I0707 23:53:22.386181 15896 agents.py:92] training 1472 of 1500: completed tf_agent.train(...) =    0.767 [loss]\n",
      "I0707 23:53:22.830183 15896 agents.py:92] training 1473 of 1500: completed tf_agent.train(...) =    0.450 [loss]\n",
      "I0707 23:53:23.253185 15896 agents.py:92] training 1474 of 1500: completed tf_agent.train(...) =    0.719 [loss]\n",
      "I0707 23:53:23.727183 15896 agents.py:92] training 1475 of 1500: completed tf_agent.train(...) =    1.678 [loss]\n",
      "I0707 23:53:25.175181 15896 agents.py:92] current policy       : avg_reward=0.543, avg_steps=33.100\n",
      "I0707 23:53:25.652182 15896 agents.py:92] training 1476 of 1500: completed tf_agent.train(...) =    1.276 [loss]\n",
      "I0707 23:53:26.106180 15896 agents.py:92] training 1477 of 1500: completed tf_agent.train(...) =    0.926 [loss]\n",
      "I0707 23:53:26.547183 15896 agents.py:92] training 1478 of 1500: completed tf_agent.train(...) =    0.375 [loss]\n",
      "I0707 23:53:27.018183 15896 agents.py:92] training 1479 of 1500: completed tf_agent.train(...) =    0.791 [loss]\n",
      "I0707 23:53:27.396181 15896 agents.py:92] training 1480 of 1500: completed tf_agent.train(...) =    0.475 [loss]\n",
      "I0707 23:53:28.035187 15896 agents.py:92] current policy       : avg_reward=0.709, avg_steps=14.200\n",
      "I0707 23:53:28.478189 15896 agents.py:92] training 1481 of 1500: completed tf_agent.train(...) =    0.530 [loss]\n",
      "I0707 23:53:28.937185 15896 agents.py:92] training 1482 of 1500: completed tf_agent.train(...) =    0.426 [loss]\n",
      "I0707 23:53:29.386184 15896 agents.py:92] training 1483 of 1500: completed tf_agent.train(...) =    0.397 [loss]\n",
      "I0707 23:53:31.291187 15896 agents.py:92] training 1484 of 1500: completed tf_agent.train(...) =   37.755 [loss]\n",
      "I0707 23:53:31.740185 15896 agents.py:92] training 1485 of 1500: completed tf_agent.train(...) =    0.592 [loss]\n",
      "I0707 23:53:32.382183 15896 agents.py:92] current policy       : avg_reward=0.718, avg_steps=14.600\n",
      "I0707 23:53:32.759187 15896 agents.py:92] training 1486 of 1500: completed tf_agent.train(...) =    2.030 [loss]\n",
      "I0707 23:53:33.273182 15896 agents.py:92] training 1487 of 1500: completed tf_agent.train(...) =   56.652 [loss]\n",
      "I0707 23:53:35.124197 15896 agents.py:92] training 1488 of 1500: completed tf_agent.train(...) =   33.106 [loss]\n",
      "I0707 23:53:35.465188 15896 agents.py:92] training 1489 of 1500: completed tf_agent.train(...) =    3.483 [loss]\n",
      "I0707 23:53:35.819180 15896 agents.py:92] training 1490 of 1500: completed tf_agent.train(...) =    1.740 [loss]\n",
      "I0707 23:53:36.500181 15896 agents.py:92] current policy       : avg_reward=0.738, avg_steps=13.800\n",
      "I0707 23:53:36.942182 15896 agents.py:92] training 1491 of 1500: completed tf_agent.train(...) =    1.586 [loss]\n",
      "I0707 23:53:37.358184 15896 agents.py:92] training 1492 of 1500: completed tf_agent.train(...) =    1.530 [loss]\n",
      "I0707 23:53:37.760184 15896 agents.py:92] training 1493 of 1500: completed tf_agent.train(...) =    2.125 [loss]\n",
      "I0707 23:53:38.111183 15896 agents.py:92] training 1494 of 1500: completed tf_agent.train(...) =  172.806 [loss]\n",
      "I0707 23:53:38.536187 15896 agents.py:92] training 1495 of 1500: completed tf_agent.train(...) =    4.580 [loss]\n",
      "I0707 23:53:39.200183 15896 agents.py:92] current policy       : avg_reward=0.753, avg_steps=13.100\n",
      "I0707 23:53:39.633184 15896 agents.py:92] training 1496 of 1500: completed tf_agent.train(...) =    4.987 [loss]\n",
      "I0707 23:53:40.063186 15896 agents.py:92] training 1497 of 1500: completed tf_agent.train(...) =    4.215 [loss]\n",
      "I0707 23:53:40.496183 15896 agents.py:92] training 1498 of 1500: completed tf_agent.train(...) =    3.041 [loss]\n",
      "I0707 23:53:40.836184 15896 agents.py:92] training 1499 of 1500: completed tf_agent.train(...) =    2.017 [loss]\n",
      "I0707 23:53:41.198181 15896 agents.py:92] training 1500 of 1500: completed tf_agent.train(...) =    1.298 [loss]\n",
      "I0707 23:53:41.966181 15896 agents.py:92] current policy       : avg_reward=0.720, avg_steps=15.300\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "ppoAgent = PpoAgent(    gym_env_name = 'Berater-v1',\n",
    "                        fc_layers=(500,500,500), \n",
    "                        training_duration=training_duration,\n",
    "                        learning_rate=1e-4\n",
    "                   )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize (with custom y-limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_average_rewards(ylim=[-2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_average_steps(ylim=[0,210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppoAgent.plot_losses(ylim=[0,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Episode: 5463   Step:    1  S --2-> C R= 0.13 totalR= 0.13 cost= 200 customerR=1000 optimum=6000\nEpisode: 5463   Step:    2  C --2-> M R=-0.02 totalR= 0.12 cost= 100 customerR=   0 optimum=6000\nEpisode: 5463   Step:    3  M --2-> N R= 0.15 totalR= 0.27 cost= 100 customerR=1000 optimum=6000\nEpisode: 5463   Step:    4  N --0-> M R=-0.02 totalR= 0.25 cost= 100 customerR=   0 optimum=6000\nEpisode: 5463   Step:    5  M --1-> L R= 0.16 totalR= 0.41 cost=  50 customerR=1000 optimum=6000\nEpisode: 5463   Step:    6  L --0-> C R=-0.03 totalR= 0.38 cost= 200 customerR=   0 optimum=6000\nEpisode: 5463   Step:    7  C --1-> B R=-0.01 totalR= 0.37 cost=  50 customerR=   0 optimum=6000\nEpisode: 5463   Step:    8  B --1-> A R= 0.15 totalR= 0.52 cost= 100 customerR=1000 optimum=6000\nEpisode: 5463   Step:    9  A --3-> D R= 0.15 totalR= 0.67 cost= 100 customerR=1000 optimum=6000\nEpisode: 5463   Step:   10  D --0-> A R=-0.02 totalR= 0.65 cost= 100 customerR=   0 optimum=6000\nEpisode: 5463   Step:   11  A --1-> B R=-0.02 totalR= 0.63 cost= 100 customerR=   0 optimum=6000\nEpisode: 5463   Step:   12  B --3-> K R= 0.13 totalR= 0.77 cost= 200 customerR=1000 optimum=6000\nEpisode: 5463   Step:   13  K --0-> B R=-0.03 totalR= 0.73 cost= 200 customerR=   0 optimum=6000\nEpisode: 5464   Step:   14  B --0-> S R=-0.02 totalR= 0.72 cost= 100 customerR=   0 optimum=6000\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "ppoAgent.render_episodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zpzHtN3-kQ26",
    "w3OdHyWEEEwy",
    "bzoq0VM85p46"
   ],
   "name": "easyagents_berater.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}