{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/easyagents_line.ipynb\" target=\"_parent\">\n",
    "   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU7ylMh1kQ2y"
   },
   "source": [
    "# Line World\n",
    "\n",
    "* an agent lives in a finite linear world of uneven elements\n",
    "* at each moment it is in a certain position\n",
    "* initial position is the middle\n",
    "* some positions gain rewards, some don't\n",
    "* rewards are between 0 and 15\n",
    "* agent can either move left or right\n",
    "* Objective: maximize total reward = sum(rewards) + sum(steps)\n",
    "* Cost per step: -1\n",
    "* Done Condition: agent is at pos 0 or total reward <= -20\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/christianhidber/easyagents/master/images/line-world.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Install gym, tensorflow, tf-agents,..., setup display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFQXRNBFI5Re",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -q easyagents\n",
    "    !pip install -q networkx==2.3.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aS8yqznR8UL7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### suppress package warnings, in colab: load additional packages for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DcilvDdeI5Ri",
    "outputId": "ebcf640f-0254-442c-cdd8-062b61c12547",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "if 'google.colab' in sys.modules:\n",
    "    !apt-get install xvfb >/dev/null\n",
    "    !pip install pyvirtualdisplay >/dev/null    \n",
    "    \n",
    "    from pyvirtualdisplay import Display\n",
    "    Display(visible=0, size=(960, 720)).start() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of OpenAI Gym Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6yEvuCliwJiD",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "\n",
    "class LineWorldEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, state=[10, 0, 0, 5, 0, 2, 15]):\n",
    "        self.state = np.array(state)\n",
    "        # the agent can perform  different actions\n",
    "        number_of_actions = 2\n",
    "        self.action_space = spaces.Discrete(number_of_actions)\n",
    "\n",
    "        self.size_of_world = len(state)\n",
    "        \n",
    "        self.max_reward = 15\n",
    "\n",
    "        # the environment's state is described by the position of the agent and the remaining rewards\n",
    "        low = np.append([0], np.full(self.size_of_world, 0))\n",
    "        high = np.append([self.size_of_world - 1], np.full(self.size_of_world, self.max_reward))\n",
    "        self.observation_space = spaces.Box(low=low,\n",
    "                                            high=high,\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        self.reward_range = (-1, 1)\n",
    "        # 32 is only theoretical, because we need to travel at least 9 steps, leaving us with 23 practically\n",
    "        self.optimum = self.state.sum() - 9\n",
    "\n",
    "        self._figure = None\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def get_observation(self):\n",
    "        return np.append([self.pos], self.remaining_rewards)\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_reward = 0\n",
    "        self.done = False\n",
    "        self.pos = math.floor(len(self.state) / 2)\n",
    "        self.steps = 0\n",
    "\n",
    "        self.remaining_rewards = np.array(self.state, copy=True)\n",
    "        return self.get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == LEFT and self.pos != 0:\n",
    "          self.pos -= 1\n",
    "        elif self.pos < self.size_of_world -1:\n",
    "          self.pos += 1\n",
    "\n",
    "        reward = self.remaining_rewards[self.pos] - 1\n",
    "        normalized_reward = reward / self.optimum\n",
    "        self.total_reward += normalized_reward\n",
    "        self.remaining_rewards[self.pos] = 0\n",
    "\n",
    "        if self.pos == 0 or self.total_reward * self.optimum <= -20:\n",
    "          self.done = True\n",
    "        self.steps += 1\n",
    "\n",
    "        observation = self.get_observation()\n",
    "        info = None\n",
    "        return observation, normalized_reward, self.done, info\n",
    "\n",
    "    def _render_to_ansi(self):\n",
    "        return 'position: {position}, remaining rewards: {rewards}, total reward so far: {total}, normalized total reward: {normalized_total}, steps so far: {steps}, game done: {done}'.format(\n",
    "            position=self.pos, \n",
    "            rewards=self.remaining_rewards, \n",
    "            total=self.total_reward * self.optimum, \n",
    "            normalized_total = self.total_reward,\n",
    "            done=self.done,\n",
    "            steps=self.steps)              \n",
    "\n",
    "    def _render_to_figure(self):\n",
    "        \"\"\" Renders the current state as a graph with matplotlib \"\"\"\n",
    "        if self._figure is not None:\n",
    "            plt.close(self._figure)\n",
    "        self._figure, ax = plt.subplots(1, figsize=(8, 4))\n",
    "        ax.set_ylim(bottom=-1, top=self.max_reward + 1)\n",
    "        x = np.arange(0, self.size_of_world, 1, dtype=np.uint8)\n",
    "        y = self.remaining_rewards\n",
    "        plt.plot([self.pos, self.pos], [0, 2], 'r^-')\n",
    "        ax.scatter(x, y, s=75)\n",
    "        self._figure.canvas.draw()\n",
    "        \n",
    "    def _render_to_human(self):\n",
    "        \"\"\" show render_to_figure in a jupyter cell. \n",
    "            the result of each call is rendered in the same cell\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        self._render_to_figure()\n",
    "        plt.pause(0.01)\n",
    "        \n",
    "    def _render_to_rgb(self):\n",
    "        \"\"\" convert the output of render_to_figure to a rgb_array \"\"\"\n",
    "        self._render_to_figure()\n",
    "        self._figure.canvas.draw()\n",
    "        buf = self._figure.canvas.tostring_rgb()\n",
    "        num_cols, num_rows = self._figure.canvas.get_width_height()\n",
    "        plt.close(self._figure)\n",
    "        self._figure = None\n",
    "        result = np.fromstring(buf, dtype=np.uint8).reshape(num_rows, num_cols, 3)\n",
    "        return result        \n",
    "\n",
    "    def render(self, mode='ansi'):\n",
    "        if mode == 'ansi':\n",
    "            return self._render_to_ansi()\n",
    "        elif mode == 'human':\n",
    "            return self._render_to_human()\n",
    "        elif mode == 'rgb_array':\n",
    "            return self._render_to_rgb()\n",
    "        else:\n",
    "            super().render(mode=mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fHrBrxxI5Rt",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "env = LineWorldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "hxcQG_ruV06p",
    "outputId": "d604f0cc-65d3-4e25-cf81-b1c901d87967"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'position: 3, remaining rewards: [10  0  0  5  0  2 15], total reward so far: 0, normalized total reward: 0, steps so far: 0, game done: False'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SJARCo5U9PfR",
    "outputId": "510eb2ef-52f8-443d-b141-f8ccf8010dad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10,  0,  0,  5,  0,  2, 15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the raw observation, position is first, then all rest rewards\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s2RYX2_2I5Ry",
    "outputId": "6e4180ed-50a9-4608-c29c-8c6b6fa9b8c9",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'position: 4, remaining rewards: [10  0  0  5  0  2 15], total reward so far: -1.0, normalized total reward: -0.043478260869565216, steps so far: 1, game done: False'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(RIGHT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mGWH4CQCBSEk",
    "outputId": "099d8200-82af-400b-d595-fab2b565cae2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'position: 5, remaining rewards: [10  0  0  5  0  0 15], total reward so far: 0.0, normalized total reward: 0.0, steps so far: 2, game done: False'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(RIGHT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JdHyE42vBhbC",
    "outputId": "6bce282e-aa51-4964-b756-6fe109b96408"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'position: 6, remaining rewards: [10  0  0  5  0  0  0], total reward so far: 14.0, normalized total reward: 0.6086956521739131, steps so far: 3, game done: False'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(RIGHT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5QRACggIChZK",
    "outputId": "cb04cf27-b695-428c-a9d6-4aead296d1a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'position: 6, remaining rewards: [10  0  0  5  0  0  0], total reward so far: 13.000000000000002, normalized total reward: 0.5652173913043479, steps so far: 4, game done: False'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(RIGHT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "RvfFHbSfckCy",
    "outputId": "296fc5b4-6af0-4133-d15c-d02a19faf71d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r4zbZtCsBx7x",
    "outputId": "d939d5f4-7a10-4c8c-960d-d9baf2c1b1e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'position: 5, remaining rewards: [10  0  0  5  0  0  0], total reward so far: 12.000000000000002, normalized total reward: 0.5217391304347827, steps so far: 5, game done: False'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5eso3wdfFfGu",
    "outputId": "0a5a3748-62a7-4830-8daa-cafcf2f10595"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'position: 4, remaining rewards: [10  0  0  5  0  0  0], total reward so far: 11.000000000000004, normalized total reward: 0.4782608695652175, steps so far: 6, game done: False'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_FUidR2SFgq2",
    "outputId": "e9222e21-038c-47bb-ccea-bf6713540cce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'position: 3, remaining rewards: [10  0  0  0  0  0  0], total reward so far: 15.000000000000004, normalized total reward: 0.6521739130434784, steps so far: 7, game done: False'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zaPJ-GDtFnRL",
    "outputId": "588a0a37-8e5b-4580-fcf1-ce49a9c1d2f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'position: 2, remaining rewards: [10  0  0  0  0  0  0], total reward so far: 14.000000000000004, normalized total reward: 0.6086956521739132, steps so far: 8, game done: False'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RdmP61D_FpTE",
    "outputId": "917980c6-e986-4a9e-e6a4-d32811881102"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'position: 1, remaining rewards: [10  0  0  0  0  0  0], total reward so far: 13.000000000000004, normalized total reward: 0.565217391304348, steps so far: 9, game done: False'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y1dxqY92FqV2",
    "outputId": "569fad4b-bac9-4074-adb9-7674b1c60eba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'position: 0, remaining rewards: [0 0 0 0 0 0 0], total reward so far: 22.000000000000007, normalized total reward: 0.956521739130435, steps so far: 10, game done: True'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render(mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Register with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OS3pnnbAI5R9",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.easyenv import register_with_gym\n",
    "\n",
    "env_name=\"LineWorld-v0\"\n",
    "register_with_gym(gym_env_name=env_name, entry_point=LineWorldEnv, max_episode_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uEfpOeeI5R8"
   },
   "source": [
    "# Train policy with tfagents PpoAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "gFISrkwhI5SZ",
    "outputId": "eabab98f-9994-4fd8-8023-1d564bc9dfaa",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'TypeSpec'\n  In call to configurable 'BatchedPyEnvironment' (<function BatchedPyEnvironment.__init__ at 0x0000021AF0FF9D90>)\n  In call to configurable 'TFPyEnvironment' (<function TFPyEnvironment.__init__ at 0x0000021AF1017268>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\easyagents\\lib\\site-packages\\easyagents\\agents.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_is_jupyter_display_figure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_render_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\easyagents\\lib\\site-packages\\easyagents\\tfagents.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;31m# Create Training Environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating environment:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mtrain_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_tfagent_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[0mobservation_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0maction_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\easyagents\\lib\\site-packages\\easyagents\\tfagents.py\u001b[0m in \u001b[0;36m_create_tfagent_env\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m                                 \u001b[0mdiscount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reward_discount_gamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                                 max_episode_steps=self._training.max_steps_per_episode)\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_py_environment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFPyEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_env\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1030\u001b[0m         \u001b[0mscope_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" in scope '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m         \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gin\\utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[1;34m(exception, message)\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mExceptionProxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\easyagents\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1009\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\easyagents\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, environment, check_dims)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatched\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m       \u001b[0menvironment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatched_py_environment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchedPyEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_dims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1030\u001b[0m         \u001b[0mscope_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" in scope '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m         \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gin\\utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[1;34m(exception, message)\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mExceptionProxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\easyagents\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1009\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\easyagents\\lib\\site-packages\\tf_agents\\environments\\batched_py_environment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, envs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_action_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_envs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_observation_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_envs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_envs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_step_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_action_spec\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m       raise ValueError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\easyagents\\lib\\site-packages\\tf_agents\\environments\\py_environment.py\u001b[0m in \u001b[0;36mtime_step_spec\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0mstep_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \"\"\"\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_step_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcurrent_time_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\easyagents\\lib\\site-packages\\tf_agents\\trajectories\\time_step.py\u001b[0m in \u001b[0;36mtime_step_spec\u001b[1;34m(observation_spec)\u001b[0m\n\u001b[0;32m    301\u001b[0m   \u001b[0mfirst_observation_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m   if isinstance(first_observation_spec,\n\u001b[1;32m--> 303\u001b[1;33m                 (tf.TypeSpec, tensor_spec.TensorSpec,\n\u001b[0m\u001b[0;32m    304\u001b[0m                  tensor_spec.BoundedTensorSpec)):\n\u001b[0;32m    305\u001b[0m     return TimeStep(\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'TypeSpec'\n  In call to configurable 'BatchedPyEnvironment' (<function BatchedPyEnvironment.__init__ at 0x0000021AF0FF9D90>)\n  In call to configurable 'TFPyEnvironment' (<function TFPyEnvironment.__init__ at 0x0000021AF1017268>)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import Training\n",
    "\n",
    "training=Training( num_iterations = 50,\n",
    "                   num_episodes_per_iteration = 10,\n",
    "                   max_steps_per_episode = 50,\n",
    "                   num_epochs_per_iteration = 5 )\n",
    "\n",
    "ppoAgent = PpoAgent(    gym_env_name = env_name,\n",
    "                        fc_layers=(500,500), \n",
    "                        training=training,\n",
    "                        learning_rate=1e-4\n",
    "                   )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "2oeN8hhRJqMr",
    "outputId": "0d96f889-bdc6-48e1-e448-67e2ac957ba9"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "policy not yet trained. call train() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-12b4035b5951>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mppoAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_episodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\easyagents\\lib\\site-packages\\easyagents\\agents.py\u001b[0m in \u001b[0;36mrender_episodes\u001b[1;34m(self, num_episodes, mode)\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                 self.play_episode(\n\u001b[1;32m--> 313\u001b[1;33m                     callback=lambda gym_env, action, state, reward, step, done, info: gym_env.render(mode=mode))\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     def render_episodes_to_jupyter(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\easyagents\\lib\\site-packages\\easyagents\\agents.py\u001b[0m in \u001b[0;36mplay_episode\u001b[1;34m(self, max_steps, callback)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;33m:\u001b[0m\u001b[0mreturns\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_play_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_play_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\easyagents\\lib\\site-packages\\easyagents\\tfagents.py\u001b[0m in \u001b[0;36m_play_episode\u001b[1;34m(self, max_steps, callback)\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;33m:\u001b[0m\u001b[0mreturns\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \"\"\"\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trained_policy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"policy not yet trained. call train() first.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"max_steps must be > 0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: policy not yet trained. call train() first."
     ]
    }
   ],
   "source": [
    "ppoAgent.render_episodes(num_episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OJYNSBaI5Sf"
   },
   "source": [
    "### Visualize training and policy (with custom y-limits and movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "JVAsAknHI5Sf",
    "outputId": "29c22448-9626-43e7-9cf9-71f4f9abcb8d",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "_ = ppoAgent.plot_episodes(ylim=[None,(0,1),(0,50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "C0XVkNWaI5Sj",
    "outputId": "9b2e16fa-238a-4e05-b50c-8ee1610816ee",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent.render_episodes_to_jupyter(num_episodes=1, fps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train policy with tfagents Reinforce (aka Vanilla Policy Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTHjAT-6Z7UQ"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from easyagents.tfagents import ReinforceAgent\n",
    "from easyagents.config import Training\n",
    "\n",
    "training=Training( num_iterations = 200,\n",
    "                   num_episodes_per_iteration = 10,\n",
    "                   max_steps_per_episode = 50,\n",
    "                   num_epochs_per_iteration = 5 )\n",
    "\n",
    "reinforceAgent = ReinforceAgent(    gym_env_name = env_name,\n",
    "                        fc_layers=(500,500), \n",
    "                        training=training,\n",
    "                        learning_rate=1e-4\n",
    "                   )\n",
    "reinforceAgent.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zpzHtN3-kQ26",
    "w3OdHyWEEEwy",
    "bzoq0VM85p46"
   ],
   "include_colab_link": true,
   "name": "easyagents_line.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
